{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 循环神经网络 RNN\n",
    "\n",
    "循环神经网络(Recurrent Neural Network, RNN)一般是指时间递归神经网络而非结构递归神经网络 (Recursive Neural Network)，其主要用于对序列数据进行建模。\n",
    "\n",
    "RNN之所以称为循环神经网路，即一个序列当前的输出与前面的输出也有关。具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。理论上，RNN能够对任何长度的序列数据进行处理。但是在实践中，为了降低复杂性往往假设当前的状态只与前面的几个状态相关。\n",
    "\n",
    "传统神经网络(包括CNN)，输入和输出都是互相独立的。如图像上的猫和狗是分隔开的，但有些任务，后续的输出和之前的内容是相关的。例如：我是中国人，我的母语是____。这是一道填空题，需要依赖之前的输入。\n",
    "\n",
    "RNN引入“记忆”的概念，也就是输出需要依赖之前的输入序列，并把关键输入记住。循环2字来源于其每个元素都执行相同的任务。它并⾮刚性地记忆所有固定长度的序列，而是通过隐藏状态来存储之前时间步的信息。\n",
    "\n",
    "RNN跟传统神经网络最大的区别在于每次都会将前一次的输出结果，带到下一次的隐藏层中，一起训练。如下图所示：\n",
    "\n",
    "![RNN_Node](images/RNN_Node.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN的网络结构\n",
    "\n",
    "基本循环神经网络结构：一个输入层、一个隐藏层和一个输出层。\n",
    "\n",
    "![RNN structure](images/RNN_Structure.jpeg)\n",
    "\n",
    "$x$是输入层的值，$s$表示隐藏层的值，$U$是输入层到隐藏层的权重矩阵，$O$是输出层的值，$V$是隐藏层到输出层的权重矩阵。\n",
    "\n",
    "循环神经网络的隐藏层的值$s$不仅仅取决于当前这次的输入$x$，还取决于上一次隐藏层的值$s$。权重矩阵$W$就是隐藏层上一次的值作为这一次的输入的权重。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN的实现\n",
    "\n",
    "\n",
    "对于最简单的 RNN，我们可以使用下面两种方式去调用，分别是 `torch.nn.RNNCell()` 和 `torch.nn.RNN()`，这两种方式的区别在于 `RNNCell()` 只能接受序列中单步的输入，且必须传入隐藏状态，而 `RNN()` 可以接受一个序列的输入，默认会传入全 0 的隐藏状态，也可以自己申明隐藏状态传入。\n",
    "\n",
    "`RNN()` 里面的参数有\n",
    "\n",
    "* `input_size` 表示输入 $x_t$ 的特征维度\n",
    "* `hidden_size` 表示输出的特征维度\n",
    "* `num_layers` 表示网络的层数\n",
    "* `nonlinearity` 表示选用的非线性激活函数，默认是 'tanh'\n",
    "* `bias` 表示是否使用偏置，默认使用\n",
    "* `batch_first` 表示输入数据的形式，默认是 False，就是这样形式，(seq, batch, feature)，也就是将序列长度放在第一位，batch 放在第二位\n",
    "* `dropout` 表示是否在输出层应用 dropout\n",
    "* `bidirectional` 表示是否使用双向的 rnn，默认是 False\n",
    "\n",
    "对于 `RNNCell()`，里面的参数就少很多，只有 input_size，hidden_size，bias 以及 nonlinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个单步的 rnn\n",
    "rnn_single = nn.RNNCell(input_size=100, hidden_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-2.7963e-02,  3.6102e-02,  5.6609e-03,  ..., -3.0035e-02,\n",
       "          2.7740e-02,  2.3327e-02],\n",
       "        [-2.8567e-02, -3.2150e-02, -2.6686e-02,  ..., -4.6441e-02,\n",
       "          3.5804e-02,  9.7260e-05],\n",
       "        [ 4.6686e-02, -1.5825e-02,  6.7149e-02,  ...,  3.3435e-02,\n",
       "         -2.7623e-02, -6.7693e-02],\n",
       "        ...,\n",
       "        [-2.0338e-02, -1.6551e-02,  5.8996e-02,  ..., -4.0145e-02,\n",
       "         -6.9111e-03, -3.2740e-02],\n",
       "        [-2.4584e-02,  2.3591e-02,  8.3090e-03,  ..., -3.6077e-02,\n",
       "         -6.0432e-03,  5.6279e-02],\n",
       "        [ 5.6955e-02, -5.1925e-02,  3.1950e-02,  ..., -5.6692e-02,\n",
       "          6.1773e-02,  1.9715e-02]], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 访问其中的参数\n",
    "rnn_single.weight_hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 构造一个序列，长为 6，batch 是 5， 特征是 100\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m100\u001b[39m) \u001b[38;5;66;03m# 这是 rnn 的输入格式\u001b[39;00m\n\u001b[1;32m      3\u001b[0m x\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# 构造一个序列，长为 6，batch 是 5， 特征是 100\n",
    "x = torch.randn(6, 5, 100) # 这是 rnn 的输入格式\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 定义初始的记忆状态\n",
    "h_t = torch.zeros(5, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 传入 rnn\n",
    "out = []\n",
    "for i in range(6): # 通过循环 6 次作用在整个序列上\n",
    "    h_t = rnn_single(x[i], h_t)\n",
    "    out.append(h_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.0136  0.3723  0.1704  ...   0.4306 -0.7909 -0.5306\n",
       "-0.2681 -0.6261 -0.3926  ...   0.1752  0.5739 -0.2061\n",
       "-0.4918 -0.7611  0.2787  ...   0.0854 -0.3899  0.0092\n",
       " 0.6050  0.1852 -0.4261  ...  -0.7220  0.6809  0.1825\n",
       "-0.6851  0.7273  0.5396  ...  -0.7969  0.6133 -0.0852\n",
       "[torch.FloatTensor of size 5x200]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 200])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape # 每个输出的维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到经过了 rnn 之后，隐藏状态的值已经被改变了，因为网络记忆了序列中的信息，同时输出 6 个结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们看看直接使用 `RNN` 的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "rnn_seq = nn.RNN(100, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "1.00000e-02 *\n",
       " 1.0998 -1.5018 -1.4337  ...   3.8385 -0.8958 -1.6781\n",
       " 5.3302 -5.4654  5.5568  ...   4.7399  5.4110  3.6170\n",
       " 1.0788 -0.6620  5.7689  ...  -5.0747 -2.9066  0.6152\n",
       "          ...             ⋱             ...          \n",
       "-5.6921  0.1843 -0.0803  ...  -4.5852  5.6194 -1.4734\n",
       " 4.4306  6.9795 -1.5736  ...   3.4236 -0.3441  3.1397\n",
       " 7.0349 -1.6120 -4.2840  ...  -5.5676  6.8897  6.1968\n",
       "[torch.FloatTensor of size 200x200]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 访问其中的参数\n",
    "rnn_seq.weight_hh_l0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "out, h_t = rnn_seq(x) # 使用默认的全 0 隐藏状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 ,.,.) = \n",
       "  0.2012  0.0517  0.0570  ...   0.2316  0.3615 -0.1247\n",
       "  0.5307  0.4147  0.7881  ...  -0.4138 -0.1444  0.3602\n",
       "  0.0882  0.4307  0.3939  ...   0.3244 -0.4629 -0.2315\n",
       "  0.2868  0.7400  0.6534  ...   0.6631  0.2624 -0.0162\n",
       "  0.0841  0.6274  0.1840  ...   0.5800  0.8780  0.4301\n",
       "[torch.FloatTensor of size 1x5x200]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的 h_t 是网络最后的隐藏状态，网络也输出了 6 个结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 自己定义初始的隐藏状态\n",
    "h_0 = Variable(torch.randn(1, 5, 200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的隐藏状态的大小有三个维度，分别是 (num_layers * num_direction, batch, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, h_t = rnn_seq(x, h_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 ,.,.) = \n",
       "  0.2091  0.0353  0.0625  ...   0.2340  0.3734 -0.1307\n",
       "  0.5498  0.4221  0.7877  ...  -0.4143 -0.1209  0.3335\n",
       "  0.0757  0.4204  0.3826  ...   0.3187 -0.4626 -0.2336\n",
       "  0.3106  0.7355  0.6436  ...   0.6611  0.2587 -0.0338\n",
       "  0.1025  0.6350  0.1943  ...   0.5720  0.8749  0.4525\n",
       "[torch.FloatTensor of size 1x5x200]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 5, 200])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时输出的结果也是 (seq, batch, feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般情况下我们都是用 `nn.RNN()` 而不是 `nn.RNNCell()`，因为 `nn.RNN()` 能够避免我们手动写循环，非常方便，同时如果不特别说明，我们也会选择使用默认的全 0 初始化隐藏状态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考资料\n",
    "* [机器学习——循环神经网络（RNN）](https://blog.csdn.net/beiye_/article/details/123526075)\n",
    "* [循环神经网络讲解（RNN/LSTM/GRU）](https://zhuanlan.zhihu.com/p/123211148)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
