{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 循环神经网络 RNN\n\n循环神经网络(Recurrent Neural Network, RNN)一般是指时间递归神经网络而非结构递归神经网络 (Recursive Neural Network)，其主要用于对序列数据进行建模。\n\nRNN之所以称为循环神经网路，即一个序列当前的输出与前面的输出也有关。具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。理论上，RNN能够对任何长度的序列数据进行处理。但是在实践中，为了降低复杂性往往假设当前的状态只与前面的几个状态相关。\n\n## 为什么需要RNN？\n\n传统神经网络(包括CNN)，输入和输出都是互相独立的。如图像上的猫和狗是分隔开的，但有些任务，后续的输出和之前的内容是相关的。例如：\n\n**问题**: \"我是中国人，我的母语是____？\"\n\n这是一道填空题，需要依赖之前的输入来推断答案。传统神经网络无法处理这种时序依赖关系。\n\n## RNN的核心思想\n\nRNN引入\"记忆\"的概念，也就是输出需要依赖之前的输入序列，并把关键输入记住。循环2字来源于其每个元素都执行相同的任务。它并非刚性地记忆所有固定长度的序列，而是通过隐藏状态来存储之前时间步的信息。\n\nRNN跟传统神经网络最大的区别在于每次都会将前一次的输出结果，带到下一次的隐藏层中，一起训练。如下图所示：\n\n![RNN_Node](images/RNN_Node.gif)\n\n**关键特点**：\n- 具有\"记忆\"能力，能记住之前的信息\n- 同一个网络结构处理序列中的每个元素\n- 参数在时间步之间共享"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## RNN的网络结构\n\n基本循环神经网络结构：一个输入层、一个隐藏层和一个输出层。\n\n![RNN structure](images/RNN_Structure.jpeg)\n\n### 数学表示\n\n- $x_t$: 时刻 $t$ 的输入向量\n- $h_t$: 时刻 $t$ 的隐藏状态\n- $o_t$: 时刻 $t$ 的输出\n- $U$: 输入到隐藏层的权重矩阵\n- $W$: 隐藏层到隐藏层的权重矩阵（循环连接）\n- $V$: 隐藏层到输出层的权重矩阵\n\n### 前向传播公式\n\n```\nh_t = tanh(U * x_t + W * h_{t-1} + b_h)\no_t = V * h_t + b_o\n```\n\n**关键理解**：\n- 当前隐藏状态 $h_t$ 不仅取决于当前输入 $x_t$，还取决于前一时刻的隐藏状态 $h_{t-1}$\n- 权重矩阵 $W$ 实现了\"记忆\"的传递\n- 相同的参数在所有时间步中共享\n\n### RNN的展开形式\n\n虽然RNN在概念上是循环的，但在实际计算中，我们将其展开为前馈网络：\n\n```\nt=0: h_0 = tanh(U * x_0 + W * h_{-1} + b_h)  # h_{-1} 通常初始化为0\nt=1: h_1 = tanh(U * x_1 + W * h_0 + b_h)\nt=2: h_2 = tanh(U * x_2 + W * h_1 + b_h)\n...\n```\n\n这种展开方式使得我们能够用标准的反向传播算法训练RNN。"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## RNN的梯度消失问题\n\n### 问题的来源\n\n在标准的反向传播中，梯度通过链式法则向前层传播：\n\n$$\\frac{\\partial L}{\\partial W} = \\sum_{t=1}^{T} \\frac{\\partial L}{\\partial o_t} \\frac{\\partial o_t}{\\partial h_t} \\frac{\\partial h_t}{\\partial W}$$\n\n其中，$\\frac{\\partial h_t}{\\partial W}$ 包含了从时刻 $t$ 回到时刻 $1$ 的所有路径：\n\n$$\\frac{\\partial h_t}{\\partial h_k} = \\prod_{i=k+1}^{t} \\frac{\\partial h_i}{\\partial h_{i-1}} = \\prod_{i=k+1}^{t} W \\cdot \\text{diag}(tanh'(z_i))$$\n\n### 数学分析\n\n由于 $tanh'(x) \\leq 1$，如果权重矩阵 $W$ 的最大特征值小于1，那么：\n\n- **梯度消失**：当 $t-k$ 很大时，$\\prod_{i=k+1}^{t} W \\cdot \\text{diag}(tanh'(z_i)) \\rightarrow 0$\n- **梯度爆炸**：当 $W$ 的特征值大于1时，梯度可能指数级增长\n\n### 实际影响\n\n1. **长期依赖学习困难**：RNN难以学习时间跨度较长的依赖关系\n2. **训练不稳定**：梯度爆炸导致训练过程不稳定\n3. **信息丢失**：早期的重要信息在传播过程中被稀释\n\n### 解决方案预览\n\n- **LSTM (Long Short-Term Memory)**：通过门控机制保护梯度流\n- **GRU (Gated Recurrent Unit)**：简化的门控单元\n- **梯度裁剪**：限制梯度的最大范数\n- **更好的初始化**：使用正交初始化等方法"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 导入必要的库\nimport torch\nfrom torch.autograd import Variable\nfrom torch import nn\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 设置随机种子以确保结果可复现\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nprint(\"PyTorch version:\", torch.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## PyTorch中的RNN实现\n\n对于最简单的 RNN，我们可以使用下面两种方式去调用，分别是 `torch.nn.RNNCell()` 和 `torch.nn.RNN()`：\n\n### RNNCell vs RNN的区别\n\n- **`RNNCell()`**: 只能接受序列中单步的输入，且必须传入隐藏状态\n- **`RNN()`**: 可以接受一个序列的输入，默认会传入全 0 的隐藏状态\n\n### RNN() 参数说明\n\n- `input_size`: 输入 $x_t$ 的特征维度\n- `hidden_size`: 隐藏状态和输出的特征维度\n- `num_layers`: 网络的层数（可以堆叠多层RNN）\n- `nonlinearity`: 非线性激活函数，默认是 'tanh'，也可以选择 'relu'\n- `bias`: 是否使用偏置，默认为 True\n- `batch_first`: 输入数据的形状格式\n  - False (默认): (seq_len, batch, features)\n  - True: (batch, seq_len, features)\n- `dropout`: 是否在输出层应用 dropout（仅在多层时有效）\n- `bidirectional`: 是否使用双向 RNN，默认为 False\n\n### RNNCell() 参数说明\n\n- `input_size`: 输入特征维度\n- `hidden_size`: 隐藏状态维度\n- `bias`: 是否使用偏置\n- `nonlinearity`: 激活函数类型\n\n让我们通过实例来理解这两种实现方式："
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 定义一个单步的 RNN Cell\nrnn_single = nn.RNNCell(input_size=100, hidden_size=200)\n\nprint(\"RNN Cell 参数形状:\")\nprint(\"Input to Hidden 权重:\", rnn_single.weight_ih.shape)  # (hidden_size, input_size)\nprint(\"Hidden to Hidden 权重:\", rnn_single.weight_hh.shape)  # (hidden_size, hidden_size)\nprint(\"偏置项:\", rnn_single.bias_ih.shape, rnn_single.bias_hh.shape)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 查看权重参数的具体值\nprint(\"Hidden to Hidden 权重矩阵 (前5x5):\")\nprint(rnn_single.weight_hh[:5, :5])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": "## 使用 RNNCell 处理序列\n\n# 构造一个序列：长度为6，batch为5，特征为100\nx = torch.randn(6, 5, 100)  # (seq_len, batch, feature)\nprint(\"输入序列形状:\", x.shape)\n\n# 定义初始隐藏状态\nh_t = torch.zeros(5, 200)  # (batch, hidden_size)\nprint(\"初始隐藏状态形状:\", h_t.shape)\n\n# 手动逐步处理序列\noutputs = []\nhidden_states = []\n\nfor i in range(6):  # 遍历序列的每个时间步\n    h_t = rnn_single(x[i], h_t)  # 单步前向传播\n    outputs.append(h_t)\n    hidden_states.append(h_t.clone())  # 保存当前隐藏状态\n\nprint(\"\\\\n处理完成:\")\nprint(\"最终隐藏状态形状:\", h_t.shape)\nprint(\"输出列表长度:\", len(outputs))\nprint(\"每个输出的形状:\", outputs[0].shape)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": "# 可视化隐藏状态的变化\nplt.figure(figsize=(12, 4))\n\n# 计算每个时间步隐藏状态的范数（衡量激活强度）\nnorms = [torch.norm(h).item() for h in hidden_states]\n\nplt.subplot(1, 2, 1)\nplt.plot(norms, 'bo-')\nplt.title('隐藏状态范数随时间变化')\nplt.xlabel('时间步')\nplt.ylabel('L2范数')\nplt.grid(True)\n\n# 可视化隐藏状态的部分维度\nplt.subplot(1, 2, 2)\nfor i in range(min(5, len(hidden_states))):\n    plt.plot(hidden_states[i][0, :10].detach().numpy(), \n             label=f'时间步 {i}', alpha=0.7)\nplt.title('第一个样本前10维隐藏状态')\nplt.xlabel('隐藏单元索引')\nplt.ylabel('激活值')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"观察：隐藏状态在不同时间步下的变化反映了RNN的记忆能力\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "可以看到经过了 RNN 之后，隐藏状态的值已经被改变了，因为网络记忆了序列中的信息，同时输出了6个时间步的结果。\n\n**关键观察**：\n1. **记忆效应**：每个时间步的隐藏状态都包含了之前所有时间步的信息\n2. **状态演化**：隐藏状态随时间逐步演化，体现了序列的动态特性\n3. **信息整合**：最后的隐藏状态包含了整个序列的信息摘要"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 使用 nn.RNN() 简化序列处理\n\n下面我们看看直接使用 `nn.RNN()` 的情况，这种方式更加便捷："
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 定义一个完整的RNN网络\nrnn_seq = nn.RNN(input_size=100, hidden_size=200, num_layers=1, batch_first=False)\n\nprint(\"RNN 网络参数:\")\nprint(\"权重 ih (input to hidden):\", rnn_seq.weight_ih_l0.shape)\nprint(\"权重 hh (hidden to hidden):\", rnn_seq.weight_hh_l0.shape)\nprint(\"偏置 ih:\", rnn_seq.bias_ih_l0.shape if rnn_seq.bias_ih_l0 is not None else \"None\")\nprint(\"偏置 hh:\", rnn_seq.bias_hh_l0.shape if rnn_seq.bias_hh_l0 is not None else \"None\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 查看具体的权重值\nprint(\"Hidden to Hidden 权重矩阵 (前5x5):\")\nprint(rnn_seq.weight_hh_l0[:5, :5])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 使用默认的全0隐藏状态处理序列\noutput, h_final = rnn_seq(x)  # x的形状是 (6, 5, 100)\n\nprint(\"RNN输出:\")\nprint(\"所有时间步输出形状:\", output.shape)  # (seq_len, batch, hidden_size)\nprint(\"最终隐藏状态形状:\", h_final.shape)    # (num_layers, batch, hidden_size)\n\nprint(\"\\\\n详细信息:\")\nprint(\"序列长度:\", output.shape[0])\nprint(\"批次大小:\", output.shape[1])\nprint(\"隐藏层维度:\", output.shape[2])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": "# 验证最后一个时间步的输出和最终隐藏状态是否相同\nprint(\"验证输出一致性:\")\nprint(\"最后时间步的输出:\", output[-1, 0, :5])  # 第一个样本的前5维\nprint(\"最终隐藏状态:\", h_final[0, 0, :5])        # 第一层第一个样本的前5维\nprint(\"是否相等:\", torch.allclose(output[-1], h_final[0]))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "这里的 `h_final` 是网络最后的隐藏状态，网络也输出了6个时间步的结果。\n\n**重要理解**：\n- `output` 包含了所有时间步的隐藏状态\n- `h_final` 只包含最后一个时间步的隐藏状态\n- 对于单层RNN，`output[-1]` 和 `h_final[0]` 是相同的"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": "## 自定义初始隐藏状态\n\n我们也可以不使用默认的隐藏状态，而是自己定义初始隐藏状态：\n\n# 自定义初始隐藏状态\nh_0 = torch.randn(1, 5, 200)  # (num_layers, batch, hidden_size)\nprint(\"自定义初始隐藏状态形状:\", h_0.shape)\n\n# 使用自定义初始状态\noutput_custom, h_final_custom = rnn_seq(x, h_0)\n\nprint(\"\\\\n使用自定义初始状态的结果:\")\nprint(\"输出形状:\", output_custom.shape)\nprint(\"最终隐藏状态形状:\", h_final_custom.shape)\n\n# 比较使用不同初始状态的结果差异\nprint(\"\\\\n初始状态对结果的影响:\")\nprint(\"默认初始化的最终状态前5维:\", h_final[0, 0, :5])\nprint(\"自定义初始化的最终状态前5维:\", h_final_custom[0, 0, :5])\nprint(\"两者差异:\", torch.norm(h_final - h_final_custom).item())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "这里的隐藏状态的大小有三个维度，分别是 `(num_layers * num_directions, batch, hidden_size)`\n\n**维度解释**：\n- `num_layers`: RNN的层数\n- `num_directions`: 方向数（单向为1，双向为2）\n- `batch`: 批次大小\n- `hidden_size`: 隐藏层维度\n\n对于单层单向RNN，第一个维度就是1。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 多层RNN示例\n\n让我们尝试一个更复杂的多层RNN：\n\n# 创建一个2层的RNN\nmulti_layer_rnn = nn.RNN(input_size=100, hidden_size=200, num_layers=2, dropout=0.1)\n\nprint(\"多层RNN参数:\")\nprint(\"第一层 weight_ih:\", multi_layer_rnn.weight_ih_l0.shape)\nprint(\"第一层 weight_hh:\", multi_layer_rnn.weight_hh_l0.shape)\nprint(\"第二层 weight_ih:\", multi_layer_rnn.weight_ih_l1.shape)\nprint(\"第二层 weight_hh:\", multi_layer_rnn.weight_hh_l1.shape)\n\n# 为多层RNN创建初始隐藏状态\nh_0_multi = torch.zeros(2, 5, 200)  # (num_layers=2, batch=5, hidden_size=200)\n\n# 前向传播\noutput_multi, h_final_multi = multi_layer_rnn(x, h_0_multi)\n\nprint(\"\\\\n多层RNN输出:\")\nprint(\"输出形状:\", output_multi.shape)          # (seq_len, batch, hidden_size)\nprint(\"最终隐藏状态形状:\", h_final_multi.shape)  # (num_layers, batch, hidden_size)\n\nprint(\"\\\\n各层最终隐藏状态:\")\nprint(\"第一层最终状态 (前5维):\", h_final_multi[0, 0, :5])\nprint(\"第二层最终状态 (前5维):\", h_final_multi[1, 0, :5])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## RNN的实际应用示例\n\n让我们通过一个简单的序列预测任务来理解RNN的实际应用：\n\n# 创建一个简单的正弦波序列预测任务\ndef generate_sine_data(seq_len=50, num_samples=1000):\n    \\\"\\\"\\\"生成正弦波数据用于序列预测\\\"\\\"\\\"\n    X, y = [], []\n    \n    for i in range(num_samples):\n        # 随机起始点和频率\n        start = np.random.random() * 2 * np.pi\n        freq = np.random.random() * 0.1 + 0.05\n        \n        # 生成序列\n        t = np.linspace(start, start + seq_len * freq, seq_len + 1)\n        data = np.sin(t)\n        \n        X.append(data[:-1])  # 输入序列\n        y.append(data[1:])   # 目标序列（向前移动一位）\n    \n    return torch.FloatTensor(X), torch.FloatTensor(y)\n\n# 生成数据\nseq_len = 20\nX_train, y_train = generate_sine_data(seq_len=seq_len, num_samples=100)\nprint(\"训练数据形状:\")\nprint(\"输入 X:\", X_train.shape)  # (100, 20)\nprint(\"目标 y:\", y_train.shape)  # (100, 20)\n\n# 可视化几个样本\nplt.figure(figsize=(12, 4))\nfor i in range(3):\n    plt.subplot(1, 3, i+1)\n    plt.plot(X_train[i].numpy(), 'b-', label='输入序列', alpha=0.7)\n    plt.plot(y_train[i].numpy(), 'r--', label='目标序列', alpha=0.7)\n    plt.title(f'样本 {i+1}')\n    plt.xlabel('时间步')\n    plt.ylabel('值')\n    plt.legend()\n    plt.grid(True)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": "## 实践建议\n\n### 何时使用 nn.RNN() vs nn.RNNCell()\n\n1. **使用 `nn.RNN()`**：\n   - 处理固定长度的序列\n   - 简化代码，避免手动循环\n   - 需要利用PyTorch的自动优化\n   - 大多数实际应用\n\n2. **使用 `nn.RNNCell()`**：\n   - 需要自定义循环逻辑\n   - 变长序列处理\n   - 需要在每个时间步进行特殊操作\n   - 研究和调试目的\n\n### 常见的RNN配置\n\n一般情况下我们都是用 `nn.RNN()` 而不是 `nn.RNNCell()`，因为 `nn.RNN()` 能够避免我们手动写循环，非常方便。\n\n**推荐设置**：\n- 如果不特别说明，使用默认的全0初始化隐藏状态\n- 对于大多数任务，单层RNN就足够了\n- 如果需要更强的表达能力，考虑使用LSTM或GRU而不是堆叠更多RNN层"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 总结与下一步\n\n### 本节要点\n\n1. **RNN的核心概念**：\n   - 具有\"记忆\"能力的神经网络\n   - 通过隐藏状态在时间步之间传递信息\n   - 参数在所有时间步之间共享\n\n2. **梯度消失问题**：\n   - RNN的主要局限性\n   - 影响长期依赖关系的学习\n   - 需要更好的架构（如LSTM）来解决\n\n3. **PyTorch实现**：\n   - `nn.RNN()` 用于大多数实际应用\n   - `nn.RNNCell()` 用于自定义循环逻辑\n   - 注意输入输出的维度格式\n\n4. **实际应用考虑**：\n   - 数据预处理的重要性\n   - 序列长度对性能的影响\n   - 初始化策略的选择\n\n### 下一步学习\n\n在下一个notebook `LSTM_GRU.ipynb` 中，我们将学习：\n- **LSTM (Long Short-Term Memory)**：如何解决梯度消失问题\n- **GRU (Gated Recurrent Unit)**：LSTM的简化版本\n- **门控机制**：遗忘门、输入门、输出门的工作原理\n- **架构选择**：何时使用RNN、LSTM或GRU\n\n### 练习思考\n\n1. 为什么RNN适合处理序列数据而CNN不适合？\n2. 梯度消失问题的数学本质是什么？\n3. 如何选择RNN的隐藏层维度？\n4. 在什么情况下你会选择使用多层RNN？\n\n## 参考资料\n\n* [Understanding RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) - Andrej Karpathy\n* [机器学习——循环神经网络（RNN）](https://blog.csdn.net/beiye_/article/details/123526075)\n* [循环神经网络讲解（RNN/LSTM/GRU）](https://zhuanlan.zhihu.com/p/123211148)\n* [PyTorch RNN文档](https://pytorch.org/docs/stable/nn.html#rnn)"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, h_t = rnn_seq(x, h_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 ,.,.) = \n",
       "  0.2091  0.0353  0.0625  ...   0.2340  0.3734 -0.1307\n",
       "  0.5498  0.4221  0.7877  ...  -0.4143 -0.1209  0.3335\n",
       "  0.0757  0.4204  0.3826  ...   0.3187 -0.4626 -0.2336\n",
       "  0.3106  0.7355  0.6436  ...   0.6611  0.2587 -0.0338\n",
       "  0.1025  0.6350  0.1943  ...   0.5720  0.8749  0.4525\n",
       "[torch.FloatTensor of size 1x5x200]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 5, 200])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时输出的结果也是 (seq, batch, feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般情况下我们都是用 `nn.RNN()` 而不是 `nn.RNNCell()`，因为 `nn.RNN()` 能够避免我们手动写循环，非常方便，同时如果不特别说明，我们也会选择使用默认的全 0 初始化隐藏状态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考资料\n",
    "* [机器学习——循环神经网络（RNN）](https://blog.csdn.net/beiye_/article/details/123526075)\n",
    "* [循环神经网络讲解（RNN/LSTM/GRU）](https://zhuanlan.zhihu.com/p/123211148)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}