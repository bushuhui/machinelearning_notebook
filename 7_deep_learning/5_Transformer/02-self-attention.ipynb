{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8b4c8d0",
   "metadata": {},
   "source": [
    "# 2. 自注意力机制 (Self-Attention)\n",
    "\n",
    "在上一个教程中，我们学习了注意力机制的基本概念。现在我们将深入学习**自注意力机制**，这是Transformer架构的核心创新点。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f8a2c0",
   "metadata": {},
   "source": [
    "## 2.1 什么是自注意力机制？\n",
    "\n",
    "**传统注意力机制**：Query来自目标序列，Key和Value来自源序列\n",
    "- 例如：机器翻译中，Query来自目标语言，Key和Value来自源语言\n",
    "\n",
    "**自注意力机制**：Query、Key和Value都来自同一个序列\n",
    "- 序列中的每个位置都可以关注序列中的所有位置（包括自己）\n",
    "- 用于捕获序列内部的依赖关系\n",
    "\n",
    "![自注意力机制示意图](images/self_attention.png)\n",
    "\n",
    "### 核心思想：\n",
    "让序列中的每个元素都能够\"看到\"整个序列，并学习到与其他元素的关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e7c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "# 设置随机种子和图表样式\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(f\"PyTorch版本: {torch.__version__}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e7c4f5",
   "metadata": {},
   "source": [
    "## 2.2 自注意力的数学公式\n",
    "\n",
    "自注意力机制的核心是通过线性变换将输入序列映射为Query、Key和Value：\n",
    "\n",
    "$$Q = XW^Q$$\n",
    "$$K = XW^K$$  \n",
    "$$V = XW^V$$\n",
    "\n",
    "然后计算自注意力：\n",
    "\n",
    "$$\\text{SelfAttention}(X) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "其中：\n",
    "- $X \\in \\mathbb{R}^{n \\times d}$ 是输入序列矩阵（n个位置，每个位置d维）\n",
    "- $W^Q, W^K, W^V \\in \\mathbb{R}^{d \\times d_k}$ 是可学习的权重矩阵\n",
    "- $d_k$ 是Query和Key的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b7f4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    自注意力机制的实现\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_k=None):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        if d_k is None:\n",
    "            d_k = d_model\n",
    "            \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        \n",
    "        # 定义Q、K、V的线性变换层\n",
    "        self.W_q = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_k, bias=False)\n",
    "        \n",
    "        # 输出投影层\n",
    "        self.W_o = nn.Linear(d_k, d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        Args:\n",
    "            x: 输入序列 [batch_size, seq_len, d_model]\n",
    "            mask: 注意力掩码 [batch_size, seq_len, seq_len]\n",
    "        \n",
    "        Returns:\n",
    "            output: 自注意力输出 [batch_size, seq_len, d_model]\n",
    "            attention_weights: 注意力权重 [batch_size, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        # 计算Q、K、V\n",
    "        Q = self.W_q(x)  # [batch_size, seq_len, d_k]\n",
    "        K = self.W_k(x)  # [batch_size, seq_len, d_k]\n",
    "        V = self.W_v(x)  # [batch_size, seq_len, d_k]\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        # scores shape: [batch_size, seq_len, seq_len]\n",
    "        \n",
    "        # 应用掩码（如果提供）\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # 计算注意力权重\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # 应用注意力权重\n",
    "        context = torch.matmul(attention_weights, V)\n",
    "        # context shape: [batch_size, seq_len, d_k]\n",
    "        \n",
    "        # 输出投影\n",
    "        output = self.W_o(context)\n",
    "        # output shape: [batch_size, seq_len, d_model]\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# 测试自注意力模块\n",
    "d_model = 64\n",
    "seq_len = 8\n",
    "batch_size = 2\n",
    "\n",
    "# 创建随机输入\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# 创建自注意力层\n",
    "self_attention = SelfAttention(d_model)\n",
    "\n",
    "# 前向传播\n",
    "output, attention_weights = self_attention(x)\n",
    "\n",
    "print(f\"输入形状: {x.shape}\")\n",
    "print(f\"输出形状: {output.shape}\")\n",
    "print(f\"注意力权重形状: {attention_weights.shape}\")\n",
    "print(f\"\\n注意力权重每行的和（应该为1）: {attention_weights[0].sum(dim=-1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8b4a3",
   "metadata": {},
   "source": [
    "## 2.3 可视化自注意力权重\n",
    "\n",
    "让我们可视化自注意力权重矩阵，理解序列中不同位置之间的关注关系："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f7c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_self_attention(attention_weights, tokens=None, title=\"自注意力权重矩阵\"):\n",
    "    \"\"\"\n",
    "    可视化自注意力权重\n",
    "    \n",
    "    Args:\n",
    "        attention_weights: 注意力权重矩阵 [seq_len, seq_len]\n",
    "        tokens: 可选的token列表，用于标注坐标轴\n",
    "        title: 图表标题\n",
    "    \"\"\"\n",
    "    # 如果是批次数据，取第一个样本\n",
    "    if attention_weights.dim() == 3:\n",
    "        weights = attention_weights[0].detach().numpy()\n",
    "    else:\n",
    "        weights = attention_weights.detach().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # 创建热力图\n",
    "    if tokens is not None:\n",
    "        sns.heatmap(weights, annot=True, cmap='Blues', fmt='.3f',\n",
    "                    xticklabels=tokens, yticklabels=tokens)\n",
    "    else:\n",
    "        sns.heatmap(weights, annot=True, cmap='Blues', fmt='.3f',\n",
    "                    xticklabels=[f'位置{i}' for i in range(weights.shape[1])],\n",
    "                    yticklabels=[f'位置{i}' for i in range(weights.shape[0])])\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('被关注的位置 (Key)')\n",
    "    plt.ylabel('查询位置 (Query)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 可视化刚才计算的自注意力权重\n",
    "visualize_self_attention(attention_weights, title=\"自注意力权重可视化\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b8b2f5",
   "metadata": {},
   "source": [
    "## 2.4 实际例子：句子中的自注意力\n",
    "\n",
    "让我们通过一个具体的句子例子来理解自注意力机制如何工作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f8b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个简单的词嵌入模拟\n",
    "def create_word_embeddings():\n",
    "    \"\"\"\n",
    "    创建简单的词嵌入，用于演示\n",
    "    \"\"\"\n",
    "    # 定义一个小词汇表\n",
    "    vocab = ['the', 'cat', 'sat', 'on', 'mat', 'big', 'small']\n",
    "    vocab_size = len(vocab)\n",
    "    embed_dim = 16\n",
    "    \n",
    "    # 创建词嵌入层\n",
    "    embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    \n",
    "    # 词汇表到索引的映射\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    \n",
    "    return embedding, word_to_idx, vocab\n",
    "\n",
    "# 创建词嵌入\n",
    "embedding, word_to_idx, vocab = create_word_embeddings()\n",
    "\n",
    "# 定义一个句子\n",
    "sentence = ['the', 'big', 'cat', 'sat', 'on', 'the', 'small', 'mat']\n",
    "print(f\"原句子: {' '.join(sentence)}\")\n",
    "\n",
    "# 将句子转换为索引\n",
    "try:\n",
    "    sentence_indices = [word_to_idx[word] for word in sentence if word in word_to_idx]\n",
    "    valid_sentence = [word for word in sentence if word in word_to_idx]\n",
    "    print(f\"有效句子: {' '.join(valid_sentence)}\")\n",
    "    print(f\"词索引: {sentence_indices}\")\n",
    "except KeyError as e:\n",
    "    print(f\"词汇表中没有找到词: {e}\")\n",
    "    # 使用一个简化的句子\n",
    "    valid_sentence = ['the', 'big', 'cat', 'sat', 'on', 'the']\n",
    "    sentence_indices = [word_to_idx[word] for word in valid_sentence]\n",
    "    print(f\"使用简化句子: {' '.join(valid_sentence)}\")\n",
    "    print(f\"词索引: {sentence_indices}\")\n",
    "\n",
    "# 获取词嵌入\n",
    "sentence_tensor = torch.tensor(sentence_indices).unsqueeze(0)  # 添加batch维度\n",
    "embedded_sentence = embedding(sentence_tensor)  # [1, seq_len, embed_dim]\n",
    "\n",
    "print(f\"\\n嵌入后的句子形状: {embedded_sentence.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对句子应用自注意力\n",
    "embed_dim = embedded_sentence.size(-1)\n",
    "sentence_self_attention = SelfAttention(embed_dim)\n",
    "\n",
    "# 计算自注意力\n",
    "attended_output, sentence_attention_weights = sentence_self_attention(embedded_sentence)\n",
    "\n",
    "print(f\"自注意力输出形状: {attended_output.shape}\")\n",
    "print(f\"注意力权重形状: {sentence_attention_weights.shape}\")\n",
    "\n",
    "# 可视化句子的自注意力权重\n",
    "visualize_self_attention(\n",
    "    sentence_attention_weights, \n",
    "    tokens=valid_sentence,\n",
    "    title=f\"句子 '{' '.join(valid_sentence)}' 的自注意力权重\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b5c8f6",
   "metadata": {},
   "source": [
    "## 2.5 分析注意力权重\n",
    "\n",
    "让我们详细分析注意力权重，理解模型学到了什么："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e7c8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_attention_patterns(attention_weights, tokens):\n",
    "    \"\"\"\n",
    "    分析注意力模式\n",
    "    \"\"\"\n",
    "    # 取第一个batch的权重\n",
    "    weights = attention_weights[0].detach().numpy()\n",
    "    \n",
    "    print(\"=== 注意力模式分析 ===\")\n",
    "    print(f\"句子: {' '.join(tokens)}\")\n",
    "    print()\n",
    "    \n",
    "    # 找出每个位置最关注的其他位置\n",
    "    for i, token in enumerate(tokens):\n",
    "        # 排除自己，找到最大注意力权重\n",
    "        other_weights = weights[i].copy()\n",
    "        other_weights[i] = 0  # 排除自己\n",
    "        max_idx = np.argmax(other_weights)\n",
    "        max_weight = other_weights[max_idx]\n",
    "        \n",
    "        # 自己的注意力权重\n",
    "        self_weight = weights[i][i]\n",
    "        \n",
    "        print(f\"'{token}' (位置{i}):\")\n",
    "        print(f\"  最关注: '{tokens[max_idx]}' (权重: {max_weight:.3f})\")\n",
    "        print(f\"  自注意力: {self_weight:.3f}\")\n",
    "        print(f\"  是否主要关注自己: {'是' if self_weight > max_weight else '否'}\")\n",
    "        print()\n",
    "\n",
    "# 分析句子的注意力模式\n",
    "analyze_attention_patterns(sentence_attention_weights, valid_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f7c8f8",
   "metadata": {},
   "source": [
    "## 2.6 掩码注意力 (Masked Attention)\n",
    "\n",
    "在某些应用中，我们需要限制注意力的范围。例如：\n",
    "1. **填充掩码（Padding Mask）**：忽略填充位置\n",
    "2. **因果掩码（Causal Mask）**：在语言模型中，防止看到未来的token\n",
    "\n",
    "让我们实现和演示掩码注意力："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f8b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    创建因果掩码（下三角矩阵）\n",
    "    防止模型看到未来的token\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    return mask.unsqueeze(0)  # 添加batch维度\n",
    "\n",
    "def create_padding_mask(seq_len, valid_len):\n",
    "    \"\"\"\n",
    "    创建填充掩码\n",
    "    \"\"\"\n",
    "    mask = torch.zeros(1, seq_len, seq_len)\n",
    "    mask[:, :valid_len, :valid_len] = 1\n",
    "    return mask\n",
    "\n",
    "# 演示因果掩码\n",
    "seq_len = 6\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "\n",
    "print(\"因果掩码（下三角矩阵）:\")\n",
    "print(causal_mask[0].numpy())\n",
    "\n",
    "# 可视化因果掩码\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(causal_mask[0].numpy(), annot=True, cmap='Blues', \n",
    "            xticklabels=[f'位置{i}' for i in range(seq_len)],\n",
    "            yticklabels=[f'位置{i}' for i in range(seq_len)])\n",
    "plt.title('因果掩码（1=可见，0=不可见）')\n",
    "plt.xlabel('被关注的位置')\n",
    "plt.ylabel('查询位置')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示带掩码的自注意力\n",
    "test_input = torch.randn(1, seq_len, 32)  # [batch_size, seq_len, d_model]\n",
    "masked_self_attention = SelfAttention(32)\n",
    "\n",
    "# 不使用掩码\n",
    "output_no_mask, weights_no_mask = masked_self_attention(test_input)\n",
    "\n",
    "# 使用因果掩码\n",
    "output_causal, weights_causal = masked_self_attention(test_input, mask=causal_mask)\n",
    "\n",
    "# 比较可视化\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 无掩码的注意力权重\n",
    "im1 = ax1.imshow(weights_no_mask[0].detach().numpy(), cmap='Blues')\n",
    "ax1.set_title('无掩码的自注意力权重')\n",
    "ax1.set_xlabel('被关注的位置')\n",
    "ax1.set_ylabel('查询位置')\n",
    "\n",
    "# 添加数值标注\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_len):\n",
    "        ax1.text(j, i, f'{weights_no_mask[0][i][j]:.2f}', \n",
    "                ha='center', va='center', fontsize=8)\n",
    "\n",
    "# 有掩码的注意力权重\n",
    "im2 = ax2.imshow(weights_causal[0].detach().numpy(), cmap='Blues')\n",
    "ax2.set_title('带因果掩码的自注意力权重')\n",
    "ax2.set_xlabel('被关注的位置')\n",
    "ax2.set_ylabel('查询位置')\n",
    "\n",
    "# 添加数值标注\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_len):\n",
    "        ax2.text(j, i, f'{weights_causal[0][i][j]:.2f}', \n",
    "                ha='center', va='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"观察：\")\n",
    "print(\"- 无掩码：每个位置都可以关注所有位置\")\n",
    "print(\"- 因果掩码：每个位置只能关注当前位置及之前的位置\")\n",
    "print(\"- 这在语言模型中很重要，防止模型\"作弊\"看到未来的词\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f7c8f9",
   "metadata": {},
   "source": [
    "## 2.7 自注意力 vs 传统注意力\n",
    "\n",
    "让我们总结一下自注意力和传统注意力的区别："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f8b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建对比表格\n",
    "import pandas as pd\n",
    "\n",
    "comparison_data = {\n",
    "    '特性': ['Query来源', 'Key来源', 'Value来源', '主要用途', '优势', '典型应用'],\n",
    "    '传统注意力': [\n",
    "        '目标序列', \n",
    "        '源序列', \n",
    "        '源序列',\n",
    "        '建立不同序列间的关系',\n",
    "        '跨序列信息传递',\n",
    "        '机器翻译、图像标注'\n",
    "    ],\n",
    "    '自注意力': [\n",
    "        '同一序列',\n",
    "        '同一序列', \n",
    "        '同一序列',\n",
    "        '建立序列内部的关系',\n",
    "        '捕获长距离依赖、并行计算',\n",
    "        'Transformer、BERT、GPT'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"传统注意力 vs 自注意力对比\")\n",
    "print(\"=\" * 50)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f7c8f8",
   "metadata": {},
   "source": [
    "## 2.8 自注意力的计算复杂度分析\n",
    "\n",
    "理解自注意力的计算复杂度对于实际应用很重要："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f8b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_complexity():\n",
    "    \"\"\"\n",
    "    分析不同序列长度下的计算复杂度\n",
    "    \"\"\"\n",
    "    seq_lengths = [64, 128, 256, 512, 1024]\n",
    "    d_model = 512\n",
    "    \n",
    "    print(\"自注意力计算复杂度分析\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"模型维度 d_model = {d_model}\")\n",
    "    print()\n",
    "    print(f\"{'序列长度':<10} {'QKV变换':<15} {'注意力计算':<15} {'总FLOPs':<15}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for n in seq_lengths:\n",
    "        # QKV线性变换的FLOPs: 3 * n * d_model^2\n",
    "        qkv_flops = 3 * n * d_model * d_model\n",
    "        \n",
    "        # 注意力计算的FLOPs: n^2 * d_model (for QK^T) + n^2 * d_model (for AV)\n",
    "        attention_flops = 2 * n * n * d_model\n",
    "        \n",
    "        total_flops = qkv_flops + attention_flops\n",
    "        \n",
    "        print(f\"{n:<10} {qkv_flops/1e6:<15.1f} {attention_flops/1e6:<15.1f} {total_flops/1e6:<15.1f}\")\n",
    "    \n",
    "    print(\"\\n注：FLOPs单位为百万次操作 (M)\")\n",
    "    print(\"观察：注意力计算的复杂度随序列长度的平方增长\")\n",
    "    \n",
    "    # 绘制复杂度曲线\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    qkv_complexity = [3 * n * d_model * d_model / 1e6 for n in seq_lengths]\n",
    "    attention_complexity = [2 * n * n * d_model / 1e6 for n in seq_lengths]\n",
    "    \n",
    "    plt.plot(seq_lengths, qkv_complexity, 'b-o', label='QKV线性变换 O(n)')\n",
    "    plt.plot(seq_lengths, attention_complexity, 'r-o', label='注意力计算 O(n²)')\n",
    "    \n",
    "    plt.xlabel('序列长度')\n",
    "    plt.ylabel('计算量 (M FLOPs)')\n",
    "    plt.title('自注意力计算复杂度分析')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "analyze_complexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f7c8f9",
   "metadata": {},
   "source": [
    "## 2.9 练习和实验\n",
    "\n",
    "### 练习1: 实现不同的注意力变体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e7c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    加性注意力机制（相对于点积注意力）\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_k):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_k, bias=False)\n",
    "        \n",
    "        # 加性注意力的参数\n",
    "        self.W_a = nn.Linear(d_k, 1, bias=False)\n",
    "        self.W_o = nn.Linear(d_k, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        Q = self.W_q(x)  # [batch_size, seq_len, d_k]\n",
    "        K = self.W_k(x)  # [batch_size, seq_len, d_k]\n",
    "        V = self.W_v(x)  # [batch_size, seq_len, d_k]\n",
    "        \n",
    "        # 加性注意力：对每对(i,j)计算 W_a^T * tanh(Q_i + K_j)\n",
    "        # 扩展维度进行广播\n",
    "        Q_expanded = Q.unsqueeze(2)  # [batch_size, seq_len, 1, d_k]\n",
    "        K_expanded = K.unsqueeze(1)  # [batch_size, 1, seq_len, d_k]\n",
    "        \n",
    "        # 计算Q + K\n",
    "        combined = Q_expanded + K_expanded  # [batch_size, seq_len, seq_len, d_k]\n",
    "        \n",
    "        # 应用tanh和线性变换\n",
    "        scores = self.W_a(torch.tanh(combined)).squeeze(-1)  # [batch_size, seq_len, seq_len]\n",
    "        \n",
    "        # Softmax\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # 应用注意力\n",
    "        context = torch.matmul(attention_weights, V)\n",
    "        output = self.W_o(context)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# 比较点积注意力和加性注意力\n",
    "d_model = 32\n",
    "d_k = 32\n",
    "seq_len = 6\n",
    "\n",
    "x_test = torch.randn(1, seq_len, d_model)\n",
    "\n",
    "# 点积注意力\n",
    "dot_attention = SelfAttention(d_model, d_k)\n",
    "dot_output, dot_weights = dot_attention(x_test)\n",
    "\n",
    "# 加性注意力\n",
    "add_attention = AdditiveAttention(d_model, d_k)\n",
    "add_output, add_weights = add_attention(x_test)\n",
    "\n",
    "# 可视化比较\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 点积注意力\n",
    "im1 = ax1.imshow(dot_weights[0].detach().numpy(), cmap='Blues')\n",
    "ax1.set_title('点积自注意力')\n",
    "ax1.set_xlabel('Key位置')\n",
    "ax1.set_ylabel('Query位置')\n",
    "\n",
    "# 加性注意力\n",
    "im2 = ax2.imshow(add_weights[0].detach().numpy(), cmap='Blues')\n",
    "ax2.set_title('加性自注意力')\n",
    "ax2.set_xlabel('Key位置')\n",
    "ax2.set_ylabel('Query位置')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"比较结果：\")\n",
    "print(f\"点积注意力输出范围: [{dot_weights.min():.3f}, {dot_weights.max():.3f}]\")\n",
    "print(f\"加性注意力输出范围: [{add_weights.min():.3f}, {add_weights.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f8b5c7",
   "metadata": {},
   "source": [
    "### 练习2: 注意力头的可视化分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_pattern_analysis(attention_weights, title=\"注意力模式分析\"):\n",
    "    \"\"\"\n",
    "    分析注意力权重的统计特性\n",
    "    \"\"\"\n",
    "    weights = attention_weights[0].detach().numpy()\n",
    "    \n",
    "    # 计算统计指标\n",
    "    diagonal_mean = np.mean(np.diag(weights))  # 对角线均值（自注意力）\n",
    "    off_diagonal_mean = np.mean(weights - np.diag(np.diag(weights)))  # 非对角线均值\n",
    "    entropy = -np.sum(weights * np.log(weights + 1e-9), axis=-1).mean()  # 注意力熵\n",
    "    \n",
    "    print(f\"=== {title} ===\")\n",
    "    print(f\"对角线注意力均值（自注意力）: {diagonal_mean:.3f}\")\n",
    "    print(f\"非对角线注意力均值: {off_diagonal_mean:.3f}\")\n",
    "    print(f\"注意力熵（分散程度）: {entropy:.3f}\")\n",
    "    print(f\"是否主要关注自己: {'是' if diagonal_mean > off_diagonal_mean else '否'}\")\n",
    "    print()\n",
    "    \n",
    "    # 可视化注意力分布\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # 1. 注意力热力图\n",
    "    sns.heatmap(weights, annot=True, fmt='.2f', cmap='Blues', ax=ax1)\n",
    "    ax1.set_title('注意力权重矩阵')\n",
    "    \n",
    "    # 2. 对角线 vs 非对角线\n",
    "    diagonal_vals = np.diag(weights)\n",
    "    positions = range(len(diagonal_vals))\n",
    "    ax2.bar(positions, diagonal_vals, alpha=0.7, label='自注意力')\n",
    "    ax2.set_xlabel('位置')\n",
    "    ax2.set_ylabel('注意力权重')\n",
    "    ax2.set_title('各位置的自注意力权重')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # 3. 注意力熵分布\n",
    "    row_entropies = -np.sum(weights * np.log(weights + 1e-9), axis=-1)\n",
    "    ax3.bar(positions, row_entropies, alpha=0.7, color='orange')\n",
    "    ax3.set_xlabel('查询位置')\n",
    "    ax3.set_ylabel('注意力熵')\n",
    "    ax3.set_title('各位置的注意力分散程度')\n",
    "    \n",
    "    # 4. 注意力权重分布直方图\n",
    "    ax4.hist(weights.flatten(), bins=20, alpha=0.7, color='green')\n",
    "    ax4.set_xlabel('注意力权重值')\n",
    "    ax4.set_ylabel('频次')\n",
    "    ax4.set_title('注意力权重分布')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 分析之前计算的注意力权重\n",
    "attention_pattern_analysis(dot_weights, \"点积自注意力模式分析\")\n",
    "attention_pattern_analysis(add_weights, \"加性自注意力模式分析\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f7c8d8",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "在这个教程中，我们深入学习了自注意力机制：\n",
    "\n",
    "### 关键概念：\n",
    "1. **自注意力定义**：Query、Key、Value都来自同一序列\n",
    "2. **数学公式**：$\\text{SelfAttention}(X) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$\n",
    "3. **掩码机制**：控制注意力的可见范围\n",
    "4. **计算复杂度**：$O(n^2 \\cdot d)$，其中n是序列长度\n",
    "\n",
    "### 重要特性：\n",
    "- **并行计算**：所有位置可以同时计算\n",
    "- **长距离依赖**：任意位置间距离为常数\n",
    "- **可解释性**：注意力权重提供直观理解\n",
    "- **灵活性**：可以通过掩码控制注意力范围\n",
    "\n",
    "### 实际应用：\n",
    "- **语言建模**：使用因果掩码防止看到未来\n",
    "- **文本理解**：捕获词语间的语义关系\n",
    "- **序列标注**：利用全局上下文信息\n",
    "\n",
    "自注意力是Transformer架构的基础。在下一个教程中，我们将学习**多头注意力机制（Multi-Head Attention）**，它进一步提升了自注意力的表达能力。\n",
    "\n",
    "### 下一步学习：\n",
    "- [03-multi-head-attention.ipynb](03-multi-head-attention.ipynb) - 多头注意力机制详解"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}