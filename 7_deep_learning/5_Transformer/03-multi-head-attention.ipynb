{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8b4c8d0",
   "metadata": {},
   "source": [
    "# 3. 多头注意力机制 (Multi-Head Attention)\n",
    "\n",
    "在前面的教程中，我们学习了自注意力机制。现在我们将学习**多头注意力机制**，这是Transformer架构中的一个关键改进，它允许模型同时关注不同类型的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f8a2c0",
   "metadata": {},
   "source": [
    "## 3.1 为什么需要多头注意力？\n",
    "\n",
    "单头自注意力有一个局限性：它只能学习一种类型的注意力模式。但是在实际应用中，我们希望模型能够同时关注不同方面的信息：\n",
    "\n",
    "1. **语法关系**：主语、谓语、宾语之间的关系\n",
    "2. **语义关系**：同义词、反义词、相关概念\n",
    "3. **位置关系**：距离远近、前后顺序\n",
    "4. **其他关系**：情感色彩、抽象概念等\n",
    "\n",
    "**多头注意力的核心思想**：让模型并行地学习多种不同的注意力模式，然后将这些信息融合起来。\n",
    "\n",
    "![多头注意力示意图](images/multi_head_attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e7c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "# 设置随机种子和图表样式\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(f\"PyTorch版本: {torch.__version__}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e7c4f5",
   "metadata": {},
   "source": [
    "## 3.2 多头注意力的数学公式\n",
    "\n",
    "多头注意力的计算过程可以分为以下步骤：\n",
    "\n",
    "1. **投影到多个子空间**：\n",
    "   $$Q_i = XW_i^Q, \\quad K_i = XW_i^K, \\quad V_i = XW_i^V$$\n",
    "   其中 $i = 1, 2, ..., h$（h是头的数量）\n",
    "\n",
    "2. **计算每个头的注意力**：\n",
    "   $$\\text{head}_i = \\text{Attention}(Q_i, K_i, V_i) = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i$$\n",
    "\n",
    "3. **拼接和投影**：\n",
    "   $$\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
    "\n",
    "其中：\n",
    "- $d_k = d_{model} / h$ 是每个头的维度\n",
    "- $W_i^Q, W_i^K, W_i^V \\in \\mathbb{R}^{d_{model} \\times d_k}$\n",
    "- $W^O \\in \\mathbb{R}^{d_{model} \\times d_{model}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b7f4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    多头注意力机制的实现\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model必须能被num_heads整除\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # 每个头的维度\n",
    "        \n",
    "        # 定义线性变换层\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Dropout层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, \n",
    "                mask: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        Args:\n",
    "            query: [batch_size, seq_len, d_model]\n",
    "            key: [batch_size, seq_len, d_model]\n",
    "            value: [batch_size, seq_len, d_model]\n",
    "            mask: [batch_size, seq_len, seq_len] 或 None\n",
    "        \n",
    "        Returns:\n",
    "            output: [batch_size, seq_len, d_model]\n",
    "            attention_weights: [batch_size, num_heads, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = query.size()\n",
    "        \n",
    "        # 1. 线性变换得到Q、K、V\n",
    "        Q = self.W_q(query)  # [batch_size, seq_len, d_model]\n",
    "        K = self.W_k(key)    # [batch_size, seq_len, d_model]\n",
    "        V = self.W_v(value)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # 2. 重塑为多头形式\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        # 形状: [batch_size, num_heads, seq_len, d_k]\n",
    "        \n",
    "        # 3. 计算缩放点积注意力\n",
    "        attention_output, attention_weights = self.scaled_dot_product_attention(\n",
    "            Q, K, V, mask\n",
    "        )\n",
    "        \n",
    "        # 4. 拼接多个头\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, d_model\n",
    "        )\n",
    "        \n",
    "        # 5. 最终的线性变换\n",
    "        output = self.W_o(attention_output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        缩放点积注意力\n",
    "        \n",
    "        Args:\n",
    "            Q, K, V: [batch_size, num_heads, seq_len, d_k]\n",
    "            mask: [batch_size, 1, seq_len, seq_len] 或 None\n",
    "        \n",
    "        Returns:\n",
    "            output: [batch_size, num_heads, seq_len, d_k]\n",
    "            attention_weights: [batch_size, num_heads, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        # 计算注意力分数\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # 应用掩码（如果提供）\n",
    "        if mask is not None:\n",
    "            if mask.dim() == 3:  # [batch_size, seq_len, seq_len]\n",
    "                mask = mask.unsqueeze(1)  # [batch_size, 1, seq_len, seq_len]\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # 计算注意力权重\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # 应用注意力权重\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# 测试多头注意力\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "# 创建随机输入\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# 创建多头注意力层\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# 自注意力：query、key、value都是同一个输入\n",
    "output, attention_weights = mha(x, x, x)\n",
    "\n",
    "print(f\"输入形状: {x.shape}\")\n",
    "print(f\"输出形状: {output.shape}\")\n",
    "print(f\"注意力权重形状: {attention_weights.shape}\")\n",
    "print(f\"每个头的维度: {mha.d_k}\")\n",
    "print(f\"头数: {mha.num_heads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8b4a3",
   "metadata": {},
   "source": [
    "## 3.3 可视化多头注意力\n",
    "\n",
    "让我们可视化不同头的注意力模式，观察它们学到的不同关注点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f7c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_multi_head_attention(attention_weights, num_heads_to_show=4, \n",
    "                                   tokens=None, title=\"多头注意力可视化\"):\n",
    "    \"\"\"\n",
    "    可视化多头注意力权重\n",
    "    \n",
    "    Args:\n",
    "        attention_weights: [batch_size, num_heads, seq_len, seq_len]\n",
    "        num_heads_to_show: 显示的头数\n",
    "        tokens: 可选的token列表\n",
    "        title: 图表标题\n",
    "    \"\"\"\n",
    "    # 取第一个批次的数据\n",
    "    weights = attention_weights[0].detach().numpy()  # [num_heads, seq_len, seq_len]\n",
    "    \n",
    "    num_heads_to_show = min(num_heads_to_show, weights.shape[0])\n",
    "    \n",
    "    # 创建子图\n",
    "    fig, axes = plt.subplots(2, (num_heads_to_show + 1) // 2, \n",
    "                             figsize=(5 * ((num_heads_to_show + 1) // 2), 8))\n",
    "    \n",
    "    if num_heads_to_show <= 2:\n",
    "        axes = axes.reshape(2, -1)\n",
    "    \n",
    "    for i in range(num_heads_to_show):\n",
    "        row = i // ((num_heads_to_show + 1) // 2)\n",
    "        col = i % ((num_heads_to_show + 1) // 2)\n",
    "        \n",
    "        # 绘制热力图\n",
    "        if tokens is not None:\n",
    "            sns.heatmap(weights[i], annot=True, fmt='.2f', cmap='Blues',\n",
    "                       xticklabels=tokens, yticklabels=tokens, ax=axes[row, col])\n",
    "        else:\n",
    "            sns.heatmap(weights[i], annot=True, fmt='.2f', cmap='Blues', ax=axes[row, col])\n",
    "        \n",
    "        axes[row, col].set_title(f'头 {i+1}')\n",
    "        axes[row, col].set_xlabel('被关注位置 (Key)')\n",
    "        axes[row, col].set_ylabel('查询位置 (Query)')\n",
    "    \n",
    "    # 隐藏多余的子图\n",
    "    for i in range(num_heads_to_show, axes.size):\n",
    "        row = i // ((num_heads_to_show + 1) // 2)\n",
    "        col = i % ((num_heads_to_show + 1) // 2)\n",
    "        axes[row, col].set_visible(False)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 可视化多头注意力权重\n",
    "visualize_multi_head_attention(attention_weights, num_heads_to_show=4,\n",
    "                              title=\"8头注意力机制的前4个头\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b8b2f5",
   "metadata": {},
   "source": [
    "## 3.4 分析不同头的注意力模式\n",
    "\n",
    "让我们分析不同头学到的注意力模式的特点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f8b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_attention_heads(attention_weights, head_names=None):\n",
    "    \"\"\"\n",
    "    分析不同头的注意力模式特征\n",
    "    \"\"\"\n",
    "    # 取第一个批次的数据\n",
    "    weights = attention_weights[0].detach().numpy()  # [num_heads, seq_len, seq_len]\n",
    "    num_heads, seq_len, _ = weights.shape\n",
    "    \n",
    "    print(\"=== 多头注意力模式分析 ===\")\n",
    "    print(f\"头数: {num_heads}, 序列长度: {seq_len}\")\n",
    "    print()\n",
    "    \n",
    "    # 为每个头计算特征指标\n",
    "    head_features = []\n",
    "    \n",
    "    for i in range(num_heads):\n",
    "        head_weight = weights[i]\n",
    "        \n",
    "        # 计算特征指标\n",
    "        diagonal_mean = np.mean(np.diag(head_weight))  # 自注意力强度\n",
    "        off_diagonal_mean = np.mean(head_weight - np.diag(np.diag(head_weight)))  # 交叉注意力\n",
    "        entropy = -np.sum(head_weight * np.log(head_weight + 1e-9), axis=-1).mean()  # 注意力熵\n",
    "        max_attention = np.max(head_weight)  # 最大注意力值\n",
    "        \n",
    "        # 局部性分析：计算注意力是否集中在邻近位置\n",
    "        locality_score = 0\n",
    "        for row in range(seq_len):\n",
    "            for col in range(seq_len):\n",
    "                distance = abs(row - col)\n",
    "                if distance <= 2:  # 考虑距离2以内的邻近位置\n",
    "                    locality_score += head_weight[row, col]\n",
    "        locality_score /= seq_len\n",
    "        \n",
    "        head_features.append({\n",
    "            'head': i + 1,\n",
    "            'self_attention': diagonal_mean,\n",
    "            'cross_attention': off_diagonal_mean,\n",
    "            'entropy': entropy,\n",
    "            'max_attention': max_attention,\n",
    "            'locality': locality_score\n",
    "        })\n",
    "        \n",
    "        # 确定头的类型\n",
    "        if diagonal_mean > 0.3:\n",
    "            head_type = \"自关注型\"\n",
    "        elif locality_score > 0.5:\n",
    "            head_type = \"局部型\"\n",
    "        elif entropy > 2.0:\n",
    "            head_type = \"全局型\"\n",
    "        else:\n",
    "            head_type = \"混合型\"\n",
    "        \n",
    "        print(f\"头 {i+1} ({head_type}):\")\n",
    "        print(f\"  自注意力: {diagonal_mean:.3f}\")\n",
    "        print(f\"  交叉注意力: {off_diagonal_mean:.3f}\")\n",
    "        print(f\"  注意力熵: {entropy:.3f}\")\n",
    "        print(f\"  局部性得分: {locality_score:.3f}\")\n",
    "        print(f\"  最大注意力: {max_attention:.3f}\")\n",
    "        print()\n",
    "    \n",
    "    return head_features\n",
    "\n",
    "# 分析注意力头\n",
    "head_features = analyze_attention_heads(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f7c8f6",
   "metadata": {},
   "source": [
    "## 3.5 对比单头 vs 多头注意力\n",
    "\n",
    "让我们直接比较单头注意力和多头注意力的效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b3b5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单头注意力类（从之前的教程）\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(SingleHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_model)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        return output, attention_weights.unsqueeze(1)  # 添加头维度以便比较\n",
    "\n",
    "# 对比实验\n",
    "d_model = 64  # 使用较小的维度便于可视化\n",
    "seq_len = 8\n",
    "batch_size = 1\n",
    "\n",
    "# 创建测试输入\n",
    "test_input = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# 单头注意力\n",
    "single_head = SingleHeadAttention(d_model)\n",
    "single_output, single_weights = single_head(test_input)\n",
    "\n",
    "# 多头注意力\n",
    "multi_head = MultiHeadAttention(d_model, num_heads=4)\n",
    "multi_output, multi_weights = multi_head(test_input, test_input, test_input)\n",
    "\n",
    "print(f\"单头注意力输出形状: {single_output.shape}\")\n",
    "print(f\"多头注意力输出形状: {multi_output.shape}\")\n",
    "print(f\"单头注意力权重形状: {single_weights.shape}\")\n",
    "print(f\"多头注意力权重形状: {multi_weights.shape}\")\n",
    "\n",
    "# 可视化对比\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# 单头注意力\n",
    "sns.heatmap(single_weights[0, 0].detach().numpy(), annot=True, fmt='.2f', \n",
    "           cmap='Blues', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('单头注意力')\n",
    "\n",
    "# 多头注意力的前两个头\n",
    "for i in range(2):\n",
    "    sns.heatmap(multi_weights[0, i].detach().numpy(), annot=True, fmt='.2f',\n",
    "               cmap='Blues', ax=axes[0, i+1])\n",
    "    axes[0, i+1].set_title(f'多头注意力 - 头{i+1}')\n",
    "\n",
    "# 平均注意力权重对比\n",
    "single_avg = single_weights[0, 0].detach().numpy()\n",
    "multi_avg = multi_weights[0].mean(dim=0).detach().numpy()\n",
    "\n",
    "sns.heatmap(single_avg, annot=True, fmt='.2f', cmap='Reds', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('单头注意力权重')\n",
    "\n",
    "sns.heatmap(multi_avg, annot=True, fmt='.2f', cmap='Reds', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('多头注意力平均权重')\n",
    "\n",
    "# 差异可视化\n",
    "diff = multi_avg - single_avg\n",
    "sns.heatmap(diff, annot=True, fmt='.2f', cmap='RdBu_r', center=0, ax=axes[1, 2])\n",
    "axes[1, 2].set_title('差异 (多头 - 单头)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f7c8f8",
   "metadata": {},
   "source": [
    "## 3.6 多头注意力的变体\n",
    "\n",
    "让我们实现几种多头注意力的变体："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    多查询注意力（Multi-Query Attention）\n",
    "    所有头共享同一个Key和Value，但有独立的Query\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiQueryAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # 多个Query投影，但只有一个Key和Value投影\n",
    "        self.W_q = nn.Linear(d_model, d_model)  # 多个Query\n",
    "        self.W_k = nn.Linear(d_model, self.d_k)  # 单个Key\n",
    "        self.W_v = nn.Linear(d_model, self.d_k)  # 单个Value\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query, key, value):\n",
    "        batch_size, seq_len, _ = query.size()\n",
    "        \n",
    "        # 多个Query\n",
    "        Q = self.W_q(query).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # 单个Key和Value，广播到所有头\n",
    "        K = self.W_k(key).unsqueeze(1).expand(batch_size, self.num_heads, seq_len, self.d_k)\n",
    "        V = self.W_v(value).unsqueeze(1).expand(batch_size, self.num_heads, seq_len, self.d_k)\n",
    "        \n",
    "        # 计算注意力\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # 拼接和投影\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    分组查询注意力（Grouped Query Attention）\n",
    "    将头分为几组，每组共享Key和Value\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, num_kv_heads):\n",
    "        super(GroupedQueryAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0\n",
    "        assert num_heads % num_kv_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.d_kv = d_model // num_kv_heads\n",
    "        self.group_size = num_heads // num_kv_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, self.d_kv * num_kv_heads)\n",
    "        self.W_v = nn.Linear(d_model, self.d_kv * num_kv_heads)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query, key, value):\n",
    "        batch_size, seq_len, _ = query.size()\n",
    "        \n",
    "        # Query: [batch_size, num_heads, seq_len, d_k]\n",
    "        Q = self.W_q(query).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Key和Value: [batch_size, num_kv_heads, seq_len, d_kv]\n",
    "        K = self.W_k(key).view(batch_size, seq_len, self.num_kv_heads, self.d_kv).transpose(1, 2)\n",
    "        V = self.W_v(value).view(batch_size, seq_len, self.num_kv_heads, self.d_kv).transpose(1, 2)\n",
    "        \n",
    "        # 将Key和Value复制到对应的Query组\n",
    "        K = K.repeat_interleave(self.group_size, dim=1)\n",
    "        V = V.repeat_interleave(self.group_size, dim=1)\n",
    "        \n",
    "        # 计算注意力\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # 拼接和投影\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# 测试不同的注意力变体\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "seq_len = 6\n",
    "batch_size = 1\n",
    "\n",
    "test_input = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# 标准多头注意力\n",
    "standard_mha = MultiHeadAttention(d_model, num_heads)\n",
    "standard_out, standard_weights = standard_mha(test_input, test_input, test_input)\n",
    "\n",
    "# 多查询注意力\n",
    "mqa = MultiQueryAttention(d_model, num_heads)\n",
    "mqa_out, mqa_weights = mqa(test_input, test_input, test_input)\n",
    "\n",
    "# 分组查询注意力\n",
    "gqa = GroupedQueryAttention(d_model, num_heads, num_kv_heads=2)\n",
    "gqa_out, gqa_weights = gqa(test_input, test_input, test_input)\n",
    "\n",
    "print(\"注意力变体对比:\")\n",
    "print(f\"标准多头注意力参数量: {sum(p.numel() for p in standard_mha.parameters()):,}\")\n",
    "print(f\"多查询注意力参数量: {sum(p.numel() for p in mqa.parameters()):,}\")\n",
    "print(f\"分组查询注意力参数量: {sum(p.numel() for p in gqa.parameters()):,}\")\n",
    "\n",
    "print(f\"\\n输出形状对比:\")\n",
    "print(f\"标准多头: {standard_out.shape}\")\n",
    "print(f\"多查询: {mqa_out.shape}\")\n",
    "print(f\"分组查询: {gqa_out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f7c8f9",
   "metadata": {},
   "source": [
    "## 3.7 注意力头的多样性分析\n",
    "\n",
    "让我们分析多头注意力中不同头之间的多样性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f8b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_head_diversity(attention_weights):\n",
    "    \"\"\"\n",
    "    分析注意力头之间的多样性\n",
    "    \"\"\"\n",
    "    # 取第一个批次的数据\n",
    "    weights = attention_weights[0].detach().numpy()  # [num_heads, seq_len, seq_len]\n",
    "    num_heads, seq_len, _ = weights.shape\n",
    "    \n",
    "    print(\"=== 注意力头多样性分析 ===\")\n",
    "    \n",
    "    # 计算头之间的相似性矩阵\n",
    "    similarity_matrix = np.zeros((num_heads, num_heads))\n",
    "    \n",
    "    for i in range(num_heads):\n",
    "        for j in range(num_heads):\n",
    "            # 使用余弦相似度\n",
    "            flat_i = weights[i].flatten()\n",
    "            flat_j = weights[j].flatten()\n",
    "            \n",
    "            dot_product = np.dot(flat_i, flat_j)\n",
    "            norm_i = np.linalg.norm(flat_i)\n",
    "            norm_j = np.linalg.norm(flat_j)\n",
    "            \n",
    "            similarity_matrix[i, j] = dot_product / (norm_i * norm_j)\n",
    "    \n",
    "    # 可视化相似性矩阵\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # 相似性热力图\n",
    "    sns.heatmap(similarity_matrix, annot=True, fmt='.3f', cmap='RdYlBu_r',\n",
    "                xticklabels=[f'头{i+1}' for i in range(num_heads)],\n",
    "                yticklabels=[f'头{i+1}' for i in range(num_heads)], ax=ax1)\n",
    "    ax1.set_title('注意力头相似性矩阵')\n",
    "    \n",
    "    # 多样性指标\n",
    "    off_diagonal = similarity_matrix[~np.eye(num_heads, dtype=bool)]\n",
    "    avg_similarity = np.mean(off_diagonal)\n",
    "    diversity_score = 1 - avg_similarity  # 多样性得分\n",
    "    \n",
    "    # 绘制相似性分布\n",
    "    ax2.hist(off_diagonal, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax2.axvline(avg_similarity, color='red', linestyle='--', \n",
    "                label=f'平均相似性: {avg_similarity:.3f}')\n",
    "    ax2.set_xlabel('头间相似性')\n",
    "    ax2.set_ylabel('频次')\n",
    "    ax2.set_title('头间相似性分布')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"平均头间相似性: {avg_similarity:.3f}\")\n",
    "    print(f\"多样性得分: {diversity_score:.3f}\")\n",
    "    print(f\"最相似的头对: 头{np.unravel_index(np.argmax(similarity_matrix - np.eye(num_heads)), similarity_matrix.shape)}\")\n",
    "    \n",
    "    # 计算每个头的独特性\n",
    "    uniqueness_scores = []\n",
    "    for i in range(num_heads):\n",
    "        others_similarity = [similarity_matrix[i, j] for j in range(num_heads) if i != j]\n",
    "        uniqueness = 1 - np.mean(others_similarity)\n",
    "        uniqueness_scores.append(uniqueness)\n",
    "    \n",
    "    print(\"\\n各头的独特性得分:\")\n",
    "    for i, score in enumerate(uniqueness_scores):\n",
    "        print(f\"头{i+1}: {score:.3f}\")\n",
    "    \n",
    "    return similarity_matrix, diversity_score\n",
    "\n",
    "# 分析之前计算的多头注意力\n",
    "similarity_matrix, diversity_score = analyze_head_diversity(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f7c8f9",
   "metadata": {},
   "source": [
    "## 3.8 多头注意力的计算效率\n",
    "\n",
    "让我们分析多头注意力的计算效率："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e7c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_attention_variants():\n",
    "    \"\"\"\n",
    "    基准测试不同注意力变体的性能\n",
    "    \"\"\"\n",
    "    d_model = 512\n",
    "    seq_len = 128\n",
    "    batch_size = 16\n",
    "    num_heads = 8\n",
    "    \n",
    "    # 创建测试数据\n",
    "    test_data = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # 创建不同的注意力模型\n",
    "    models = {\n",
    "        '单头注意力': SingleHeadAttention(d_model),\n",
    "        '标准多头注意力': MultiHeadAttention(d_model, num_heads),\n",
    "        '多查询注意力': MultiQueryAttention(d_model, num_heads),\n",
    "        '分组查询注意力': GroupedQueryAttention(d_model, num_heads, num_kv_heads=2)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(\"=== 注意力变体性能基准测试 ===\")\n",
    "    print(f\"测试配置: batch_size={batch_size}, seq_len={seq_len}, d_model={d_model}\")\n",
    "    print()\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        model.eval()\n",
    "        \n",
    "        # 预热\n",
    "        for _ in range(5):\n",
    "            if name == '单头注意力':\n",
    "                _ = model(test_data)\n",
    "            else:\n",
    "                _ = model(test_data, test_data, test_data)\n",
    "        \n",
    "        # 正式测试\n",
    "        num_runs = 50\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for _ in range(num_runs):\n",
    "            if name == '单头注意力':\n",
    "                output, weights = model(test_data)\n",
    "            else:\n",
    "                output, weights = model(test_data, test_data, test_data)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        avg_time = (end_time - start_time) / num_runs * 1000  # 转换为毫秒\n",
    "        \n",
    "        # 计算参数量\n",
    "        num_params = sum(p.numel() for p in model.parameters())\n",
    "        \n",
    "        results[name] = {\n",
    "            'time': avg_time,\n",
    "            'params': num_params,\n",
    "            'output_shape': output.shape\n",
    "        }\n",
    "        \n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  平均推理时间: {avg_time:.2f} ms\")\n",
    "        print(f\"  参数量: {num_params:,}\")\n",
    "        print(f\"  输出形状: {output.shape}\")\n",
    "        print()\n",
    "    \n",
    "    # 可视化结果\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    names = list(results.keys())\n",
    "    times = [results[name]['time'] for name in names]\n",
    "    params = [results[name]['params'] for name in names]\n",
    "    \n",
    "    # 推理时间对比\n",
    "    bars1 = ax1.bar(names, times, color=['red', 'blue', 'green', 'orange'])\n",
    "    ax1.set_ylabel('推理时间 (ms)')\n",
    "    ax1.set_title('不同注意力机制的推理时间对比')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 在柱状图上添加数值\n",
    "    for bar, time_val in zip(bars1, times):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                f'{time_val:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 参数量对比\n",
    "    bars2 = ax2.bar(names, [p/1000 for p in params], color=['red', 'blue', 'green', 'orange'])\n",
    "    ax2.set_ylabel('参数量 (K)')\n",
    "    ax2.set_title('不同注意力机制的参数量对比')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 在柱状图上添加数值\n",
    "    for bar, param_val in zip(bars2, params):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10,\n",
    "                f'{param_val/1000:.1f}K', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 运行基准测试\n",
    "benchmark_results = benchmark_attention_variants()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f7c8f9",
   "metadata": {},
   "source": [
    "## 3.9 练习和实验\n",
    "\n",
    "### 练习1：实现位置感知的多头注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e7c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalMultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    位置感知的多头注意力\n",
    "    在计算注意力时考虑位置信息\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, max_seq_len=1000):\n",
    "        super(PositionalMultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # 标准的Q、K、V投影\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # 位置偏置参数\n",
    "        self.position_bias = nn.Parameter(torch.randn(num_heads, max_seq_len, max_seq_len))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # 计算Q、K、V\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # 计算内容注意力分数\n",
    "        content_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # 添加位置偏置\n",
    "        position_scores = self.position_bias[:, :seq_len, :seq_len].unsqueeze(0)\n",
    "        \n",
    "        # 总注意力分数\n",
    "        total_scores = content_scores + position_scores\n",
    "        \n",
    "        # 计算注意力权重\n",
    "        attention_weights = F.softmax(total_scores, dim=-1)\n",
    "        \n",
    "        # 应用注意力\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # 拼接和投影\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# 测试位置感知注意力\n",
    "pos_mha = PositionalMultiHeadAttention(d_model=64, num_heads=4)\n",
    "test_input = torch.randn(1, 8, 64)\n",
    "\n",
    "pos_output, pos_weights = pos_mha(test_input)\n",
    "print(f\"位置感知多头注意力输出形状: {pos_output.shape}\")\n",
    "print(f\"位置感知注意力权重形状: {pos_weights.shape}\")\n",
    "\n",
    "# 可视化位置偏置\n",
    "position_bias = pos_mha.position_bias[:4, :8, :8].detach().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(4):\n",
    "    sns.heatmap(position_bias[i], annot=True, fmt='.2f', cmap='RdBu_r',\n",
    "                center=0, ax=axes[i])\n",
    "    axes[i].set_title(f'头{i+1}的位置偏置')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f7c8d8",
   "metadata": {},
   "source": [
    "### 练习2：注意力头功能分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def functional_analysis_of_heads(attention_weights, tokens=None):\n",
    "    \"\"\"\n",
    "    分析不同注意力头的功能特化\n",
    "    \"\"\"\n",
    "    weights = attention_weights[0].detach().numpy()  # [num_heads, seq_len, seq_len]\n",
    "    num_heads, seq_len, _ = weights.shape\n",
    "    \n",
    "    print(\"=== 注意力头功能分析 ===\")\n",
    "    \n",
    "    for head_idx in range(num_heads):\n",
    "        head_weight = weights[head_idx]\n",
    "        \n",
    "        # 分析注意力模式\n",
    "        diagonal_strength = np.mean(np.diag(head_weight))\n",
    "        \n",
    "        # 分析是否关注相邻位置\n",
    "        adjacent_strength = 0\n",
    "        for i in range(seq_len - 1):\n",
    "            adjacent_strength += head_weight[i, i+1] + head_weight[i+1, i]\n",
    "        adjacent_strength /= (2 * (seq_len - 1))\n",
    "        \n",
    "        # 分析是否有全局注意力模式\n",
    "        global_strength = np.mean(head_weight) - diagonal_strength\n",
    "        \n",
    "        # 分析注意力的方向性（前向 vs 后向）\n",
    "        forward_strength = 0\n",
    "        backward_strength = 0\n",
    "        \n",
    "        for i in range(seq_len):\n",
    "            for j in range(seq_len):\n",
    "                if i < j:  # 前向\n",
    "                    forward_strength += head_weight[i, j]\n",
    "                elif i > j:  # 后向\n",
    "                    backward_strength += head_weight[i, j]\n",
    "        \n",
    "        forward_strength /= (seq_len * (seq_len - 1) / 2)\n",
    "        backward_strength /= (seq_len * (seq_len - 1) / 2)\n",
    "        \n",
    "        # 确定头的功能类型\n",
    "        if diagonal_strength > 0.4:\n",
    "            head_type = \"自关注型\"\n",
    "        elif adjacent_strength > 0.3:\n",
    "            head_type = \"邻接型\"\n",
    "        elif forward_strength > backward_strength * 1.5:\n",
    "            head_type = \"前向型\"\n",
    "        elif backward_strength > forward_strength * 1.5:\n",
    "            head_type = \"后向型\"\n",
    "        else:\n",
    "            head_type = \"全局型\"\n",
    "        \n",
    "        print(f\"\\n头 {head_idx + 1} - {head_type}:\")\n",
    "        print(f\"  自关注强度: {diagonal_strength:.3f}\")\n",
    "        print(f\"  邻接关注强度: {adjacent_strength:.3f}\")\n",
    "        print(f\"  全局关注强度: {global_strength:.3f}\")\n",
    "        print(f\"  前向关注强度: {forward_strength:.3f}\")\n",
    "        print(f\"  后向关注强度: {backward_strength:.3f}\")\n",
    "    \n",
    "    # 可视化头的功能分布\n",
    "    fig, axes = plt.subplots(2, (num_heads + 1) // 2, figsize=(15, 8))\n",
    "    if num_heads <= 2:\n",
    "        axes = axes.reshape(2, -1)\n",
    "    \n",
    "    for i in range(num_heads):\n",
    "        row = i // ((num_heads + 1) // 2)\n",
    "        col = i % ((num_heads + 1) // 2)\n",
    "        \n",
    "        # 绘制注意力模式\n",
    "        im = axes[row, col].imshow(weights[i], cmap='Blues')\n",
    "        axes[row, col].set_title(f'头 {i+1}')\n",
    "        \n",
    "        # 添加网格线突出对角线和邻接线\n",
    "        axes[row, col].plot([0, seq_len-1], [0, seq_len-1], 'r--', alpha=0.5, linewidth=1)\n",
    "        if seq_len > 1:\n",
    "            axes[row, col].plot([0, seq_len-2], [1, seq_len-1], 'g--', alpha=0.5, linewidth=1)\n",
    "            axes[row, col].plot([1, seq_len-1], [0, seq_len-2], 'g--', alpha=0.5, linewidth=1)\n",
    "    \n",
    "    # 隐藏多余的子图\n",
    "    for i in range(num_heads, axes.size):\n",
    "        row = i // ((num_heads + 1) // 2)\n",
    "        col = i % ((num_heads + 1) // 2)\n",
    "        axes[row, col].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 分析之前计算的注意力头功能\n",
    "functional_analysis_of_heads(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f8b5c7",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "在这个教程中，我们深入学习了多头注意力机制：\n",
    "\n",
    "### 关键概念：\n",
    "1. **多头机制**：并行学习多种注意力模式\n",
    "2. **子空间投影**：将输入投影到不同的表示子空间\n",
    "3. **注意力融合**：拼接多个头的输出并投影\n",
    "4. **头的多样性**：不同头学习不同类型的关系\n",
    "\n",
    "### 数学公式：\n",
    "$$\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
    "$$\\text{where } \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
    "\n",
    "### 重要优势：\n",
    "- **表达能力**：能够捕获多种类型的依赖关系\n",
    "- **并行性**：所有头可以并行计算\n",
    "- **专业化**：不同头可以专注于不同的语言现象\n",
    "- **鲁棒性**：多个头提供冗余和互补信息\n",
    "\n",
    "### 变体和优化：\n",
    "- **多查询注意力（MQA）**：减少KV参数，提高效率\n",
    "- **分组查询注意力（GQA）**：在效率和性能间平衡\n",
    "- **位置感知注意力**：显式建模位置信息\n",
    "\n",
    "### 实际应用洞察：\n",
    "- 不同的头往往学习不同的语言学功能\n",
    "- 有些头关注语法关系，有些关注语义关系\n",
    "- 头的多样性是模型性能的重要指标\n",
    "\n",
    "多头注意力是Transformer强大表达能力的关键来源。在下一个教程中，我们将学习**位置编码（Positional Encoding）**，它解决了注意力机制中缺乏位置信息的问题。\n",
    "\n",
    "### 下一步学习：\n",
    "- [04-positional-encoding.ipynb](04-positional-encoding.ipynb) - 位置编码详解"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}