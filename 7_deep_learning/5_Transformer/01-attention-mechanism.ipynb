{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8c35a4a",
   "metadata": {},
   "source": [
    "# 1. 注意力机制基础\n",
    "\n",
    "在深入学习Transformer之前，我们需要理解注意力机制（Attention Mechanism）的基本概念。注意力机制是Transformer架构的核心组件。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e4a5c6",
   "metadata": {},
   "source": [
    "## 1.1 传统RNN的局限性\n",
    "\n",
    "让我们先回顾一下传统RNN在序列处理中的问题：\n",
    "\n",
    "1. **串行计算**：RNN必须按顺序处理序列，无法并行化\n",
    "2. **长期依赖问题**：随着序列长度增加，早期信息容易丢失\n",
    "3. **信息瓶颈**：所有信息都必须压缩到最后一个隐藏状态中\n",
    "\n",
    "![RNN问题示意图](images/rnn_problems.png)\n",
    "\n",
    "注意力机制的提出就是为了解决这些问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89c8a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# 设置中文字体和图表样式\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(f\"PyTorch版本: {torch.__version__}\")\n",
    "print(f\"是否有CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a5f4b3",
   "metadata": {},
   "source": [
    "## 1.2 注意力机制的直观理解\n",
    "\n",
    "注意力机制模拟了人类的注意力过程。当我们看一张图片或读一段文字时，我们会把注意力集中在最相关的部分。\n",
    "\n",
    "**例子**：翻译句子 \"The cat sat on the mat\" 为中文时，当我们翻译\"cat\"这个词时，我们主要关注原句中的\"cat\"，而不是其他词。\n",
    "\n",
    "注意力机制的核心思想：\n",
    "- **查询（Query）**：我们想要关注什么\n",
    "- **键（Key）**：用于匹配查询的索引\n",
    "- **值（Value）**：实际的信息内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e3f4b5",
   "metadata": {},
   "source": [
    "## 1.3 数学公式\n",
    "\n",
    "注意力机制的基本计算过程可以用以下公式表示：\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "其中：\n",
    "- $Q$ 是查询矩阵 (queries)\n",
    "- $K$ 是键矩阵 (keys)\n",
    "- $V$ 是值矩阵 (values)\n",
    "- $d_k$ 是键向量的维度\n",
    "- $\\sqrt{d_k}$ 是缩放因子，防止softmax饱和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b5c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_attention(query, key, value):\n",
    "    \"\"\"\n",
    "    简单的注意力机制实现\n",
    "    \n",
    "    Args:\n",
    "        query: 查询向量 [batch_size, seq_len, d_model]\n",
    "        key: 键向量 [batch_size, seq_len, d_model]\n",
    "        value: 值向量 [batch_size, seq_len, d_model]\n",
    "    \n",
    "    Returns:\n",
    "        output: 注意力输出\n",
    "        attention_weights: 注意力权重\n",
    "    \"\"\"\n",
    "    # 计算注意力分数 (query与key的点积)\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(d_k)\n",
    "    \n",
    "    # 应用softmax获得注意力权重\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # 计算加权输出\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# 测试简单注意力机制\n",
    "batch_size = 1\n",
    "seq_len = 4\n",
    "d_model = 6\n",
    "\n",
    "# 创建示例输入\n",
    "torch.manual_seed(42)\n",
    "query = torch.randn(batch_size, seq_len, d_model)\n",
    "key = torch.randn(batch_size, seq_len, d_model)\n",
    "value = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, attention_weights = simple_attention(query, key, value)\n",
    "\n",
    "print(f\"输入形状:\")\n",
    "print(f\"Query: {query.shape}\")\n",
    "print(f\"Key: {key.shape}\")\n",
    "print(f\"Value: {value.shape}\")\n",
    "print(f\"\\n输出形状:\")\n",
    "print(f\"Output: {output.shape}\")\n",
    "print(f\"Attention weights: {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f8a2f9",
   "metadata": {},
   "source": [
    "## 1.4 可视化注意力权重\n",
    "\n",
    "让我们可视化注意力权重矩阵，理解不同位置之间的关注程度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b9a1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(attention_weights, title=\"注意力权重热力图\"):\n",
    "    \"\"\"\n",
    "    可视化注意力权重矩阵\n",
    "    \"\"\"\n",
    "    # 取第一个batch的注意力权重\n",
    "    weights = attention_weights[0].detach().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(weights, annot=True, cmap='Blues', fmt='.3f',\n",
    "                xticklabels=[f'位置{i}' for i in range(weights.shape[1])],\n",
    "                yticklabels=[f'查询{i}' for i in range(weights.shape[0])])\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Key位置')\n",
    "    plt.ylabel('Query位置')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 可视化之前计算的注意力权重\n",
    "visualize_attention(attention_weights, \"简单注意力机制权重可视化\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b7f4b2",
   "metadata": {},
   "source": [
    "## 1.5 注意力机制的实际应用示例\n",
    "\n",
    "让我们通过一个文本分类的例子来理解注意力机制的作用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b3b5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    带有注意力机制的简单分类器\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super(AttentionClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # 注意力层\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        # 分类层\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 词嵌入\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_len, embed_dim]\n",
    "        \n",
    "        # LSTM编码\n",
    "        lstm_out, _ = self.lstm(embedded)  # [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # 计算注意力权重\n",
    "        attention_scores = self.attention(lstm_out)  # [batch_size, seq_len, 1]\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)  # [batch_size, seq_len, 1]\n",
    "        \n",
    "        # 加权求和\n",
    "        weighted_output = torch.sum(lstm_out * attention_weights, dim=1)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # 分类\n",
    "        logits = self.classifier(weighted_output)\n",
    "        \n",
    "        return logits, attention_weights.squeeze(-1)\n",
    "\n",
    "# 创建模型实例\n",
    "vocab_size = 1000\n",
    "embed_dim = 128\n",
    "hidden_dim = 64\n",
    "num_classes = 2\n",
    "\n",
    "model = AttentionClassifier(vocab_size, embed_dim, hidden_dim, num_classes)\n",
    "\n",
    "# 模拟一些数据\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "sample_input = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "# 前向传播\n",
    "logits, attention_weights = model(sample_input)\n",
    "\n",
    "print(f\"输入形状: {sample_input.shape}\")\n",
    "print(f\"输出logits形状: {logits.shape}\")\n",
    "print(f\"注意力权重形状: {attention_weights.shape}\")\n",
    "print(f\"\\n每个样本的注意力权重和: {attention_weights.sum(dim=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e7c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化分类器的注意力权重\n",
    "def visualize_sequence_attention(attention_weights, sample_idx=0):\n",
    "    \"\"\"\n",
    "    可视化序列中每个位置的注意力权重\n",
    "    \"\"\"\n",
    "    weights = attention_weights[sample_idx].detach().numpy()\n",
    "    positions = range(len(weights))\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    bars = plt.bar(positions, weights, alpha=0.7, color='skyblue')\n",
    "    plt.xlabel('序列位置')\n",
    "    plt.ylabel('注意力权重')\n",
    "    plt.title(f'样本 {sample_idx} 的注意力权重分布')\n",
    "    plt.xticks(positions)\n",
    "    \n",
    "    # 在柱状图上添加数值\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{weights[i]:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 可视化两个样本的注意力权重\n",
    "for i in range(min(2, batch_size)):\n",
    "    visualize_sequence_attention(attention_weights, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f7c8e5",
   "metadata": {},
   "source": [
    "## 1.6 注意力机制的优势\n",
    "\n",
    "通过上面的例子，我们可以看到注意力机制的几个关键优势：\n",
    "\n",
    "1. **并行计算**：不同位置的注意力可以同时计算\n",
    "2. **长距离依赖**：任意两个位置之间的距离都是常数\n",
    "3. **可解释性**：注意力权重提供了模型关注点的直观解释\n",
    "4. **灵活性**：可以根据任务需求调整注意力的计算方式\n",
    "\n",
    "### 与传统方法的对比：\n",
    "\n",
    "| 特性 | RNN/LSTM | 注意力机制 |\n",
    "|------|----------|------------|\n",
    "| 计算复杂度 | O(n) | O(n²) |\n",
    "| 并行性 | 串行 | 并行 |\n",
    "| 长距离依赖 | 困难 | 容易 |\n",
    "| 可解释性 | 低 | 高 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b7c4f7",
   "metadata": {},
   "source": [
    "## 1.7 练习和思考\n",
    "\n",
    "1. **代码实践**：尝试修改上面的`simple_attention`函数，添加温度参数来控制注意力的尖锐程度\n",
    "\n",
    "2. **理论思考**：为什么要除以$\\sqrt{d_k}$？尝试不加这个缩放因子，观察结果有什么变化\n",
    "\n",
    "3. **应用思考**：注意力机制除了在NLP中应用，还可以应用在哪些领域？\n",
    "\n",
    "4. **实验扩展**：尝试在不同的数据集上应用带注意力的分类器，观察注意力权重的分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f8b2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 练习1：带温度参数的注意力机制\n",
    "def attention_with_temperature(query, key, value, temperature=1.0):\n",
    "    \"\"\"\n",
    "    带温度参数的注意力机制\n",
    "    \n",
    "    Args:\n",
    "        temperature: 温度参数，控制注意力的尖锐程度\n",
    "                    temperature > 1: 更平滑的注意力分布\n",
    "                    temperature < 1: 更尖锐的注意力分布\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(d_k)\n",
    "    \n",
    "    # 应用温度参数\n",
    "    scores = scores / temperature\n",
    "    \n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# 比较不同温度下的注意力分布\n",
    "temperatures = [0.5, 1.0, 2.0]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i, temp in enumerate(temperatures):\n",
    "    _, weights = attention_with_temperature(query, key, value, temperature=temp)\n",
    "    \n",
    "    im = axes[i].imshow(weights[0].detach().numpy(), cmap='Blues')\n",
    "    axes[i].set_title(f'温度 = {temp}')\n",
    "    axes[i].set_xlabel('Key位置')\n",
    "    axes[i].set_ylabel('Query位置')\n",
    "    \n",
    "    # 添加数值标注\n",
    "    for row in range(weights.shape[1]):\n",
    "        for col in range(weights.shape[2]):\n",
    "            axes[i].text(col, row, f'{weights[0][row][col]:.2f}', \n",
    "                        ha='center', va='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"观察：\")\n",
    "print(\"- 温度 < 1：注意力更加集中，权重分布更尖锐\")\n",
    "print(\"- 温度 > 1：注意力更加分散，权重分布更平滑\")\n",
    "print(\"- 温度 = 1：标准的注意力机制\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e5d8b9",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "在这个教程中，我们学习了：\n",
    "\n",
    "1. **注意力机制的动机**：解决RNN的串行计算和长期依赖问题\n",
    "2. **核心概念**：Query、Key、Value的概念和计算过程\n",
    "3. **数学原理**：注意力机制的数学公式\n",
    "4. **实际应用**：如何在神经网络中使用注意力机制\n",
    "5. **可视化技巧**：如何理解和分析注意力权重\n",
    "\n",
    "注意力机制是理解Transformer的基础。在下一个教程中，我们将深入学习**自注意力机制（Self-Attention）**，这是Transformer架构的核心组件。\n",
    "\n",
    "### 下一步学习：\n",
    "- [02-self-attention.ipynb](02-self-attention.ipynb) - 自注意力机制详解"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}