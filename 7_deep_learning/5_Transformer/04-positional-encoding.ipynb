{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8b4c8d0",
   "metadata": {},
   "source": [
    "# 4. 位置编码 (Positional Encoding)\n",
    "\n",
    "在前面的教程中，我们学习了注意力机制和多头注意力。但是这些机制有一个重要的局限性：**它们无法感知序列中元素的位置信息**。位置编码就是为了解决这个问题而设计的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f8a2c0",
   "metadata": {},
   "source": [
    "## 4.1 为什么需要位置编码？\n",
    "\n",
    "### 注意力机制的位置盲区\n",
    "\n",
    "自注意力机制本质上是一个**置换不变**的操作，这意味着：\n",
    "- 如果我们打乱输入序列的顺序，注意力权重的模式会保持不变\n",
    "- 模型无法区分 \"我爱你\" 和 \"你爱我\" 这样的句子\n",
    "- 在很多NLP任务中，词序是极其重要的语法和语义信息\n",
    "\n",
    "### 位置信息的重要性\n",
    "\n",
    "考虑以下例子：\n",
    "1. **\"The cat sat on the mat\"** vs **\"The mat sat on the cat\"**\n",
    "2. **\"John loves Mary\"** vs **\"Mary loves John\"**\n",
    "3. **\"Not good\"** vs **\"Good not\"**\n",
    "\n",
    "显然，位置信息对于理解语言的含义至关重要。\n",
    "\n",
    "![位置编码示意图](images/positional_encoding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e7c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "# 设置随机种子和图表样式\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(f\"PyTorch版本: {torch.__version__}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e7c4f5",
   "metadata": {},
   "source": [
    "## 4.2 演示注意力的位置不变性\n",
    "\n",
    "让我们首先通过实验证明注意力机制确实是位置不变的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b7f4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从前面的教程导入多头注意力\n",
    "class SimpleMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(SimpleMultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# 创建测试数据\n",
    "d_model = 32\n",
    "seq_len = 5\n",
    "batch_size = 1\n",
    "\n",
    "# 原始序列\n",
    "original_seq = torch.randn(batch_size, seq_len, d_model)\n",
    "print(f\"原始序列形状: {original_seq.shape}\")\n",
    "\n",
    "# 创建一个置换（打乱顺序）\n",
    "permutation = torch.randperm(seq_len)\n",
    "permuted_seq = original_seq[:, permutation, :]\n",
    "print(f\"置换索引: {permutation.tolist()}\")\n",
    "print(f\"置换后序列形状: {permuted_seq.shape}\")\n",
    "\n",
    "# 测试注意力机制\n",
    "attention = SimpleMultiHeadAttention(d_model, num_heads=4)\n",
    "\n",
    "# 对原始序列和置换序列应用注意力\n",
    "original_output, original_weights = attention(original_seq)\n",
    "permuted_output, permuted_weights = attention(permuted_seq)\n",
    "\n",
    "print(f\"\\n原始序列注意力权重形状: {original_weights.shape}\")\n",
    "print(f\"置换序列注意力权重形状: {permuted_weights.shape}\")\n",
    "\n",
    "# 检查注意力权重的置换不变性\n",
    "# 我们需要按照置换顺序重新排列权重矩阵进行比较\n",
    "reordered_weights = permuted_weights[0, 0, permutation, :][:, permutation]\n",
    "original_weights_first_head = original_weights[0, 0]\n",
    "\n",
    "print(f\"\\n注意力权重的差异（应该很小）: {torch.mean(torch.abs(reordered_weights - original_weights_first_head)):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8b4a3",
   "metadata": {},
   "source": [
    "## 4.3 正弦位置编码 (Sinusoidal Positional Encoding)\n",
    "\n",
    "Transformer论文中提出的经典位置编码使用正弦和余弦函数：\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "其中：\n",
    "- $pos$ 是位置索引\n",
    "- $i$ 是维度索引\n",
    "- $d_{model}$ 是模型维度\n",
    "\n",
    "### 正弦位置编码的优点：\n",
    "1. **唯一性**：每个位置都有唯一的编码\n",
    "2. **相对位置**：模型可以学习相对位置关系\n",
    "3. **外推性**：可以处理训练时未见过的序列长度\n",
    "4. **确定性**：不需要学习参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f7c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    正弦位置编码的实现\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, max_seq_len: int = 5000):\n",
    "        super(SinusoidalPositionalEncoding, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # 创建位置编码矩阵\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # 计算分母项\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                            (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # 计算正弦和余弦\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # 偶数维度使用sin\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # 奇数维度使用cos\n",
    "        \n",
    "        # 添加batch维度并注册为buffer（不会被当作参数）\n",
    "        pe = pe.unsqueeze(0)  # [1, max_seq_len, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, seq_len, d_model]\n",
    "        Returns:\n",
    "            x + positional_encoding: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "    \n",
    "    def get_encoding(self, seq_len: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        获取指定长度的位置编码\n",
    "        \"\"\"\n",
    "        return self.pe[:, :seq_len, :]\n",
    "\n",
    "# 创建位置编码实例\n",
    "d_model = 64\n",
    "max_seq_len = 100\n",
    "pos_encoding = SinusoidalPositionalEncoding(d_model, max_seq_len)\n",
    "\n",
    "# 获取前20个位置的编码用于可视化\n",
    "seq_len_viz = 20\n",
    "pe_matrix = pos_encoding.get_encoding(seq_len_viz)\n",
    "print(f\"位置编码矩阵形状: {pe_matrix.shape}\")\n",
    "print(f\"位置编码数值范围: [{pe_matrix.min():.3f}, {pe_matrix.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b8b2f5",
   "metadata": {},
   "source": [
    "## 4.4 可视化位置编码\n",
    "\n",
    "让我们可视化位置编码，理解其模式和特性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f8b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_positional_encoding(pos_encoding, seq_len=50, d_model=64):\n",
    "    \"\"\"\n",
    "    可视化位置编码\n",
    "    \"\"\"\n",
    "    # 获取位置编码\n",
    "    pe = pos_encoding.get_encoding(seq_len)[0].numpy()  # [seq_len, d_model]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. 位置编码热力图\n",
    "    im1 = axes[0, 0].imshow(pe.T, cmap='RdBu_r', aspect='auto')\n",
    "    axes[0, 0].set_title('位置编码热力图')\n",
    "    axes[0, 0].set_xlabel('位置')\n",
    "    axes[0, 0].set_ylabel('维度')\n",
    "    plt.colorbar(im1, ax=axes[0, 0])\n",
    "    \n",
    "    # 2. 选择几个维度展示波形\n",
    "    positions = np.arange(seq_len)\n",
    "    selected_dims = [0, 1, 2, 3, 10, 20]  # 选择几个维度\n",
    "    \n",
    "    for i, dim in enumerate(selected_dims):\n",
    "        if dim < d_model:\n",
    "            axes[0, 1].plot(positions, pe[:, dim], label=f'维度 {dim}', alpha=0.7)\n",
    "    \n",
    "    axes[0, 1].set_title('不同维度的位置编码波形')\n",
    "    axes[0, 1].set_xlabel('位置')\n",
    "    axes[0, 1].set_ylabel('编码值')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. 不同位置的编码分布\n",
    "    selected_positions = [0, 5, 10, 15, 20]\n",
    "    \n",
    "    for pos in selected_positions:\n",
    "        if pos < seq_len:\n",
    "            axes[1, 0].plot(pe[pos, :], label=f'位置 {pos}', alpha=0.7)\n",
    "    \n",
    "    axes[1, 0].set_title('不同位置的编码向量')\n",
    "    axes[1, 0].set_xlabel('维度')\n",
    "    axes[1, 0].set_ylabel('编码值')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. 位置编码的频率分析\n",
    "    # 计算不同维度的频率\n",
    "    div_terms = [10000 ** (2 * i / d_model) for i in range(d_model // 2)]\n",
    "    frequencies = [1 / term for term in div_terms]\n",
    "    \n",
    "    axes[1, 1].semilogy(frequencies[:min(32, len(frequencies))], 'o-')\n",
    "    axes[1, 1].set_title('位置编码的频率谱')\n",
    "    axes[1, 1].set_xlabel('维度对 (i)')\n",
    "    axes[1, 1].set_ylabel('频率 (log scale)')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return pe\n",
    "\n",
    "# 可视化位置编码\n",
    "pe_matrix = visualize_positional_encoding(pos_encoding, seq_len=50, d_model=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f7c8f6",
   "metadata": {},
   "source": [
    "## 4.5 位置编码的相对位置特性\n",
    "\n",
    "正弦位置编码的一个重要特性是它可以表示相对位置关系。让我们验证这个特性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b3b5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_relative_positions(pos_encoding, max_seq_len=30):\n",
    "    \"\"\"\n",
    "    分析位置编码的相对位置特性\n",
    "    \"\"\"\n",
    "    # 获取位置编码\n",
    "    pe = pos_encoding.get_encoding(max_seq_len)[0]  # [seq_len, d_model]\n",
    "    \n",
    "    # 计算所有位置对之间的相似性\n",
    "    similarity_matrix = torch.zeros(max_seq_len, max_seq_len)\n",
    "    \n",
    "    for i in range(max_seq_len):\n",
    "        for j in range(max_seq_len):\n",
    "            # 使用余弦相似度\n",
    "            cos_sim = F.cosine_similarity(pe[i:i+1], pe[j:j+1], dim=1)\n",
    "            similarity_matrix[i, j] = cos_sim\n",
    "    \n",
    "    # 可视化相似性矩阵\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # 位置相似性热力图\n",
    "    im1 = ax1.imshow(similarity_matrix.numpy(), cmap='RdBu_r')\n",
    "    ax1.set_title('位置编码相似性矩阵')\n",
    "    ax1.set_xlabel('位置 j')\n",
    "    ax1.set_ylabel('位置 i')\n",
    "    plt.colorbar(im1, ax=ax1)\n",
    "    \n",
    "    # 分析相对距离与相似性的关系\n",
    "    distances = []\n",
    "    similarities = []\n",
    "    \n",
    "    for i in range(max_seq_len):\n",
    "        for j in range(max_seq_len):\n",
    "            distance = abs(i - j)\n",
    "            similarity = similarity_matrix[i, j].item()\n",
    "            distances.append(distance)\n",
    "            similarities.append(similarity)\n",
    "    \n",
    "    # 按距离分组计算平均相似性\n",
    "    max_distance = max(distances)\n",
    "    avg_similarities = []\n",
    "    distance_range = range(max_distance + 1)\n",
    "    \n",
    "    for d in distance_range:\n",
    "        same_distance_sims = [sim for dist, sim in zip(distances, similarities) if dist == d]\n",
    "        avg_similarities.append(np.mean(same_distance_sims))\n",
    "    \n",
    "    ax2.plot(distance_range, avg_similarities, 'o-', linewidth=2, markersize=4)\n",
    "    ax2.set_title('相对距离 vs 位置编码相似性')\n",
    "    ax2.set_xlabel('相对距离')\n",
    "    ax2.set_ylabel('平均余弦相似性')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"位置0和位置1的相似性: {similarity_matrix[0, 1]:.4f}\")\n",
    "    print(f\"位置0和位置5的相似性: {similarity_matrix[0, 5]:.4f}\")\n",
    "    print(f\"位置5和位置6的相似性: {similarity_matrix[5, 6]:.4f}\")\n",
    "    print(f\"位置0和位置15的相似性: {similarity_matrix[0, 15]:.4f}\")\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "# 分析相对位置特性\n",
    "similarity_matrix = analyze_relative_positions(pos_encoding, max_seq_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f7c8f8",
   "metadata": {},
   "source": [
    "## 4.6 可学习位置编码 (Learned Positional Encoding)\n",
    "\n",
    "除了固定的正弦位置编码，我们还可以使用可学习的位置编码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedPositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    可学习的位置编码\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, max_seq_len: int = 5000):\n",
    "        super(LearnedPositionalEncoding, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # 创建可学习的位置嵌入\n",
    "        self.position_embeddings = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "        # 初始化\n",
    "        nn.init.normal_(self.position_embeddings.weight, mean=0, std=0.1)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, seq_len, d_model]\n",
    "        Returns:\n",
    "            x + positional_encoding: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # 创建位置索引\n",
    "        positions = torch.arange(0, seq_len, device=x.device, dtype=torch.long)\n",
    "        positions = positions.unsqueeze(0).expand(batch_size, -1)  # [batch_size, seq_len]\n",
    "        \n",
    "        # 获取位置编码\n",
    "        position_encodings = self.position_embeddings(positions)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        return x + position_encodings\n",
    "    \n",
    "    def get_encoding(self, seq_len: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        获取指定长度的位置编码\n",
    "        \"\"\"\n",
    "        positions = torch.arange(0, seq_len, dtype=torch.long)\n",
    "        return self.position_embeddings(positions).unsqueeze(0)\n",
    "\n",
    "class RelativePositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    相对位置编码（T5风格）\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, max_relative_distance: int = 32):\n",
    "        super(RelativePositionalEncoding, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.max_relative_distance = max_relative_distance\n",
    "        \n",
    "        # 创建相对位置嵌入\n",
    "        vocab_size = 2 * max_relative_distance + 1  # -max_dist 到 +max_dist\n",
    "        self.relative_embeddings = nn.Embedding(vocab_size, d_model)\n",
    "    \n",
    "    def forward(self, seq_len: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        计算相对位置编码矩阵\n",
    "        \n",
    "        Args:\n",
    "            seq_len: 序列长度\n",
    "        \n",
    "        Returns:\n",
    "            relative_position_encoding: [seq_len, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # 创建相对位置矩阵\n",
    "        positions = torch.arange(seq_len, dtype=torch.long)\n",
    "        relative_positions = positions.unsqueeze(1) - positions.unsqueeze(0)  # [seq_len, seq_len]\n",
    "        \n",
    "        # 截断相对距离\n",
    "        relative_positions = torch.clamp(\n",
    "            relative_positions, \n",
    "            -self.max_relative_distance, \n",
    "            self.max_relative_distance\n",
    "        )\n",
    "        \n",
    "        # 转换为正索引\n",
    "        relative_positions += self.max_relative_distance\n",
    "        \n",
    "        # 获取相对位置编码\n",
    "        relative_encodings = self.relative_embeddings(relative_positions)\n",
    "        \n",
    "        return relative_encodings\n",
    "\n",
    "# 创建不同类型的位置编码\n",
    "d_model = 32\n",
    "seq_len = 15\n",
    "\n",
    "# 正弦位置编码\n",
    "sin_pos_enc = SinusoidalPositionalEncoding(d_model)\n",
    "\n",
    "# 可学习位置编码\n",
    "learned_pos_enc = LearnedPositionalEncoding(d_model)\n",
    "\n",
    "# 相对位置编码\n",
    "relative_pos_enc = RelativePositionalEncoding(d_model)\n",
    "\n",
    "# 获取编码\n",
    "sin_encoding = sin_pos_enc.get_encoding(seq_len)[0]\n",
    "learned_encoding = learned_pos_enc.get_encoding(seq_len)[0]\n",
    "relative_encoding = relative_pos_enc(seq_len)\n",
    "\n",
    "print(f\"正弦位置编码形状: {sin_encoding.shape}\")\n",
    "print(f\"可学习位置编码形状: {learned_encoding.shape}\")\n",
    "print(f\"相对位置编码形状: {relative_encoding.shape}\")\n",
    "\n",
    "print(f\"\\n各类编码的参数量:\")\n",
    "print(f\"正弦位置编码: 0 (无参数)\")\n",
    "print(f\"可学习位置编码: {sum(p.numel() for p in learned_pos_enc.parameters()):,}\")\n",
    "print(f\"相对位置编码: {sum(p.numel() for p in relative_pos_enc.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f7c8f9",
   "metadata": {},
   "source": [
    "## 4.7 对比不同位置编码方法\n",
    "\n",
    "让我们对比不同位置编码方法的效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f8b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_positional_encodings():\n",
    "    \"\"\"\n",
    "    比较不同的位置编码方法\n",
    "    \"\"\"\n",
    "    d_model = 64\n",
    "    seq_len = 20\n",
    "    \n",
    "    # 创建不同的位置编码\n",
    "    sin_pe = SinusoidalPositionalEncoding(d_model)\n",
    "    learned_pe = LearnedPositionalEncoding(d_model)\n",
    "    \n",
    "    # 获取编码\n",
    "    sin_encoding = sin_pe.get_encoding(seq_len)[0].detach().numpy()\n",
    "    learned_encoding = learned_pe.get_encoding(seq_len)[0].detach().numpy()\n",
    "    \n",
    "    # 可视化对比\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    # 正弦位置编码热力图\n",
    "    im1 = axes[0, 0].imshow(sin_encoding.T, cmap='RdBu_r', aspect='auto')\n",
    "    axes[0, 0].set_title('正弦位置编码')\n",
    "    axes[0, 0].set_xlabel('位置')\n",
    "    axes[0, 0].set_ylabel('维度')\n",
    "    plt.colorbar(im1, ax=axes[0, 0])\n",
    "    \n",
    "    # 可学习位置编码热力图\n",
    "    im2 = axes[0, 1].imshow(learned_encoding.T, cmap='RdBu_r', aspect='auto')\n",
    "    axes[0, 1].set_title('可学习位置编码')\n",
    "    axes[0, 1].set_xlabel('位置')\n",
    "    axes[0, 1].set_ylabel('维度')\n",
    "    plt.colorbar(im2, ax=axes[0, 1])\n",
    "    \n",
    "    # 编码差异\n",
    "    diff = learned_encoding - sin_encoding\n",
    "    im3 = axes[0, 2].imshow(diff.T, cmap='RdBu_r', aspect='auto')\n",
    "    axes[0, 2].set_title('差异 (可学习 - 正弦)')\n",
    "    axes[0, 2].set_xlabel('位置')\n",
    "    axes[0, 2].set_ylabel('维度')\n",
    "    plt.colorbar(im3, ax=axes[0, 2])\n",
    "    \n",
    "    # 选择几个位置的编码向量进行比较\n",
    "    positions_to_show = [0, 5, 10, 15]\n",
    "    \n",
    "    for pos in positions_to_show:\n",
    "        axes[1, 0].plot(sin_encoding[pos, :], label=f'位置 {pos}', alpha=0.7)\n",
    "    axes[1, 0].set_title('正弦编码 - 不同位置的向量')\n",
    "    axes[1, 0].set_xlabel('维度')\n",
    "    axes[1, 0].set_ylabel('编码值')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    for pos in positions_to_show:\n",
    "        axes[1, 1].plot(learned_encoding[pos, :], label=f'位置 {pos}', alpha=0.7)\n",
    "    axes[1, 1].set_title('可学习编码 - 不同位置的向量')\n",
    "    axes[1, 1].set_xlabel('维度')\n",
    "    axes[1, 1].set_ylabel('编码值')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 统计特性比较\n",
    "    sin_std = np.std(sin_encoding, axis=1)\n",
    "    learned_std = np.std(learned_encoding, axis=1)\n",
    "    \n",
    "    axes[1, 2].plot(sin_std, label='正弦编码标准差', marker='o')\n",
    "    axes[1, 2].plot(learned_std, label='可学习编码标准差', marker='s')\n",
    "    axes[1, 2].set_title('不同位置编码的变异性')\n",
    "    axes[1, 2].set_xlabel('位置')\n",
    "    axes[1, 2].set_ylabel('标准差')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 打印统计信息\n",
    "    print(\"位置编码统计比较:\")\n",
    "    print(f\"正弦编码 - 均值: {np.mean(sin_encoding):.4f}, 标准差: {np.std(sin_encoding):.4f}\")\n",
    "    print(f\"可学习编码 - 均值: {np.mean(learned_encoding):.4f}, 标准差: {np.std(learned_encoding):.4f}\")\n",
    "    print(f\"编码差异的均值: {np.mean(np.abs(diff)):.4f}\")\n",
    "\n",
    "# 运行比较\n",
    "compare_positional_encodings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f7c8f9",
   "metadata": {},
   "source": [
    "## 4.8 位置编码对注意力的影响\n",
    "\n",
    "让我们实验位置编码如何影响注意力模式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e7c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_positional_encoding_effect():\n",
    "    \"\"\"\n",
    "    演示位置编码对注意力模式的影响\n",
    "    \"\"\"\n",
    "    d_model = 64\n",
    "    num_heads = 4\n",
    "    seq_len = 8\n",
    "    batch_size = 1\n",
    "    \n",
    "    # 创建输入数据（相同的内容，不同的位置）\n",
    "    # 为了演示效果，我们创建一个重复的模式\n",
    "    base_vector = torch.randn(1, 1, d_model)\n",
    "    repeated_input = base_vector.repeat(batch_size, seq_len, 1)\n",
    "    print(f\"输入形状: {repeated_input.shape}\")\n",
    "    print(\"注意：输入内容在所有位置都相同，只有位置不同\")\n",
    "    \n",
    "    # 创建注意力层\n",
    "    attention = SimpleMultiHeadAttention(d_model, num_heads)\n",
    "    \n",
    "    # 创建位置编码\n",
    "    pos_encoding = SinusoidalPositionalEncoding(d_model)\n",
    "    \n",
    "    # 1. 没有位置编码的注意力\n",
    "    output_no_pos, weights_no_pos = attention(repeated_input)\n",
    "    \n",
    "    # 2. 有位置编码的注意力\n",
    "    input_with_pos = pos_encoding(repeated_input)\n",
    "    output_with_pos, weights_with_pos = attention(input_with_pos)\n",
    "    \n",
    "    # 可视化对比\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    \n",
    "    # 显示前4个头的注意力权重\n",
    "    for head in range(4):\n",
    "        # 无位置编码\n",
    "        im1 = axes[0, head].imshow(weights_no_pos[0, head].detach().numpy(), \n",
    "                                  cmap='Blues', vmin=0, vmax=1)\n",
    "        axes[0, head].set_title(f'无位置编码 - 头 {head+1}')\n",
    "        axes[0, head].set_xlabel('Key位置')\n",
    "        axes[0, head].set_ylabel('Query位置')\n",
    "        \n",
    "        # 添加数值标注\n",
    "        weights_np = weights_no_pos[0, head].detach().numpy()\n",
    "        for i in range(seq_len):\n",
    "            for j in range(seq_len):\n",
    "                axes[0, head].text(j, i, f'{weights_np[i, j]:.2f}', \n",
    "                                  ha='center', va='center', fontsize=8)\n",
    "        \n",
    "        # 有位置编码\n",
    "        im2 = axes[1, head].imshow(weights_with_pos[0, head].detach().numpy(), \n",
    "                                  cmap='Blues', vmin=0, vmax=1)\n",
    "        axes[1, head].set_title(f'有位置编码 - 头 {head+1}')\n",
    "        axes[1, head].set_xlabel('Key位置')\n",
    "        axes[1, head].set_ylabel('Query位置')\n",
    "        \n",
    "        # 添加数值标注\n",
    "        weights_np = weights_with_pos[0, head].detach().numpy()\n",
    "        for i in range(seq_len):\n",
    "            for j in range(seq_len):\n",
    "                axes[1, head].text(j, i, f'{weights_np[i, j]:.2f}', \n",
    "                                  ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 分析注意力权重的差异\n",
    "    print(\"\\n注意力权重分析:\")\n",
    "    \n",
    "    # 计算注意力权重的熵（衡量分散程度）\n",
    "    def calculate_entropy(weights):\n",
    "        # weights: [batch, heads, seq_len, seq_len]\n",
    "        weights_np = weights[0].detach().numpy()\n",
    "        entropies = []\n",
    "        for head in range(weights_np.shape[0]):\n",
    "            head_entropy = []\n",
    "            for i in range(weights_np.shape[1]):\n",
    "                row = weights_np[head, i, :]\n",
    "                entropy = -np.sum(row * np.log(row + 1e-9))\n",
    "                head_entropy.append(entropy)\n",
    "            entropies.append(np.mean(head_entropy))\n",
    "        return entropies\n",
    "    \n",
    "    entropy_no_pos = calculate_entropy(weights_no_pos)\n",
    "    entropy_with_pos = calculate_entropy(weights_with_pos)\n",
    "    \n",
    "    for head in range(4):\n",
    "        print(f\"头 {head+1}:\")\n",
    "        print(f\"  无位置编码平均熵: {entropy_no_pos[head]:.3f}\")\n",
    "        print(f\"  有位置编码平均熵: {entropy_with_pos[head]:.3f}\")\n",
    "        print(f\"  熵增加: {entropy_with_pos[head] - entropy_no_pos[head]:.3f}\")\n",
    "    \n",
    "    # 计算对角线注意力强度（自注意力）\n",
    "    def calculate_diagonal_attention(weights):\n",
    "        weights_np = weights[0].detach().numpy()\n",
    "        diagonal_strengths = []\n",
    "        for head in range(weights_np.shape[0]):\n",
    "            diagonal = np.diag(weights_np[head])\n",
    "            diagonal_strengths.append(np.mean(diagonal))\n",
    "        return diagonal_strengths\n",
    "    \n",
    "    diag_no_pos = calculate_diagonal_attention(weights_no_pos)\n",
    "    diag_with_pos = calculate_diagonal_attention(weights_with_pos)\n",
    "    \n",
    "    print(\"\\n对角线注意力强度（自注意力）:\")\n",
    "    for head in range(4):\n",
    "        print(f\"头 {head+1}: 无位置 {diag_no_pos[head]:.3f}, 有位置 {diag_with_pos[head]:.3f}\")\n",
    "\n",
    "# 运行演示\n",
    "demonstrate_positional_encoding_effect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f7c8d8",
   "metadata": {},
   "source": [
    "## 4.9 位置编码的变体和优化\n",
    "\n",
    "让我们探索一些位置编码的变体："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPEPositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    旋转位置编码 (Rotary Position Embedding - RoPE)\n",
    "    用于GPT-NeoX、LLaMA等模型\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, max_seq_len: int = 5000):\n",
    "        super(RoPEPositionalEncoding, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # 计算频率\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, d_model, 2).float() / d_model))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "        \n",
    "        # 预计算位置编码\n",
    "        self._update_cos_sin_cache(max_seq_len)\n",
    "    \n",
    "    def _update_cos_sin_cache(self, seq_len: int):\n",
    "        positions = torch.arange(seq_len, dtype=torch.float)\n",
    "        freqs = torch.outer(positions, self.inv_freq)\n",
    "        \n",
    "        cos_freqs = torch.cos(freqs)\n",
    "        sin_freqs = torch.sin(freqs)\n",
    "        \n",
    "        self.register_buffer('cos_cache', cos_freqs)\n",
    "        self.register_buffer('sin_cache', sin_freqs)\n",
    "    \n",
    "    def apply_rope(self, x: torch.Tensor, seq_len: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        应用旋转位置编码\n",
    "        \n",
    "        Args:\n",
    "            x: [batch_size, seq_len, d_model]\n",
    "        \n",
    "        Returns:\n",
    "            rotated_x: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        if seq_len > self.cos_cache.size(0):\n",
    "            self._update_cos_sin_cache(seq_len)\n",
    "        \n",
    "        cos = self.cos_cache[:seq_len]\n",
    "        sin = self.sin_cache[:seq_len]\n",
    "        \n",
    "        # 将输入分为偶数和奇数维度\n",
    "        x_even = x[..., 0::2]  # 偶数维度\n",
    "        x_odd = x[..., 1::2]   # 奇数维度\n",
    "        \n",
    "        # 应用旋转\n",
    "        rotated_even = x_even * cos.unsqueeze(0) - x_odd * sin.unsqueeze(0)\n",
    "        rotated_odd = x_even * sin.unsqueeze(0) + x_odd * cos.unsqueeze(0)\n",
    "        \n",
    "        # 重新组合\n",
    "        rotated_x = torch.zeros_like(x)\n",
    "        rotated_x[..., 0::2] = rotated_even\n",
    "        rotated_x[..., 1::2] = rotated_odd\n",
    "        \n",
    "        return rotated_x\n",
    "\n",
    "class ALiBiPositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    ALiBi (Attention with Linear Biases) 位置编码\n",
    "    用于PaLM等模型\n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads: int):\n",
    "        super(ALiBiPositionalEncoding, self).__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # 计算每个头的斜率\n",
    "        slopes = self._get_slopes(num_heads)\n",
    "        self.register_buffer('slopes', slopes)\n",
    "    \n",
    "    def _get_slopes(self, num_heads: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        计算ALiBi斜率\n",
    "        \"\"\"\n",
    "        def get_slopes_power_of_2(n):\n",
    "            start = (2**(-2**-(math.log2(n)-3)))\n",
    "            ratio = start\n",
    "            return [start*ratio**i for i in range(n)]\n",
    "        \n",
    "        if math.log2(num_heads).is_integer():\n",
    "            slopes = get_slopes_power_of_2(num_heads)\n",
    "        else:\n",
    "            closest_power_of_2 = 2**math.floor(math.log2(num_heads))\n",
    "            slopes = get_slopes_power_of_2(closest_power_of_2)\n",
    "            slopes.extend(get_slopes_power_of_2(2*closest_power_of_2)[0:num_heads-closest_power_of_2])\n",
    "        \n",
    "        return torch.tensor(slopes, dtype=torch.float32)\n",
    "    \n",
    "    def forward(self, seq_len: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        计算ALiBi位置偏置\n",
    "        \n",
    "        Args:\n",
    "            seq_len: 序列长度\n",
    "        \n",
    "        Returns:\n",
    "            bias: [num_heads, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        # 创建距离矩阵\n",
    "        positions = torch.arange(seq_len, dtype=torch.float32, device=self.slopes.device)\n",
    "        distances = positions.unsqueeze(1) - positions.unsqueeze(0)\n",
    "        \n",
    "        # 应用斜率\n",
    "        bias = self.slopes.unsqueeze(1).unsqueeze(2) * distances.unsqueeze(0)\n",
    "        \n",
    "        return bias\n",
    "\n",
    "# 测试不同的位置编码变体\n",
    "d_model = 32\n",
    "seq_len = 10\n",
    "num_heads = 4\n",
    "\n",
    "# 创建测试输入\n",
    "test_input = torch.randn(1, seq_len, d_model)\n",
    "\n",
    "# RoPE\n",
    "rope = RoPEPositionalEncoding(d_model)\n",
    "rope_output = rope.apply_rope(test_input, seq_len)\n",
    "\n",
    "# ALiBi\n",
    "alibi = ALiBiPositionalEncoding(num_heads)\n",
    "alibi_bias = alibi(seq_len)\n",
    "\n",
    "print(f\"原始输入形状: {test_input.shape}\")\n",
    "print(f\"RoPE输出形状: {rope_output.shape}\")\n",
    "print(f\"ALiBi偏置形状: {alibi_bias.shape}\")\n",
    "\n",
    "# 可视化ALiBi偏置\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for head in range(num_heads):\n",
    "    im = axes[head].imshow(alibi_bias[head].numpy(), cmap='RdBu_r')\n",
    "    axes[head].set_title(f'ALiBi偏置 - 头 {head+1}')\n",
    "    axes[head].set_xlabel('Key位置')\n",
    "    axes[head].set_ylabel('Query位置')\n",
    "    plt.colorbar(im, ax=axes[head])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nALiBi斜率: {alibi.slopes.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f8b5c7",
   "metadata": {},
   "source": [
    "## 4.10 位置编码的性能比较\n",
    "\n",
    "让我们比较不同位置编码方法的性能特点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_positional_encodings():\n",
    "    \"\"\"\n",
    "    基准测试不同位置编码的性能\n",
    "    \"\"\"\n",
    "    d_model = 512\n",
    "    seq_len = 1024\n",
    "    batch_size = 16\n",
    "    num_heads = 8\n",
    "    \n",
    "    # 创建测试数据\n",
    "    test_data = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # 创建不同的位置编码\n",
    "    encodings = {\n",
    "        '正弦位置编码': SinusoidalPositionalEncoding(d_model),\n",
    "        '可学习位置编码': LearnedPositionalEncoding(d_model),\n",
    "        'RoPE编码': RoPEPositionalEncoding(d_model),\n",
    "        'ALiBi编码': ALiBiPositionalEncoding(num_heads)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(\"=== 位置编码性能基准测试 ===\")\n",
    "    print(f\"测试配置: batch_size={batch_size}, seq_len={seq_len}, d_model={d_model}\")\n",
    "    print()\n",
    "    \n",
    "    for name, encoding in encodings.items():\n",
    "        # 预热\n",
    "        for _ in range(5):\n",
    "            if name == 'RoPE编码':\n",
    "                _ = encoding.apply_rope(test_data, seq_len)\n",
    "            elif name == 'ALiBi编码':\n",
    "                _ = encoding(seq_len)\n",
    "            else:\n",
    "                _ = encoding(test_data)\n",
    "        \n",
    "        # 正式测试\n",
    "        num_runs = 100\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for _ in range(num_runs):\n",
    "            if name == 'RoPE编码':\n",
    "                result = encoding.apply_rope(test_data, seq_len)\n",
    "            elif name == 'ALiBi编码':\n",
    "                result = encoding(seq_len)\n",
    "            else:\n",
    "                result = encoding(test_data)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        avg_time = (end_time - start_time) / num_runs * 1000  # 转换为毫秒\n",
    "        \n",
    "        # 计算内存使用（参数量）\n",
    "        if hasattr(encoding, 'parameters'):\n",
    "            num_params = sum(p.numel() for p in encoding.parameters())\n",
    "        else:\n",
    "            num_params = 0\n",
    "        \n",
    "        # 计算额外的特性\n",
    "        trainable = num_params > 0\n",
    "        \n",
    "        if name == 'ALiBi编码':\n",
    "            extrapolation = \"优秀\"  # ALiBi对长序列外推性好\n",
    "        elif name == 'RoPE编码':\n",
    "            extrapolation = \"良好\"  # RoPE有一定外推性\n",
    "        elif name == '正弦位置编码':\n",
    "            extrapolation = \"良好\"  # 正弦编码有外推性\n",
    "        else:\n",
    "            extrapolation = \"有限\"  # 可学习编码外推性有限\n",
    "        \n",
    "        results[name] = {\n",
    "            'time': avg_time,\n",
    "            'params': num_params,\n",
    "            'trainable': trainable,\n",
    "            'extrapolation': extrapolation\n",
    "        }\n",
    "        \n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  平均处理时间: {avg_time:.2f} ms\")\n",
    "        print(f\"  参数量: {num_params:,}\")\n",
    "        print(f\"  可训练: {'是' if trainable else '否'}\")\n",
    "        print(f\"  外推能力: {extrapolation}\")\n",
    "        print()\n",
    "    \n",
    "    # 创建对比表格\n",
    "    import pandas as pd\n",
    "    \n",
    "    df_data = {\n",
    "        '方法': list(results.keys()),\n",
    "        '处理时间(ms)': [results[name]['time'] for name in results.keys()],\n",
    "        '参数量': [results[name]['params'] for name in results.keys()],\n",
    "        '可训练': [results[name]['trainable'] for name in results.keys()],\n",
    "        '外推能力': [results[name]['extrapolation'] for name in results.keys()]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(df_data)\n",
    "    print(\"位置编码方法对比表:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 运行基准测试\n",
    "benchmark_results = benchmark_positional_encodings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f7c8f9",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "在这个教程中，我们深入学习了位置编码的各个方面：\n",
    "\n",
    "### 核心概念：\n",
    "1. **位置编码的必要性**：注意力机制本身是位置不变的\n",
    "2. **正弦位置编码**：使用sin/cos函数创建位置感知的编码\n",
    "3. **可学习位置编码**：通过训练学习位置表示\n",
    "4. **相对位置编码**：直接建模位置间的相对关系\n",
    "\n",
    "### 数学公式（正弦编码）：\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "### 不同方法的特点：\n",
    "\n",
    "| 方法 | 参数量 | 外推能力 | 性能开销 | 适用场景 |\n",
    "|------|--------|----------|----------|----------|\n",
    "| 正弦编码 | 0 | 良好 | 低 | 通用Transformer |\n",
    "| 可学习编码 | O(L·d) | 有限 | 低 | 固定长度序列 |\n",
    "| RoPE | 0 | 良好 | 中等 | 现代LLM |\n",
    "| ALiBi | 0 | 优秀 | 低 | 长序列应用 |\n",
    "\n",
    "### 关键洞察：\n",
    "1. **位置编码直接影响注意力模式**：它改变了模型对不同位置的关注程度\n",
    "2. **外推能力很重要**：模型需要处理比训练时更长的序列\n",
    "3. **没有万能的方法**：不同应用场景适合不同的位置编码\n",
    "4. **相对位置比绝对位置更重要**：现代方法更关注位置间的相对关系\n",
    "\n",
    "### 实际应用建议：\n",
    "- **通用任务**：使用正弦位置编码\n",
    "- **长序列处理**：考虑ALiBi或RoPE\n",
    "- **固定长度任务**：可学习位置编码可能效果更好\n",
    "- **现代LLM**：RoPE已成为主流选择\n",
    "\n",
    "位置编码解决了Transformer架构中的一个关键问题。在下一个教程中，我们将学习如何将所有这些组件组合成完整的**Transformer块**。\n",
    "\n",
    "### 下一步学习：\n",
    "- [05-transformer-block.ipynb](05-transformer-block.ipynb) - Transformer基本块详解"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}