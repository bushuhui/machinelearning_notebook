{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8b4c8d0",
   "metadata": {},
   "source": [
    "# 7. Transformerå˜ä½“å’Œç°ä»£åº”ç”¨\n",
    "\n",
    "åœ¨å®Œæˆäº†å®Œæ•´Transformerçš„å­¦ä¹ åï¼Œè®©æˆ‘ä»¬æ¢ç´¢Transformerå®¶æ—çš„é‡è¦å˜ä½“å’Œç°ä»£åº”ç”¨ã€‚è¿™äº›å˜ä½“æ¨åŠ¨äº†NLPå’Œå…¶ä»–é¢†åŸŸçš„é‡å¤§çªç ´ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f8a2c0",
   "metadata": {},
   "source": [
    "## 7.1 Transformerå®¶æ—æ¦‚è§ˆ\n",
    "\n",
    "è‡ª2017å¹´åŸå§‹Transformerå‘å¸ƒä»¥æ¥ï¼Œå‡ºç°äº†ä¼—å¤šé‡è¦çš„å˜ä½“ï¼š\n",
    "\n",
    "### æŒ‰æ¶æ„åˆ†ç±»ï¼š\n",
    "1. **ä»…ç¼–ç å™¨æ¶æ„**ï¼šBERTã€RoBERTaã€DeBERTaç­‰\n",
    "2. **ä»…è§£ç å™¨æ¶æ„**ï¼šGPTç³»åˆ—ã€PaLMã€LLaMAç­‰\n",
    "3. **ç¼–ç å™¨-è§£ç å™¨æ¶æ„**ï¼šT5ã€BARTã€mT5ç­‰\n",
    "\n",
    "### æŒ‰åº”ç”¨é¢†åŸŸåˆ†ç±»ï¼š\n",
    "1. **è‡ªç„¶è¯­è¨€å¤„ç†**ï¼šBERTã€GPTã€T5ç­‰\n",
    "2. **è®¡ç®—æœºè§†è§‰**ï¼šVision Transformer (ViT)ã€DETRç­‰\n",
    "3. **å¤šæ¨¡æ€**ï¼šCLIPã€DALLEã€GPT-4ç­‰\n",
    "4. **ç§‘å­¦è®¡ç®—**ï¼šAlphaFoldã€ESMç­‰\n",
    "\n",
    "![Transformerå®¶æ—æ ‘](images/transformer_family.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e7c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "# ä»utilså¯¼å…¥åŸºç¡€ç»„ä»¶\n",
    "from utils import (\n",
    "    MultiHeadAttention, \n",
    "    SinusoidalPositionalEncoding, \n",
    "    FeedForwardNetwork,\n",
    "    TransformerBlock,\n",
    "    PreLNTransformerBlock,\n",
    "    create_padding_mask,\n",
    "    create_causal_mask,\n",
    "    setup_matplotlib_chinese,\n",
    "    set_random_seed\n",
    ")\n",
    "\n",
    "# è®¾ç½®ç¯å¢ƒ\n",
    "setup_matplotlib_chinese()\n",
    "set_random_seed(42)\n",
    "\n",
    "print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ä½¿ç”¨è®¾å¤‡: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e7c4f5",
   "metadata": {},
   "source": [
    "## 7.2 BERTï¼šåŒå‘ç¼–ç å™¨è¡¨ç¤º\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) æ˜¯æœ€å…·å½±å“åŠ›çš„Transformerå˜ä½“ä¹‹ä¸€ï¼š\n",
    "\n",
    "### æ ¸å¿ƒç‰¹ç‚¹ï¼š\n",
    "- **ä»…ç¼–ç å™¨æ¶æ„**ï¼šåªä½¿ç”¨Transformerçš„ç¼–ç å™¨éƒ¨åˆ†\n",
    "- **åŒå‘ä¸Šä¸‹æ–‡**ï¼šèƒ½å¤ŸåŒæ—¶çœ‹åˆ°å·¦è¾¹å’Œå³è¾¹çš„ä¸Šä¸‹æ–‡\n",
    "- **é¢„è®­ç»ƒ+å¾®è°ƒ**ï¼šåœ¨å¤§è§„æ¨¡è¯­æ–™ä¸Šé¢„è®­ç»ƒï¼Œç„¶ååœ¨å…·ä½“ä»»åŠ¡ä¸Šå¾®è°ƒ\n",
    "\n",
    "### é¢„è®­ç»ƒä»»åŠ¡ï¼š\n",
    "1. **æ©ç è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰**ï¼šéšæœºæ©ç›–ä¸€äº›è¯ï¼Œè®©æ¨¡å‹é¢„æµ‹\n",
    "2. **ä¸‹ä¸€å¥é¢„æµ‹ï¼ˆNSPï¼‰**ï¼šé¢„æµ‹ä¸¤ä¸ªå¥å­æ˜¯å¦è¿ç»­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b7f4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    ç®€åŒ–çš„BERTæ¨¡å‹å®ç°\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, d_model: int = 768, num_heads: int = 12, \n",
    "                 d_ff: int = 3072, num_layers: int = 12, max_seq_len: int = 512, \n",
    "                 dropout: float = 0.1):\n",
    "        super(BERTModel, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # åµŒå…¥å±‚\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        self.token_type_embedding = nn.Embedding(2, d_model)  # ç”¨äºåŒºåˆ†å¥å­Aå’ŒB\n",
    "        \n",
    "        # Transformerç¼–ç å™¨å±‚\n",
    "        self.layers = nn.ModuleList([\n",
    "            PreLNTransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # è¾“å‡ºå±‚\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # ç”¨äºMLMå’ŒNSPçš„å¤´\n",
    "        self.mlm_head = nn.Linear(d_model, vocab_size)\n",
    "        self.nsp_head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(d_model, 2)\n",
    "        )\n",
    "        \n",
    "        # ç‰¹æ®Štokenç´¢å¼•\n",
    "        self.cls_token_id = 0  # [CLS]\n",
    "        self.sep_token_id = 1  # [SEP]\n",
    "        self.mask_token_id = 2  # [MASK]\n",
    "        self.pad_token_id = 3  # [PAD]\n",
    "        \n",
    "    def forward(self, input_ids: torch.Tensor, token_type_ids: Optional[torch.Tensor] = None,\n",
    "                attention_mask: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­\n",
    "        \n",
    "        Args:\n",
    "            input_ids: [batch_size, seq_len]\n",
    "            token_type_ids: [batch_size, seq_len] å¥å­ç±»å‹ID\n",
    "            attention_mask: [batch_size, seq_len] æ³¨æ„åŠ›æ©ç \n",
    "        \n",
    "        Returns:\n",
    "            last_hidden_state: [batch_size, seq_len, d_model]\n",
    "            pooler_output: [batch_size, d_model] ç”¨äºåˆ†ç±»çš„[CLS]è¡¨ç¤º\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "        \n",
    "        # åˆ›å»ºä½ç½®ID\n",
    "        position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        # å¦‚æœæ²¡æœ‰æä¾›token_type_idsï¼Œé»˜è®¤éƒ½æ˜¯0\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "        \n",
    "        # åµŒå…¥\n",
    "        token_embeds = self.token_embedding(input_ids)\n",
    "        position_embeds = self.position_embedding(position_ids)\n",
    "        token_type_embeds = self.token_type_embedding(token_type_ids)\n",
    "        \n",
    "        # åˆå¹¶åµŒå…¥\n",
    "        embeddings = token_embeds + position_embeds + token_type_embeds\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        # åˆ›å»ºæ³¨æ„åŠ›æ©ç \n",
    "        if attention_mask is not None:\n",
    "            # æ‰©å±•æ©ç ç»´åº¦: [batch_size, 1, 1, seq_len]\n",
    "            extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "            extended_attention_mask = extended_attention_mask.expand(-1, -1, seq_len, -1)\n",
    "        else:\n",
    "            extended_attention_mask = None\n",
    "        \n",
    "        # é€šè¿‡Transformerå±‚\n",
    "        hidden_states = embeddings\n",
    "        attention_weights_list = []\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            hidden_states, attention_weights = layer(hidden_states, extended_attention_mask)\n",
    "            attention_weights_list.append(attention_weights)\n",
    "        \n",
    "        # è·å–[CLS]çš„è¡¨ç¤ºç”¨äºå¥å­çº§ä»»åŠ¡\n",
    "        pooler_output = self.nsp_head(hidden_states[:, 0, :])  # [CLS]æ˜¯ç¬¬ä¸€ä¸ªtoken\n",
    "        \n",
    "        return {\n",
    "            'last_hidden_state': hidden_states,\n",
    "            'pooler_output': pooler_output,\n",
    "            'attention_weights': attention_weights_list\n",
    "        }\n",
    "    \n",
    "    def get_mlm_predictions(self, hidden_states: torch.Tensor):\n",
    "        \"\"\"è·å–æ©ç è¯­è¨€æ¨¡å‹çš„é¢„æµ‹\"\"\"\n",
    "        return self.mlm_head(hidden_states)\n",
    "\n",
    "# åˆ›å»ºBERTæ¨¡å‹\n",
    "bert_model = BERTModel(\n",
    "    vocab_size=1000,\n",
    "    d_model=256,  # ä½¿ç”¨è¾ƒå°çš„ç»´åº¦ç”¨äºæ¼”ç¤º\n",
    "    num_heads=8,\n",
    "    d_ff=1024,\n",
    "    num_layers=4,\n",
    "    max_seq_len=128\n",
    ")\n",
    "\n",
    "# æµ‹è¯•BERTæ¨¡å‹\n",
    "batch_size = 2\n",
    "seq_len = 16\n",
    "\n",
    "# æ¨¡æ‹Ÿè¾“å…¥\n",
    "input_ids = torch.randint(4, 1000, (batch_size, seq_len))  # é¿å¼€ç‰¹æ®Štoken\n",
    "input_ids[:, 0] = 0  # è®¾ç½®[CLS]\n",
    "input_ids[:, seq_len//2] = 1  # è®¾ç½®[SEP]\n",
    "\n",
    "attention_mask = torch.ones(batch_size, seq_len)\n",
    "token_type_ids = torch.zeros(batch_size, seq_len)\n",
    "token_type_ids[:, seq_len//2+1:] = 1  # ç¬¬äºŒä¸ªå¥å­\n",
    "\n",
    "# å‰å‘ä¼ æ’­\n",
    "outputs = bert_model(input_ids, token_type_ids, attention_mask)\n",
    "\n",
    "print(f\"BERTæ¨¡å‹è¾“å‡º:\")\n",
    "print(f\"æœ€åéšè—å±‚: {outputs['last_hidden_state'].shape}\")\n",
    "print(f\"æ± åŒ–è¾“å‡º: {outputs['pooler_output'].shape}\")\n",
    "print(f\"æ³¨æ„åŠ›å±‚æ•°: {len(outputs['attention_weights'])}\")\n",
    "print(f\"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in bert_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8b4a3",
   "metadata": {},
   "source": [
    "## 7.3 GPTï¼šç”Ÿæˆå¼é¢„è®­ç»ƒTransformer\n",
    "\n",
    "GPT (Generative Pre-trained Transformer) ç³»åˆ—æ˜¯å¦ä¸€ä¸ªé‡è¦çš„Transformerå˜ä½“ï¼š\n",
    "\n",
    "### æ ¸å¿ƒç‰¹ç‚¹ï¼š\n",
    "- **ä»…è§£ç å™¨æ¶æ„**ï¼šä½¿ç”¨å› æœæ©ç çš„è‡ªæ³¨æ„åŠ›\n",
    "- **è‡ªå›å½’ç”Ÿæˆ**ï¼šä»å·¦åˆ°å³é€ä¸ªç”Ÿæˆtoken\n",
    "- **è¯­è¨€å»ºæ¨¡**ï¼šé¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒ\n",
    "\n",
    "### GPTç³»åˆ—æ¼”è¿›ï¼š\n",
    "- **GPT-1** (117Må‚æ•°)ï¼šè¯æ˜äº†é¢„è®­ç»ƒ+å¾®è°ƒçš„æœ‰æ•ˆæ€§\n",
    "- **GPT-2** (1.5Bå‚æ•°)ï¼šå±•ç¤ºäº†è§„æ¨¡åŒ–çš„å¨åŠ›\n",
    "- **GPT-3** (175Bå‚æ•°)ï¼šå®ç°äº†Few-shot Learning\n",
    "- **GPT-4** (æœªå…¬å¸ƒå‚æ•°)ï¼šå¤šæ¨¡æ€èƒ½åŠ›ï¼Œæ›´å¼ºçš„æ¨ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f7c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    ç®€åŒ–çš„GPTæ¨¡å‹å®ç°\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, d_model: int = 768, num_heads: int = 12, \n",
    "                 d_ff: int = 3072, num_layers: int = 12, max_seq_len: int = 1024, \n",
    "                 dropout: float = 0.1):\n",
    "        super(GPTModel, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # åµŒå…¥å±‚\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "        # Transformerè§£ç å™¨å±‚ï¼ˆä½¿ç”¨å› æœæ©ç ï¼‰\n",
    "        self.layers = nn.ModuleList([\n",
    "            PreLNTransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # è¾“å‡ºå±‚\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # è¯­è¨€å»ºæ¨¡å¤´\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        \n",
    "        # æƒé‡å…±äº«ï¼ˆå¯é€‰ï¼‰\n",
    "        # self.lm_head.weight = self.token_embedding.weight\n",
    "        \n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­\n",
    "        \n",
    "        Args:\n",
    "            input_ids: [batch_size, seq_len]\n",
    "            attention_mask: [batch_size, seq_len]\n",
    "        \n",
    "        Returns:\n",
    "            logits: [batch_size, seq_len, vocab_size]\n",
    "            hidden_states: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "        \n",
    "        # åˆ›å»ºä½ç½®ID\n",
    "        position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        # åµŒå…¥\n",
    "        token_embeds = self.token_embedding(input_ids)\n",
    "        position_embeds = self.position_embedding(position_ids)\n",
    "        \n",
    "        hidden_states = token_embeds + position_embeds\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        \n",
    "        # åˆ›å»ºå› æœæ©ç \n",
    "        causal_mask = create_causal_mask(seq_len).to(input_ids.device)\n",
    "        causal_mask = causal_mask.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        \n",
    "        # ç»“åˆå¡«å……æ©ç ï¼ˆå¦‚æœæä¾›ï¼‰\n",
    "        if attention_mask is not None:\n",
    "            padding_mask = attention_mask.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "            combined_mask = causal_mask & padding_mask\n",
    "        else:\n",
    "            combined_mask = causal_mask\n",
    "        \n",
    "        # é€šè¿‡Transformerå±‚\n",
    "        attention_weights_list = []\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            hidden_states, attention_weights = layer(hidden_states, combined_mask)\n",
    "            attention_weights_list.append(attention_weights)\n",
    "        \n",
    "        hidden_states = self.layer_norm(hidden_states)\n",
    "        \n",
    "        # è¯­è¨€å»ºæ¨¡è¾“å‡º\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        \n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'hidden_states': hidden_states,\n",
    "            'attention_weights': attention_weights_list\n",
    "        }\n",
    "    \n",
    "    def generate(self, input_ids: torch.Tensor, max_new_tokens: int = 50, \n",
    "                 temperature: float = 1.0, top_k: int = None, top_p: float = None):\n",
    "        \"\"\"\n",
    "        æ–‡æœ¬ç”Ÿæˆæ–¹æ³•\n",
    "        \n",
    "        Args:\n",
    "            input_ids: è¾“å…¥çš„tokenåºåˆ—\n",
    "            max_new_tokens: æœ€å¤§ç”Ÿæˆçš„æ–°tokenæ•°\n",
    "            temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§\n",
    "            top_k: Top-Ké‡‡æ ·\n",
    "            top_p: Top-Pé‡‡æ ·ï¼ˆæ ¸é‡‡æ ·ï¼‰\n",
    "        \n",
    "        Returns:\n",
    "            generated_ids: ç”Ÿæˆçš„å®Œæ•´åºåˆ—\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        generated_ids = input_ids.clone()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                # å¦‚æœåºåˆ—å¤ªé•¿ï¼Œæˆªæ–­\n",
    "                if generated_ids.size(1) > self.max_seq_len:\n",
    "                    generated_ids = generated_ids[:, -self.max_seq_len:]\n",
    "                \n",
    "                # å‰å‘ä¼ æ’­\n",
    "                outputs = self.forward(generated_ids)\n",
    "                logits = outputs['logits'][:, -1, :] / temperature  # å–æœ€åä¸€ä¸ªä½ç½®çš„logits\n",
    "                \n",
    "                # é‡‡æ ·ç­–ç•¥\n",
    "                if top_k is not None:\n",
    "                    # Top-Ké‡‡æ ·\n",
    "                    top_k_logits, top_k_indices = torch.topk(logits, top_k)\n",
    "                    logits = torch.full_like(logits, float('-inf'))\n",
    "                    logits.scatter_(1, top_k_indices, top_k_logits)\n",
    "                \n",
    "                if top_p is not None:\n",
    "                    # Top-Pé‡‡æ ·ï¼ˆæ ¸é‡‡æ ·ï¼‰\n",
    "                    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                    \n",
    "                    # ç§»é™¤ç´¯ç§¯æ¦‚ç‡è¶…è¿‡top_pçš„token\n",
    "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                    sorted_indices_to_remove[..., 0] = 0\n",
    "                    \n",
    "                    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "                    logits[indices_to_remove] = float('-inf')\n",
    "                \n",
    "                # è®¡ç®—æ¦‚ç‡å¹¶é‡‡æ ·\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                \n",
    "                # æ·»åŠ åˆ°ç”Ÿæˆåºåˆ—\n",
    "                generated_ids = torch.cat([generated_ids, next_token], dim=1)\n",
    "        \n",
    "        return generated_ids\n",
    "\n",
    "# åˆ›å»ºGPTæ¨¡å‹\n",
    "gpt_model = GPTModel(\n",
    "    vocab_size=1000,\n",
    "    d_model=256,  # ä½¿ç”¨è¾ƒå°çš„ç»´åº¦ç”¨äºæ¼”ç¤º\n",
    "    num_heads=8,\n",
    "    d_ff=1024,\n",
    "    num_layers=4,\n",
    "    max_seq_len=128\n",
    ")\n",
    "\n",
    "# æµ‹è¯•GPTæ¨¡å‹\n",
    "input_ids = torch.randint(0, 1000, (1, 10))\n",
    "outputs = gpt_model(input_ids)\n",
    "\n",
    "print(f\"GPTæ¨¡å‹è¾“å‡º:\")\n",
    "print(f\"Logitså½¢çŠ¶: {outputs['logits'].shape}\")\n",
    "print(f\"éšè—çŠ¶æ€å½¢çŠ¶: {outputs['hidden_states'].shape}\")\n",
    "print(f\"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in gpt_model.parameters()):,}\")\n",
    "\n",
    "# æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ\n",
    "generated = gpt_model.generate(input_ids, max_new_tokens=10, temperature=0.8, top_k=50)\n",
    "print(f\"\\nç”Ÿæˆåºåˆ—é•¿åº¦: {generated.shape[1]} (åŸé•¿åº¦: {input_ids.shape[1]})\")\n",
    "print(f\"ç”Ÿæˆçš„token: {generated[0].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b8b2f5",
   "metadata": {},
   "source": [
    "## 7.4 Vision Transformer (ViT)\n",
    "\n",
    "Vision Transformerå°†TransformeræˆåŠŸåº”ç”¨åˆ°è®¡ç®—æœºè§†è§‰é¢†åŸŸï¼š\n",
    "\n",
    "### æ ¸å¿ƒæ€æƒ³ï¼š\n",
    "1. **å›¾åƒåˆ†å—**ï¼šå°†å›¾åƒåˆ†å‰²ä¸ºå›ºå®šå¤§å°çš„patch\n",
    "2. **çº¿æ€§åµŒå…¥**ï¼šå°†æ¯ä¸ªpatché€šè¿‡çº¿æ€§å±‚æ˜ å°„ä¸ºtoken\n",
    "3. **ä½ç½®ç¼–ç **ï¼šä¸ºæ¯ä¸ªpatchæ·»åŠ ä½ç½®ä¿¡æ¯\n",
    "4. **Transformerç¼–ç **ï¼šä½¿ç”¨æ ‡å‡†çš„Transformerç¼–ç å™¨\n",
    "5. **åˆ†ç±»å¤´**ï¼šä½¿ç”¨[CLS] tokenè¿›è¡Œå›¾åƒåˆ†ç±»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f8b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer (ViT) å®ç°\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size: int = 224, patch_size: int = 16, num_classes: int = 1000,\n",
    "                 d_model: int = 768, num_heads: int = 12, d_ff: int = 3072, \n",
    "                 num_layers: int = 12, channels: int = 3, dropout: float = 0.1):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        \n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # PatchåµŒå…¥\n",
    "        self.patch_embedding = nn.Conv2d(\n",
    "            channels, d_model, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "        \n",
    "        # å¯å­¦ä¹ çš„[CLS] token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        \n",
    "        # ä½ç½®ç¼–ç \n",
    "        self.position_embedding = nn.Parameter(\n",
    "            torch.randn(1, self.num_patches + 1, d_model)\n",
    "        )\n",
    "        \n",
    "        # Transformerç¼–ç å™¨\n",
    "        self.layers = nn.ModuleList([\n",
    "            PreLNTransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # è¾“å‡ºå±‚\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # åˆ†ç±»å¤´\n",
    "        self.classification_head = nn.Linear(d_model, num_classes)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­\n",
    "        \n",
    "        Args:\n",
    "            x: [batch_size, channels, height, width]\n",
    "        \n",
    "        Returns:\n",
    "            logits: [batch_size, num_classes]\n",
    "            attention_weights: æ³¨æ„åŠ›æƒé‡åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # 1. PatchåµŒå…¥\n",
    "        # [batch_size, d_model, num_patches_h, num_patches_w]\n",
    "        patch_embeds = self.patch_embedding(x)\n",
    "        \n",
    "        # é‡å¡‘ä¸ºåºåˆ—: [batch_size, num_patches, d_model]\n",
    "        patch_embeds = patch_embeds.flatten(2).transpose(1, 2)\n",
    "        \n",
    "        # 2. æ·»åŠ [CLS] token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        # [batch_size, num_patches + 1, d_model]\n",
    "        token_embeds = torch.cat([cls_tokens, patch_embeds], dim=1)\n",
    "        \n",
    "        # 3. æ·»åŠ ä½ç½®ç¼–ç \n",
    "        token_embeds = token_embeds + self.position_embedding\n",
    "        token_embeds = self.dropout(token_embeds)\n",
    "        \n",
    "        # 4. é€šè¿‡Transformerå±‚\n",
    "        hidden_states = token_embeds\n",
    "        attention_weights_list = []\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            hidden_states, attention_weights = layer(hidden_states)\n",
    "            attention_weights_list.append(attention_weights)\n",
    "        \n",
    "        hidden_states = self.layer_norm(hidden_states)\n",
    "        \n",
    "        # 5. åˆ†ç±»\n",
    "        # ä½¿ç”¨[CLS] tokenï¼ˆç¬¬ä¸€ä¸ªä½ç½®ï¼‰è¿›è¡Œåˆ†ç±»\n",
    "        cls_output = hidden_states[:, 0, :]\n",
    "        logits = self.classification_head(cls_output)\n",
    "        \n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'hidden_states': hidden_states,\n",
    "            'attention_weights': attention_weights_list\n",
    "        }\n",
    "\n",
    "# åˆ›å»ºViTæ¨¡å‹\n",
    "vit_model = VisionTransformer(\n",
    "    img_size=224,\n",
    "    patch_size=16,\n",
    "    num_classes=10,  # ç®€åŒ–ä¸º10ç±»\n",
    "    d_model=256,     # ä½¿ç”¨è¾ƒå°çš„ç»´åº¦ç”¨äºæ¼”ç¤º\n",
    "    num_heads=8,\n",
    "    d_ff=1024,\n",
    "    num_layers=4,\n",
    "    channels=3\n",
    ")\n",
    "\n",
    "# æµ‹è¯•ViTæ¨¡å‹\n",
    "batch_size = 2\n",
    "test_images = torch.randn(batch_size, 3, 224, 224)\n",
    "\n",
    "outputs = vit_model(test_images)\n",
    "\n",
    "print(f\"Vision Transformerè¾“å‡º:\")\n",
    "print(f\"è¾“å…¥å›¾åƒå½¢çŠ¶: {test_images.shape}\")\n",
    "print(f\"åˆ†ç±»logitså½¢çŠ¶: {outputs['logits'].shape}\")\n",
    "print(f\"éšè—çŠ¶æ€å½¢çŠ¶: {outputs['hidden_states'].shape}\")\n",
    "print(f\"Patchæ•°é‡: {vit_model.num_patches}\")\n",
    "print(f\"åºåˆ—é•¿åº¦: {vit_model.num_patches + 1} (åŒ…å«CLS token)\")\n",
    "print(f\"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in vit_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f7c8f6",
   "metadata": {},
   "source": [
    "## 7.5 å¯è§†åŒ–ViTçš„æ³¨æ„åŠ›æ¨¡å¼\n",
    "\n",
    "è®©æˆ‘ä»¬å¯è§†åŒ–Vision Transformerå­¦åˆ°çš„æ³¨æ„åŠ›æ¨¡å¼ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b3b5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_vit_attention(model, image, layer_idx=-1, head_idx=0):\n",
    "    \"\"\"\n",
    "    å¯è§†åŒ–ViTçš„æ³¨æ„åŠ›æ¨¡å¼\n",
    "    \n",
    "    Args:\n",
    "        model: ViTæ¨¡å‹\n",
    "        image: è¾“å…¥å›¾åƒ [1, 3, H, W]\n",
    "        layer_idx: è¦å¯è§†åŒ–çš„å±‚ç´¢å¼•\n",
    "        head_idx: è¦å¯è§†åŒ–çš„å¤´ç´¢å¼•\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        \n",
    "        # è·å–æŒ‡å®šå±‚å’Œå¤´çš„æ³¨æ„åŠ›æƒé‡\n",
    "        attention_weights = outputs['attention_weights'][layer_idx]  # [1, num_heads, seq_len, seq_len]\n",
    "        attention_map = attention_weights[0, head_idx].cpu().numpy()  # [seq_len, seq_len]\n",
    "        \n",
    "        # è·å–[CLS] tokenå¯¹æ‰€æœ‰patchçš„æ³¨æ„åŠ›\n",
    "        cls_attention = attention_map[0, 1:]  # å»æ‰[CLS] tokenè‡ªå·±\n",
    "        \n",
    "        # é‡å¡‘ä¸ºå›¾åƒå½¢çŠ¶\n",
    "        num_patches_per_side = int(np.sqrt(len(cls_attention)))\n",
    "        attention_image = cls_attention.reshape(num_patches_per_side, num_patches_per_side)\n",
    "        \n",
    "        # å¯è§†åŒ–\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        # åŸå§‹å›¾åƒ\n",
    "        if image.shape[1] == 3:  # RGBå›¾åƒ\n",
    "            img_np = image[0].permute(1, 2, 0).cpu().numpy()\n",
    "            img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())  # å½’ä¸€åŒ–åˆ°[0,1]\n",
    "            axes[0].imshow(img_np)\n",
    "        else:\n",
    "            axes[0].imshow(image[0, 0].cpu().numpy(), cmap='gray')\n",
    "        axes[0].set_title('åŸå§‹å›¾åƒ')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # [CLS]æ³¨æ„åŠ›çƒ­åŠ›å›¾\n",
    "        im1 = axes[1].imshow(attention_image, cmap='hot', interpolation='nearest')\n",
    "        axes[1].set_title(f'[CLS] Tokenæ³¨æ„åŠ›\\n(å±‚{layer_idx}, å¤´{head_idx})')\n",
    "        axes[1].axis('off')\n",
    "        plt.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # å®Œæ•´æ³¨æ„åŠ›çŸ©é˜µ\n",
    "        im2 = axes[2].imshow(attention_map, cmap='Blues')\n",
    "        axes[2].set_title('å®Œæ•´æ³¨æ„åŠ›çŸ©é˜µ')\n",
    "        axes[2].set_xlabel('Keyä½ç½®')\n",
    "        axes[2].set_ylabel('Queryä½ç½®')\n",
    "        plt.colorbar(im2, ax=axes[2], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return attention_image, attention_map\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•å›¾åƒï¼ˆä¸­å¿ƒæœ‰ä¸€ä¸ªäº®å—ï¼‰\n",
    "test_image = torch.zeros(1, 3, 224, 224)\n",
    "test_image[:, :, 80:144, 80:144] = 1.0  # ä¸­å¿ƒäº®å—\n",
    "test_image[:, :, 100:124, 100:124] = 0.5  # ä¸­å¿ƒæ›´äº®çš„å°å—\n",
    "\n",
    "# å¯è§†åŒ–æ³¨æ„åŠ›\n",
    "attention_img, attention_full = visualize_vit_attention(vit_model, test_image, layer_idx=-1, head_idx=0)\n",
    "\n",
    "print(f\"æ³¨æ„åŠ›å›¾åƒå½¢çŠ¶: {attention_img.shape}\")\n",
    "print(f\"å®Œæ•´æ³¨æ„åŠ›çŸ©é˜µå½¢çŠ¶: {attention_full.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f7c8f8",
   "metadata": {},
   "source": [
    "## 7.6 æ•ˆç‡ä¼˜åŒ–å˜ä½“\n",
    "\n",
    "éšç€Transformeræ¨¡å‹è§„æ¨¡çš„å¢é•¿ï¼Œå‡ºç°äº†è®¸å¤šä¸“æ³¨äºæ•ˆç‡ä¼˜åŒ–çš„å˜ä½“ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    çº¿æ€§æ³¨æ„åŠ›æœºåˆ¶\n",
    "    å°†å¤æ‚åº¦ä»O(nÂ²)é™ä½åˆ°O(n)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        super(LinearAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):\n",
    "        batch_size, seq_len, _ = query.size()\n",
    "        \n",
    "        Q = self.W_q(query).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(key).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(value).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # åº”ç”¨ç‰¹å¾æ˜ å°„å‡½æ•°ï¼ˆè¿™é‡Œä½¿ç”¨ReLUä½œä¸ºç¤ºä¾‹ï¼‰\n",
    "        Q = F.relu(Q)\n",
    "        K = F.relu(K)\n",
    "        \n",
    "        # çº¿æ€§æ³¨æ„åŠ›è®¡ç®—: O(n) å¤æ‚åº¦\n",
    "        # è®¡ç®— K^T V\n",
    "        KV = torch.matmul(K.transpose(-2, -1), V)  # [batch, heads, d_k, d_k]\n",
    "        \n",
    "        # è®¡ç®— Q (K^T V)\n",
    "        output = torch.matmul(Q, KV)  # [batch, heads, seq_len, d_k]\n",
    "        \n",
    "        # å½’ä¸€åŒ–\n",
    "        normalizer = torch.matmul(Q, K.sum(dim=-2, keepdim=True).transpose(-2, -1))\n",
    "        output = output / (normalizer + 1e-6)\n",
    "        \n",
    "        # é‡å¡‘å’Œè¾“å‡ºæŠ•å½±\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class SlidingWindowAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    æ»‘åŠ¨çª—å£æ³¨æ„åŠ›\n",
    "    åªå…³æ³¨å±€éƒ¨çª—å£å†…çš„tokenï¼Œé™ä½å¤æ‚åº¦\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int, window_size: int = 128, dropout: float = 0.1):\n",
    "        super(SlidingWindowAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):\n",
    "        batch_size, seq_len, _ = query.size()\n",
    "        \n",
    "        Q = self.W_q(query).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(key).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(value).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # åˆ›å»ºæ»‘åŠ¨çª—å£æ©ç \n",
    "        mask = torch.zeros(seq_len, seq_len, device=query.device)\n",
    "        for i in range(seq_len):\n",
    "            start = max(0, i - self.window_size // 2)\n",
    "            end = min(seq_len, i + self.window_size // 2 + 1)\n",
    "            mask[i, start:end] = 1\n",
    "        \n",
    "        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # åº”ç”¨æ»‘åŠ¨çª—å£æ©ç \n",
    "        scores = scores.masked_fill(mask.unsqueeze(0).unsqueeze(0) == 0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# æ¯”è¾ƒä¸åŒæ³¨æ„åŠ›æœºåˆ¶çš„å¤æ‚åº¦\n",
    "def compare_attention_complexity():\n",
    "    \"\"\"\n",
    "    æ¯”è¾ƒä¸åŒæ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—å¤æ‚åº¦\n",
    "    \"\"\"\n",
    "    seq_lengths = [128, 256, 512, 1024, 2048]\n",
    "    d_model = 512\n",
    "    \n",
    "    # è®¡ç®—å¤æ‚åº¦ï¼ˆç®€åŒ–ï¼Œä»¥FLOPsä¸ºå•ä½ï¼‰\n",
    "    standard_complexity = []\n",
    "    linear_complexity = []\n",
    "    sliding_window_complexity = []\n",
    "    \n",
    "    window_size = 128\n",
    "    \n",
    "    for n in seq_lengths:\n",
    "        # æ ‡å‡†æ³¨æ„åŠ›: O(nÂ² * d)\n",
    "        standard = n * n * d_model\n",
    "        \n",
    "        # çº¿æ€§æ³¨æ„åŠ›: O(n * dÂ²)\n",
    "        linear = n * d_model * d_model\n",
    "        \n",
    "        # æ»‘åŠ¨çª—å£æ³¨æ„åŠ›: O(n * w * d), wä¸ºçª—å£å¤§å°\n",
    "        sliding = n * window_size * d_model\n",
    "        \n",
    "        standard_complexity.append(standard / 1e9)  # è½¬æ¢ä¸ºG FLOPs\n",
    "        linear_complexity.append(linear / 1e9)\n",
    "        sliding_window_complexity.append(sliding / 1e9)\n",
    "    \n",
    "    # ç»˜åˆ¶å¤æ‚åº¦å¯¹æ¯”\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.plot(seq_lengths, standard_complexity, 'r-o', label='æ ‡å‡†æ³¨æ„åŠ› O(nÂ²d)', linewidth=2)\n",
    "    plt.plot(seq_lengths, linear_complexity, 'g-s', label='çº¿æ€§æ³¨æ„åŠ› O(ndÂ²)', linewidth=2)\n",
    "    plt.plot(seq_lengths, sliding_window_complexity, 'b-^', label=f'æ»‘åŠ¨çª—å£æ³¨æ„åŠ› O(nwd), w={window_size}', linewidth=2)\n",
    "    \n",
    "    plt.xlabel('åºåˆ—é•¿åº¦')\n",
    "    plt.ylabel('è®¡ç®—å¤æ‚åº¦ (G FLOPs)')\n",
    "    plt.title('ä¸åŒæ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—å¤æ‚åº¦å¯¹æ¯”')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n",
    "    \n",
    "    # æ‰“å°å¤æ‚åº¦è¡¨æ ¼\n",
    "    print(\"ä¸åŒæ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—å¤æ‚åº¦å¯¹æ¯” (G FLOPs):\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'åºåˆ—é•¿åº¦':<10} {'æ ‡å‡†æ³¨æ„åŠ›':<15} {'çº¿æ€§æ³¨æ„åŠ›':<15} {'æ»‘åŠ¨çª—å£':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, n in enumerate(seq_lengths):\n",
    "        print(f\"{n:<10} {standard_complexity[i]:<15.2f} {linear_complexity[i]:<15.2f} {sliding_window_complexity[i]:<15.2f}\")\n",
    "\n",
    "# è¿è¡Œå¤æ‚åº¦å¯¹æ¯”\n",
    "compare_attention_complexity()\n",
    "\n",
    "# æµ‹è¯•çº¿æ€§æ³¨æ„åŠ›\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "seq_len = 64\n",
    "batch_size = 2\n",
    "\n",
    "test_input = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# æ ‡å‡†æ³¨æ„åŠ›\n",
    "standard_attn = MultiHeadAttention(d_model, num_heads)\n",
    "standard_output, _ = standard_attn(test_input, test_input, test_input)\n",
    "\n",
    "# çº¿æ€§æ³¨æ„åŠ›\n",
    "linear_attn = LinearAttention(d_model, num_heads)\n",
    "linear_output = linear_attn(test_input, test_input, test_input)\n",
    "\n",
    "# æ»‘åŠ¨çª—å£æ³¨æ„åŠ›\n",
    "sliding_attn = SlidingWindowAttention(d_model, num_heads, window_size=32)\n",
    "sliding_output = sliding_attn(test_input, test_input, test_input)\n",
    "\n",
    "print(f\"\\nå„ç§æ³¨æ„åŠ›æœºåˆ¶è¾“å‡ºå½¢çŠ¶å¯¹æ¯”:\")\n",
    "print(f\"è¾“å…¥: {test_input.shape}\")\n",
    "print(f\"æ ‡å‡†æ³¨æ„åŠ›è¾“å‡º: {standard_output.shape}\")\n",
    "print(f\"çº¿æ€§æ³¨æ„åŠ›è¾“å‡º: {linear_output.shape}\")\n",
    "print(f\"æ»‘åŠ¨çª—å£æ³¨æ„åŠ›è¾“å‡º: {sliding_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f7c8f9",
   "metadata": {},
   "source": [
    "## 7.7 Transformeråº”ç”¨é¢†åŸŸæ€»ç»“\n",
    "\n",
    "è®©æˆ‘ä»¬æ€»ç»“Transformeråœ¨å„ä¸ªé¢†åŸŸçš„åº”ç”¨ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f8b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# åˆ›å»ºTransformeråº”ç”¨é¢†åŸŸæ€»ç»“è¡¨\n",
    "transformer_applications = {\n",
    "    'é¢†åŸŸ': [\n",
    "        'è‡ªç„¶è¯­è¨€å¤„ç†', 'è‡ªç„¶è¯­è¨€å¤„ç†', 'è‡ªç„¶è¯­è¨€å¤„ç†', 'è‡ªç„¶è¯­è¨€å¤„ç†',\n",
    "        'è®¡ç®—æœºè§†è§‰', 'è®¡ç®—æœºè§†è§‰', 'è®¡ç®—æœºè§†è§‰',\n",
    "        'å¤šæ¨¡æ€', 'å¤šæ¨¡æ€', 'å¤šæ¨¡æ€',\n",
    "        'ç§‘å­¦è®¡ç®—', 'ç§‘å­¦è®¡ç®—', 'ç§‘å­¦è®¡ç®—',\n",
    "        'è¯­éŸ³å¤„ç†', 'è¯­éŸ³å¤„ç†',\n",
    "        'æ¨èç³»ç»Ÿ', 'æ—¶é—´åºåˆ—'\n",
    "    ],\n",
    "    'æ¨¡å‹åç§°': [\n",
    "        'BERT', 'GPT-3/4', 'T5', 'PaLM',\n",
    "        'ViT', 'DETR', 'Swin Transformer',\n",
    "        'CLIP', 'DALLE-2', 'GPT-4V',\n",
    "        'AlphaFold', 'ESM', 'ProtTrans',\n",
    "        'Whisper', 'Speech-T5',\n",
    "        'SASRec', 'Informer'\n",
    "    ],\n",
    "    'ä¸»è¦ä»»åŠ¡': [\n",
    "        'æ–‡æœ¬ç†è§£ã€åˆ†ç±»', 'æ–‡æœ¬ç”Ÿæˆ', 'æ–‡æœ¬åˆ°æ–‡æœ¬ç”Ÿæˆ', 'é€šç”¨è¯­è¨€ä»»åŠ¡',\n",
    "        'å›¾åƒåˆ†ç±»', 'ç›®æ ‡æ£€æµ‹', 'åˆ†å±‚è§†è§‰ä»»åŠ¡',\n",
    "        'å›¾æ–‡åŒ¹é…', 'æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ', 'å¤šæ¨¡æ€ç†è§£',\n",
    "        'è›‹ç™½è´¨ç»“æ„é¢„æµ‹', 'è›‹ç™½è´¨åºåˆ—åˆ†æ', 'è›‹ç™½è´¨åŠŸèƒ½é¢„æµ‹',\n",
    "        'è¯­éŸ³è¯†åˆ«', 'è¯­éŸ³åˆæˆ',\n",
    "        'åºåˆ—æ¨è', 'æ—¶é—´åºåˆ—é¢„æµ‹'\n",
    "    ],\n",
    "    'å…³é”®åˆ›æ–°': [\n",
    "        'åŒå‘ç¼–ç ', 'è‡ªå›å½’ç”Ÿæˆ', 'ç»Ÿä¸€ç¼–è§£ç ', 'è§„æ¨¡åŒ–è®­ç»ƒ',\n",
    "        'PatchåµŒå…¥', 'ç«¯åˆ°ç«¯æ£€æµ‹', 'åˆ†å±‚æ³¨æ„åŠ›',\n",
    "        'å¯¹æ¯”å­¦ä¹ ', 'æ‰©æ•£æ¨¡å‹ç»“åˆ', 'è§†è§‰è¯­è¨€èåˆ',\n",
    "        'å‡ ä½•çº¦æŸ', 'è¿›åŒ–ä¿¡æ¯åˆ©ç”¨', 'å¤šå°ºåº¦ç‰¹å¾',\n",
    "        'å¤šè¯­è¨€æ”¯æŒ', 'ç»Ÿä¸€æ¡†æ¶',\n",
    "        'åºåˆ—å»ºæ¨¡', 'é•¿åºåˆ—å¤„ç†'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(transformer_applications)\n",
    "\n",
    "print(\"Transformeråœ¨å„é¢†åŸŸçš„åº”ç”¨æ€»ç»“\")\n",
    "print(\"=\" * 80)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# ç»˜åˆ¶åº”ç”¨é¢†åŸŸåˆ†å¸ƒé¥¼å›¾\n",
    "domain_counts = df['é¢†åŸŸ'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['#FF9999', '#66B2FF', '#99FF99', '#FFCC99', '#FF99CC', '#99CCFF', '#FFB366']\n",
    "wedges, texts, autotexts = plt.pie(domain_counts.values, labels=domain_counts.index, \n",
    "                                  autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "\n",
    "plt.title('Transformeråº”ç”¨é¢†åŸŸåˆ†å¸ƒ', fontsize=16, pad=20)\n",
    "\n",
    "# ç¾åŒ–æ–‡å­—\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontweight('bold')\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "# å‘å±•æ—¶é—´çº¿\n",
    "timeline_data = {\n",
    "    'å¹´ä»½': [2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024],\n",
    "    'é‡è¦æ¨¡å‹/äº‹ä»¶': [\n",
    "        'Transformerè®ºæ–‡å‘å¸ƒ',\n",
    "        'BERTå‘å¸ƒï¼ŒNLPå¤§çªç ´',\n",
    "        'GPT-2å‘å¸ƒï¼Œç”Ÿæˆèƒ½åŠ›å±•ç°',\n",
    "        'GPT-3å‘å¸ƒï¼ŒViTè®ºæ–‡ï¼Œå¤šé¢†åŸŸæ‰©å±•',\n",
    "        'CLIPã€DALLEç­‰å¤šæ¨¡æ€æ¨¡å‹',\n",
    "        'ChatGPTå‘å¸ƒï¼ŒAIåº”ç”¨çˆ†å‘',\n",
    "        'GPT-4ã€å¤šæ¨¡æ€å¤§æ¨¡å‹',\n",
    "        'Soraã€æ›´å¼ºçš„å¤šæ¨¡æ€èƒ½åŠ›'\n",
    "    ],\n",
    "    'å‚æ•°è§„æ¨¡': [\n",
    "        '65M (Base)',\n",
    "        '340M (BERT-Large)',\n",
    "        '1.5B (GPT-2)',\n",
    "        '175B (GPT-3)',\n",
    "        '12B (CLIP)',\n",
    "        '175B (ChatGPT)',\n",
    "        'æœªå…¬å¸ƒ (GPT-4)',\n",
    "        'æœªå…¬å¸ƒ (Sora)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "timeline_df = pd.DataFrame(timeline_data)\n",
    "\n",
    "print(\"\\n\\nTransformerå‘å±•æ—¶é—´çº¿\")\n",
    "print(\"=\" * 80)\n",
    "print(timeline_df.to_string(index=False))\n",
    "\n",
    "# ç»˜åˆ¶å‚æ•°è§„æ¨¡å‘å±•è¶‹åŠ¿\n",
    "param_counts = [65, 340, 1500, 175000, 12000, 175000, 500000, 1000000]  # ä¼°ç®—å€¼ï¼Œå•ä½M\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(timeline_data['å¹´ä»½'][:6], param_counts[:6], 'bo-', linewidth=3, markersize=8)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('å¹´ä»½')\n",
    "plt.ylabel('å‚æ•°è§„æ¨¡ (ç™¾ä¸‡)')\n",
    "plt.title('Transformeræ¨¡å‹å‚æ•°è§„æ¨¡å‘å±•è¶‹åŠ¿')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# æ·»åŠ æ ‡æ³¨\n",
    "annotations = ['Transformer', 'BERT', 'GPT-2', 'GPT-3', 'CLIP', 'ChatGPT']\n",
    "for i, (year, params, label) in enumerate(zip(timeline_data['å¹´ä»½'][:6], param_counts[:6], annotations)):\n",
    "    plt.annotate(label, (year, params), textcoords=\"offset points\", \n",
    "                xytext=(0,10), ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f7c8f9",
   "metadata": {},
   "source": [
    "## 7.8 æœªæ¥å‘å±•è¶‹åŠ¿\n",
    "\n",
    "è®©æˆ‘ä»¬è®¨è®ºTransformerçš„æœªæ¥å‘å±•æ–¹å‘ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e7c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœªæ¥å‘å±•è¶‹åŠ¿åˆ†æ\n",
    "future_trends = {\n",
    "    'å‘å±•æ–¹å‘': [\n",
    "        'æ•ˆç‡ä¼˜åŒ–',\n",
    "        'å¤šæ¨¡æ€èåˆ',\n",
    "        'é•¿åºåˆ—å»ºæ¨¡',\n",
    "        'ç¨€ç–åŒ–æŠ€æœ¯',\n",
    "        'æ¶æ„åˆ›æ–°',\n",
    "        'è®­ç»ƒæ–¹æ³•',\n",
    "        'åº”ç”¨æ‹“å±•',\n",
    "        'å¯è§£é‡Šæ€§'\n",
    "    ],\n",
    "    'å…·ä½“æŠ€æœ¯': [\n",
    "        'çº¿æ€§æ³¨æ„åŠ›ã€Flash Attentionã€é‡åŒ–å‹ç¼©',\n",
    "        'CLIPå‡çº§ç‰ˆã€ç»Ÿä¸€å¤šæ¨¡æ€æ¶æ„',\n",
    "        'Longformerã€BigBirdã€æ–°ä½ç½®ç¼–ç ',\n",
    "        'MoEã€ç¨€ç–æ³¨æ„åŠ›ã€åŠ¨æ€è®¡ç®—',\n",
    "        'RetNetã€Mambaã€æ–°æ¿€æ´»å‡½æ•°',\n",
    "        'In-context Learningã€å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–',\n",
    "        'ç§‘å­¦è®¡ç®—ã€æœºå™¨äººã€æ¸¸æˆAI',\n",
    "        'æ³¨æ„åŠ›å¯è§†åŒ–ã€å› æœåˆ†æ'\n",
    "    ],\n",
    "    'æŒ‘æˆ˜': [\n",
    "        'ç»´æŒæ€§èƒ½çš„åŒæ—¶é™ä½è®¡ç®—æˆæœ¬',\n",
    "        'ä¸åŒæ¨¡æ€çš„æœ‰æ•ˆå¯¹é½å’Œèåˆ',\n",
    "        'å†…å­˜é™åˆ¶ã€ä½ç½®ç¼–ç å¤–æ¨',\n",
    "        'ç¨€ç–æ¨¡å¼å­¦ä¹ ã€ç¡¬ä»¶é€‚é…',\n",
    "        'è®­ç»ƒç¨³å®šæ€§ã€æ”¶æ•›é€Ÿåº¦',\n",
    "        'æ•°æ®æ•ˆç‡ã€å®‰å…¨å¯¹é½',\n",
    "        'é¢†åŸŸé€‚åº”ã€å®æ—¶æ¨ç†',\n",
    "        'é»‘ç›’é—®é¢˜ã€å†³ç­–é€æ˜åº¦'\n",
    "    ],\n",
    "    'é¢„æœŸå½±å“': [\n",
    "        'å¤§æ¨¡å‹æ°‘ä¸»åŒ–ã€è¾¹ç¼˜éƒ¨ç½²',\n",
    "        'é€šç”¨äººå·¥æ™ºèƒ½ã€åˆ›æ„å†…å®¹ç”Ÿæˆ',\n",
    "        'é•¿æ–‡æ¡£ç†è§£ã€å¤æ‚æ¨ç†',\n",
    "        'è¶…å¤§è§„æ¨¡æ¨¡å‹ã€ä¸ªæ€§åŒ–AI',\n",
    "        'æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ã€æ–°å…´ä»»åŠ¡',\n",
    "        'æ›´å®‰å…¨å¯é çš„AIç³»ç»Ÿ',\n",
    "        'ç§‘å­¦å‘ç°åŠ é€Ÿã€ç”Ÿäº§åŠ›é©å‘½',\n",
    "        'å¯ä¿¡AIã€è´Ÿè´£ä»»çš„AIéƒ¨ç½²'\n",
    "    ]\n",
    "}\n",
    "\n",
    "trends_df = pd.DataFrame(future_trends)\n",
    "\n",
    "print(\"Transformeræœªæ¥å‘å±•è¶‹åŠ¿åˆ†æ\")\n",
    "print(\"=\" * 100)\n",
    "print(trends_df.to_string(index=False, max_colwidth=30))\n",
    "\n",
    "# æŠ€æœ¯æˆç†Ÿåº¦æ›²çº¿\n",
    "technologies = [\n",
    "    'GPTç³»åˆ—', 'BERTç³»åˆ—', 'Vision Transformer', 'å¤šæ¨¡æ€Transformer',\n",
    "    'æ•ˆç‡ä¼˜åŒ–æŠ€æœ¯', 'é•¿åºåˆ—å»ºæ¨¡', 'ç¨€ç–Transformer', 'ç§‘å­¦è®¡ç®—åº”ç”¨'\n",
    "]\n",
    "\n",
    "maturity_scores = [0.9, 0.95, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3]  # æˆç†Ÿåº¦è¯„åˆ†\n",
    "adoption_scores = [0.9, 0.95, 0.7, 0.6, 0.4, 0.3, 0.2, 0.2]  # é‡‡ç”¨ç¨‹åº¦è¯„åˆ†\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(technologies)))\n",
    "\n",
    "for i, (tech, maturity, adoption, color) in enumerate(zip(technologies, maturity_scores, adoption_scores, colors)):\n",
    "    plt.scatter(maturity, adoption, s=200, c=[color], alpha=0.7, edgecolors='black')\n",
    "    plt.annotate(tech, (maturity, adoption), xytext=(5, 5), \n",
    "                textcoords='offset points', fontsize=9, \n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.xlabel('æŠ€æœ¯æˆç†Ÿåº¦', fontsize=12)\n",
    "plt.ylabel('å¸‚åœºé‡‡ç”¨ç¨‹åº¦', fontsize=12)\n",
    "plt.title('Transformerç›¸å…³æŠ€æœ¯æˆç†Ÿåº¦æ›²çº¿', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# æ·»åŠ è±¡é™æ ‡ç­¾\n",
    "plt.text(0.05, 0.95, 'æ–°å…´æŠ€æœ¯', fontsize=10, alpha=0.7, \n",
    "         bbox=dict(boxstyle='round,pad=0.3', facecolor='lightblue', alpha=0.5))\n",
    "plt.text(0.75, 0.95, 'æˆç†ŸæŠ€æœ¯', fontsize=10, alpha=0.7,\n",
    "         bbox=dict(boxstyle='round,pad=0.3', facecolor='lightgreen', alpha=0.5))\n",
    "plt.text(0.05, 0.05, 'å®éªŒé˜¶æ®µ', fontsize=10, alpha=0.7,\n",
    "         bbox=dict(boxstyle='round,pad=0.3', facecolor='lightyellow', alpha=0.5))\n",
    "plt.text(0.75, 0.05, 'è½åæŠ€æœ¯', fontsize=10, alpha=0.7,\n",
    "         bbox=dict(boxstyle='round,pad=0.3', facecolor='lightcoral', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nå…³é”®æ´å¯Ÿ:\")\n",
    "print(\"1. GPTå’ŒBERTç³»åˆ—å·²ç»éå¸¸æˆç†Ÿï¼Œå¹¿æ³›åº”ç”¨\")\n",
    "print(\"2. Vision Transformerå¿«é€Ÿå‘å±•ï¼Œæ­£åœ¨å–ä»£CNN\")\n",
    "print(\"3. å¤šæ¨¡æ€æŠ€æœ¯æ˜¯å½“å‰çƒ­ç‚¹ï¼Œæ½œåŠ›å·¨å¤§\")\n",
    "print(\"4. æ•ˆç‡ä¼˜åŒ–å’Œé•¿åºåˆ—å»ºæ¨¡æ˜¯æœªæ¥é‡ç‚¹\")\n",
    "print(\"5. ç¨€ç–åŒ–å’Œç§‘å­¦è®¡ç®—åº”ç”¨ä»åœ¨æ—©æœŸé˜¶æ®µ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f7c8d8",
   "metadata": {},
   "source": [
    "## æ€»ç»“\n",
    "\n",
    "åœ¨è¿™ä¸ªæœ€åçš„æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å…¨é¢æ¢ç´¢äº†Transformerçš„å˜ä½“å’Œåº”ç”¨ï¼š\n",
    "\n",
    "### ğŸ—ï¸ ä¸»è¦æ¶æ„å˜ä½“ï¼š\n",
    "1. **BERTç³»åˆ—**ï¼šåŒå‘ç¼–ç å™¨ï¼Œç†è§£ä»»åŠ¡çš„æ ‡æ†\n",
    "2. **GPTç³»åˆ—**ï¼šè‡ªå›å½’è§£ç å™¨ï¼Œç”Ÿæˆä»»åŠ¡çš„ç‹è€…\n",
    "3. **Vision Transformer**ï¼šå°†Transformerå¼•å…¥è®¡ç®—æœºè§†è§‰\n",
    "4. **æ•ˆç‡ä¼˜åŒ–å˜ä½“**ï¼šçº¿æ€§æ³¨æ„åŠ›ã€æ»‘åŠ¨çª—å£ç­‰\n",
    "\n",
    "### ğŸŒ åº”ç”¨é¢†åŸŸæ‰©å±•ï¼š\n",
    "- **è‡ªç„¶è¯­è¨€å¤„ç†**ï¼šä»ç†è§£åˆ°ç”Ÿæˆçš„å…¨é¢è¦†ç›–\n",
    "- **è®¡ç®—æœºè§†è§‰**ï¼šå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€åˆ†å‰²\n",
    "- **å¤šæ¨¡æ€å­¦ä¹ **ï¼šå›¾æ–‡ç†è§£ã€è§†é¢‘åˆ†æ\n",
    "- **ç§‘å­¦è®¡ç®—**ï¼šè›‹ç™½è´¨ã€ææ–™ã€æ°”å€™å»ºæ¨¡\n",
    "- **å…¶ä»–é¢†åŸŸ**ï¼šè¯­éŸ³ã€æ¨èã€æ—¶é—´åºåˆ—\n",
    "\n",
    "### ğŸš€ æŠ€æœ¯å‘å±•è¶‹åŠ¿ï¼š\n",
    "1. **æ•ˆç‡ä¼˜åŒ–**ï¼šé™ä½è®¡ç®—å¤æ‚åº¦ï¼Œå®ç°æ°‘ä¸»åŒ–éƒ¨ç½²\n",
    "2. **å¤šæ¨¡æ€èåˆ**ï¼šèµ°å‘é€šç”¨äººå·¥æ™ºèƒ½\n",
    "3. **é•¿åºåˆ—å»ºæ¨¡**ï¼šå¤„ç†æ›´å¤æ‚çš„æ¨ç†ä»»åŠ¡\n",
    "4. **æ¶æ„åˆ›æ–°**ï¼šæ¢ç´¢Transformerçš„æ›¿ä»£æ–¹æ¡ˆ\n",
    "\n",
    "### ğŸ’¡ å…³é”®æ´å¯Ÿï¼š\n",
    "- **ç»Ÿä¸€æ¶æ„çš„åŠ›é‡**ï¼šåŒä¸€å¥—æœºåˆ¶é€‚ç”¨äºå¤šç§ä»»åŠ¡å’Œæ¨¡æ€\n",
    "- **è§„æ¨¡åŒ–çš„æ•ˆåº”**ï¼šæ›´å¤§çš„æ¨¡å‹å¸¦æ¥æ„æƒ³ä¸åˆ°çš„èƒ½åŠ›æ¶Œç°\n",
    "- **æ³¨æ„åŠ›çš„é€šç”¨æ€§**ï¼šä¸ä»…ä»…æ˜¯åºåˆ—ï¼Œä»»ä½•å…³ç³»å»ºæ¨¡éƒ½å¯ä»¥ç”¨\n",
    "- **æ•ˆç‡ä¸æ€§èƒ½çš„å¹³è¡¡**ï¼šå®é™…åº”ç”¨éœ€è¦åœ¨ä¸¤è€…é—´æ‰¾åˆ°æœ€ä½³ç‚¹\n",
    "\n",
    "### ğŸ”® æœªæ¥å±•æœ›ï¼š\n",
    "Transformerå·²ç»ä»ä¸€ä¸ªæœºå™¨ç¿»è¯‘æ¨¡å‹å‘å±•æˆä¸ºAIé¢†åŸŸçš„åŸºç¡€æ¶æ„ã€‚æœªæ¥ï¼Œæˆ‘ä»¬å¯èƒ½çœ‹åˆ°ï¼š\n",
    "- æ›´åŠ é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶\n",
    "- çœŸæ­£çš„å¤šæ¨¡æ€ç»Ÿä¸€æ¨¡å‹\n",
    "- åœ¨ç§‘å­¦å‘ç°ä¸­çš„é‡å¤§çªç ´\n",
    "- å‘é€šç”¨äººå·¥æ™ºèƒ½çš„è¿›ä¸€æ­¥è¿ˆè¿›\n",
    "\n",
    "Transformerçš„æ•…äº‹è¿˜åœ¨ç»§ç»­ï¼Œå®ƒçš„å½±å“åŠ›å°†ç»§ç»­å¡‘é€ AIçš„æœªæ¥ã€‚é€šè¿‡è¿™ä¸ªå®Œæ•´çš„æ•™ç¨‹ç³»åˆ—ï¼Œä½ å·²ç»æŒæ¡äº†ç†è§£å’Œåº”ç”¨è¿™ä¸€é©å‘½æ€§æ¶æ„çš„æ‰€æœ‰åŸºç¡€çŸ¥è¯†ã€‚\n",
    "\n",
    "### ğŸ“ å­¦ä¹ å®Œæˆï¼\n",
    "æ­å–œä½ å®Œæˆäº†å®Œæ•´çš„Transformeræ•™ç¨‹ï¼ç°åœ¨ä½ å·²ç»å…·å¤‡äº†ï¼š\n",
    "- æ·±å…¥ç†è§£Transformerçš„å·¥ä½œåŸç†\n",
    "- å®ç°å„ç§Transformerå˜ä½“çš„èƒ½åŠ›\n",
    "- å°†Transformeråº”ç”¨åˆ°ä¸åŒé¢†åŸŸçš„çŸ¥è¯†\n",
    "- å¯¹æœªæ¥å‘å±•æ–¹å‘çš„å‰ç»æ€§è®¤è¯†\n",
    "\n",
    "ç»§ç»­æ¢ç´¢ï¼Œåœ¨AIçš„æµ·æ´‹ä¸­èˆªè¡Œï¼ğŸŒŠ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}