{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8b4c8d0",
   "metadata": {},
   "source": [
    "# 7. Transformer变体和现代应用\n",
    "\n",
    "在完成了完整Transformer的学习后，让我们探索Transformer家族的重要变体和现代应用。这些变体推动了NLP和其他领域的重大突破。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f8a2c0",
   "metadata": {},
   "source": [
    "## 7.1 Transformer家族概览\n",
    "\n",
    "自2017年原始Transformer发布以来，出现了众多重要的变体：\n",
    "\n",
    "### 按架构分类：\n",
    "1. **仅编码器架构**：BERT、RoBERTa、DeBERTa等\n",
    "2. **仅解码器架构**：GPT系列、PaLM、LLaMA等\n",
    "3. **编码器-解码器架构**：T5、BART、mT5等\n",
    "\n",
    "### 按应用领域分类：\n",
    "1. **自然语言处理**：BERT、GPT、T5等\n",
    "2. **计算机视觉**：Vision Transformer (ViT)、DETR等\n",
    "3. **多模态**：CLIP、DALLE、GPT-4等\n",
    "4. **科学计算**：AlphaFold、ESM等\n",
    "\n",
    "![Transformer家族树](images/transformer_family.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e7c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "# 从utils导入基础组件\n",
    "from utils import (\n",
    "    MultiHeadAttention, \n",
    "    SinusoidalPositionalEncoding, \n",
    "    FeedForwardNetwork,\n",
    "    TransformerBlock,\n",
    "    PreLNTransformerBlock,\n",
    "    create_padding_mask,\n",
    "    create_causal_mask,\n",
    "    setup_matplotlib_chinese,\n",
    "    set_random_seed\n",
    ")\n",
    "\n",
    "# 设置环境\n",
    "setup_matplotlib_chinese()\n",
    "set_random_seed(42)\n",
    "\n",
    "print(f\"PyTorch版本: {torch.__version__}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e7c4f5",
   "metadata": {},
   "source": [
    "## 7.2 BERT：双向编码器表示\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) 是最具影响力的Transformer变体之一：\n",
    "\n",
    "### 核心特点：\n",
    "- **仅编码器架构**：只使用Transformer的编码器部分\n",
    "- **双向上下文**：能够同时看到左边和右边的上下文\n",
    "- **预训练+微调**：在大规模语料上预训练，然后在具体任务上微调\n",
    "\n",
    "### 预训练任务：\n",
    "1. **掩码语言模型（MLM）**：随机掩盖一些词，让模型预测\n",
    "2. **下一句预测（NSP）**：预测两个句子是否连续"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b7f4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    简化的BERT模型实现\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, d_model: int = 768, num_heads: int = 12, \n",
    "                 d_ff: int = 3072, num_layers: int = 12, max_seq_len: int = 512, \n",
    "                 dropout: float = 0.1):\n",
    "        super(BERTModel, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # 嵌入层\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        self.token_type_embedding = nn.Embedding(2, d_model)  # 用于区分句子A和B\n",
    "        \n",
    "        # Transformer编码器层\n",
    "        self.layers = nn.ModuleList([\n",
    "            PreLNTransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # 输出层\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 用于MLM和NSP的头\n",
    "        self.mlm_head = nn.Linear(d_model, vocab_size)\n",
    "        self.nsp_head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(d_model, 2)\n",
    "        )\n",
    "        \n",
    "        # 特殊token索引\n",
    "        self.cls_token_id = 0  # [CLS]\n",
    "        self.sep_token_id = 1  # [SEP]\n",
    "        self.mask_token_id = 2  # [MASK]\n",
    "        self.pad_token_id = 3  # [PAD]\n",
    "        \n",
    "    def forward(self, input_ids: torch.Tensor, token_type_ids: Optional[torch.Tensor] = None,\n",
    "                attention_mask: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        Args:\n",
    "            input_ids: [batch_size, seq_len]\n",
    "            token_type_ids: [batch_size, seq_len] 句子类型ID\n",
    "            attention_mask: [batch_size, seq_len] 注意力掩码\n",
    "        \n",
    "        Returns:\n",
    "            last_hidden_state: [batch_size, seq_len, d_model]\n",
    "            pooler_output: [batch_size, d_model] 用于分类的[CLS]表示\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "        \n",
    "        # 创建位置ID\n",
    "        position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        # 如果没有提供token_type_ids，默认都是0\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "        \n",
    "        # 嵌入\n",
    "        token_embeds = self.token_embedding(input_ids)\n",
    "        position_embeds = self.position_embedding(position_ids)\n",
    "        token_type_embeds = self.token_type_embedding(token_type_ids)\n",
    "        \n",
    "        # 合并嵌入\n",
    "        embeddings = token_embeds + position_embeds + token_type_embeds\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        # 创建注意力掩码\n",
    "        if attention_mask is not None:\n",
    "            # 扩展掩码维度: [batch_size, 1, 1, seq_len]\n",
    "            extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "            extended_attention_mask = extended_attention_mask.expand(-1, -1, seq_len, -1)\n",
    "        else:\n",
    "            extended_attention_mask = None\n",
    "        \n",
    "        # 通过Transformer层\n",
    "        hidden_states = embeddings\n",
    "        attention_weights_list = []\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            hidden_states, attention_weights = layer(hidden_states, extended_attention_mask)\n",
    "            attention_weights_list.append(attention_weights)\n",
    "        \n",
    "        # 获取[CLS]的表示用于句子级任务\n",
    "        pooler_output = self.nsp_head(hidden_states[:, 0, :])  # [CLS]是第一个token\n",
    "        \n",
    "        return {\n",
    "            'last_hidden_state': hidden_states,\n",
    "            'pooler_output': pooler_output,\n",
    "            'attention_weights': attention_weights_list\n",
    "        }\n",
    "    \n",
    "    def get_mlm_predictions(self, hidden_states: torch.Tensor):\n",
    "        \"\"\"获取掩码语言模型的预测\"\"\"\n",
    "        return self.mlm_head(hidden_states)\n",
    "\n",
    "# 创建BERT模型\n",
    "bert_model = BERTModel(\n",
    "    vocab_size=1000,\n",
    "    d_model=256,  # 使用较小的维度用于演示\n",
    "    num_heads=8,\n",
    "    d_ff=1024,\n",
    "    num_layers=4,\n",
    "    max_seq_len=128\n",
    ")\n",
    "\n",
    "# 测试BERT模型\n",
    "batch_size = 2\n",
    "seq_len = 16\n",
    "\n",
    "# 模拟输入\n",
    "input_ids = torch.randint(4, 1000, (batch_size, seq_len))  # 避开特殊token\n",
    "input_ids[:, 0] = 0  # 设置[CLS]\n",
    "input_ids[:, seq_len//2] = 1  # 设置[SEP]\n",
    "\n",
    "attention_mask = torch.ones(batch_size, seq_len)\n",
    "token_type_ids = torch.zeros(batch_size, seq_len)\n",
    "token_type_ids[:, seq_len//2+1:] = 1  # 第二个句子\n",
    "\n",
    "# 前向传播\n",
    "outputs = bert_model(input_ids, token_type_ids, attention_mask)\n",
    "\n",
    "print(f\"BERT模型输出:\")\n",
    "print(f\"最后隐藏层: {outputs['last_hidden_state'].shape}\")\n",
    "print(f\"池化输出: {outputs['pooler_output'].shape}\")\n",
    "print(f\"注意力层数: {len(outputs['attention_weights'])}\")\n",
    "print(f\"模型参数量: {sum(p.numel() for p in bert_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8b4a3",
   "metadata": {},
   "source": [
    "## 7.3 GPT：生成式预训练Transformer\n",
    "\n",
    "GPT (Generative Pre-trained Transformer) 系列是另一个重要的Transformer变体：\n",
    "\n",
    "### 核心特点：\n",
    "- **仅解码器架构**：使用因果掩码的自注意力\n",
    "- **自回归生成**：从左到右逐个生成token\n",
    "- **语言建模**：预测下一个词的概率分布\n",
    "\n",
    "### GPT系列演进：\n",
    "- **GPT-1** (117M参数)：证明了预训练+微调的有效性\n",
    "- **GPT-2** (1.5B参数)：展示了规模化的威力\n",
    "- **GPT-3** (175B参数)：实现了Few-shot Learning\n",
    "- **GPT-4** (未公布参数)：多模态能力，更强的推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f7c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    简化的GPT模型实现\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, d_model: int = 768, num_heads: int = 12, \n",
    "                 d_ff: int = 3072, num_layers: int = 12, max_seq_len: int = 1024, \n",
    "                 dropout: float = 0.1):\n",
    "        super(GPTModel, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # 嵌入层\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "        # Transformer解码器层（使用因果掩码）\n",
    "        self.layers = nn.ModuleList([\n",
    "            PreLNTransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # 输出层\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 语言建模头\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        \n",
    "        # 权重共享（可选）\n",
    "        # self.lm_head.weight = self.token_embedding.weight\n",
    "        \n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        Args:\n",
    "            input_ids: [batch_size, seq_len]\n",
    "            attention_mask: [batch_size, seq_len]\n",
    "        \n",
    "        Returns:\n",
    "            logits: [batch_size, seq_len, vocab_size]\n",
    "            hidden_states: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "        \n",
    "        # 创建位置ID\n",
    "        position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        # 嵌入\n",
    "        token_embeds = self.token_embedding(input_ids)\n",
    "        position_embeds = self.position_embedding(position_ids)\n",
    "        \n",
    "        hidden_states = token_embeds + position_embeds\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        \n",
    "        # 创建因果掩码\n",
    "        causal_mask = create_causal_mask(seq_len).to(input_ids.device)\n",
    "        causal_mask = causal_mask.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        \n",
    "        # 结合填充掩码（如果提供）\n",
    "        if attention_mask is not None:\n",
    "            padding_mask = attention_mask.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "            combined_mask = causal_mask & padding_mask\n",
    "        else:\n",
    "            combined_mask = causal_mask\n",
    "        \n",
    "        # 通过Transformer层\n",
    "        attention_weights_list = []\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            hidden_states, attention_weights = layer(hidden_states, combined_mask)\n",
    "            attention_weights_list.append(attention_weights)\n",
    "        \n",
    "        hidden_states = self.layer_norm(hidden_states)\n",
    "        \n",
    "        # 语言建模输出\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        \n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'hidden_states': hidden_states,\n",
    "            'attention_weights': attention_weights_list\n",
    "        }\n",
    "    \n",
    "    def generate(self, input_ids: torch.Tensor, max_new_tokens: int = 50, \n",
    "                 temperature: float = 1.0, top_k: int = None, top_p: float = None):\n",
    "        \"\"\"\n",
    "        文本生成方法\n",
    "        \n",
    "        Args:\n",
    "            input_ids: 输入的token序列\n",
    "            max_new_tokens: 最大生成的新token数\n",
    "            temperature: 温度参数，控制随机性\n",
    "            top_k: Top-K采样\n",
    "            top_p: Top-P采样（核采样）\n",
    "        \n",
    "        Returns:\n",
    "            generated_ids: 生成的完整序列\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        generated_ids = input_ids.clone()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                # 如果序列太长，截断\n",
    "                if generated_ids.size(1) > self.max_seq_len:\n",
    "                    generated_ids = generated_ids[:, -self.max_seq_len:]\n",
    "                \n",
    "                # 前向传播\n",
    "                outputs = self.forward(generated_ids)\n",
    "                logits = outputs['logits'][:, -1, :] / temperature  # 取最后一个位置的logits\n",
    "                \n",
    "                # 采样策略\n",
    "                if top_k is not None:\n",
    "                    # Top-K采样\n",
    "                    top_k_logits, top_k_indices = torch.topk(logits, top_k)\n",
    "                    logits = torch.full_like(logits, float('-inf'))\n",
    "                    logits.scatter_(1, top_k_indices, top_k_logits)\n",
    "                \n",
    "                if top_p is not None:\n",
    "                    # Top-P采样（核采样）\n",
    "                    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                    \n",
    "                    # 移除累积概率超过top_p的token\n",
    "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                    sorted_indices_to_remove[..., 0] = 0\n",
    "                    \n",
    "                    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "                    logits[indices_to_remove] = float('-inf')\n",
    "                \n",
    "                # 计算概率并采样\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                \n",
    "                # 添加到生成序列\n",
    "                generated_ids = torch.cat([generated_ids, next_token], dim=1)\n",
    "        \n",
    "        return generated_ids\n",
    "\n",
    "# 创建GPT模型\n",
    "gpt_model = GPTModel(\n",
    "    vocab_size=1000,\n",
    "    d_model=256,  # 使用较小的维度用于演示\n",
    "    num_heads=8,\n",
    "    d_ff=1024,\n",
    "    num_layers=4,\n",
    "    max_seq_len=128\n",
    ")\n",
    "\n",
    "# 测试GPT模型\n",
    "input_ids = torch.randint(0, 1000, (1, 10))\n",
    "outputs = gpt_model(input_ids)\n",
    "\n",
    "print(f\"GPT模型输出:\")\n",
    "print(f\"Logits形状: {outputs['logits'].shape}\")\n",
    "print(f\"隐藏状态形状: {outputs['hidden_states'].shape}\")\n",
    "print(f\"模型参数量: {sum(p.numel() for p in gpt_model.parameters()):,}\")\n",
    "\n",
    "# 测试文本生成\n",
    "generated = gpt_model.generate(input_ids, max_new_tokens=10, temperature=0.8, top_k=50)\n",
    "print(f\"\\n生成序列长度: {generated.shape[1]} (原长度: {input_ids.shape[1]})\")\n",
    "print(f\"生成的token: {generated[0].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b8b2f5",
   "metadata": {},
   "source": [
    "## 7.4 Vision Transformer (ViT)\n",
    "\n",
    "Vision Transformer将Transformer成功应用到计算机视觉领域：\n",
    "\n",
    "### 核心思想：\n",
    "1. **图像分块**：将图像分割为固定大小的patch\n",
    "2. **线性嵌入**：将每个patch通过线性层映射为token\n",
    "3. **位置编码**：为每个patch添加位置信息\n",
    "4. **Transformer编码**：使用标准的Transformer编码器\n",
    "5. **分类头**：使用[CLS] token进行图像分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f8b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer (ViT) 实现\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size: int = 224, patch_size: int = 16, num_classes: int = 1000,\n",
    "                 d_model: int = 768, num_heads: int = 12, d_ff: int = 3072, \n",
    "                 num_layers: int = 12, channels: int = 3, dropout: float = 0.1):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        \n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Patch嵌入\n",
    "        self.patch_embedding = nn.Conv2d(\n",
    "            channels, d_model, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "        \n",
    "        # 可学习的[CLS] token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        \n",
    "        # 位置编码\n",
    "        self.position_embedding = nn.Parameter(\n",
    "            torch.randn(1, self.num_patches + 1, d_model)\n",
    "        )\n",
    "        \n",
    "        # Transformer编码器\n",
    "        self.layers = nn.ModuleList([\n",
    "            PreLNTransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # 输出层\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 分类头\n",
    "        self.classification_head = nn.Linear(d_model, num_classes)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        Args:\n",
    "            x: [batch_size, channels, height, width]\n",
    "        \n",
    "        Returns:\n",
    "            logits: [batch_size, num_classes]\n",
    "            attention_weights: 注意力权重列表\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # 1. Patch嵌入\n",
    "        # [batch_size, d_model, num_patches_h, num_patches_w]\n",
    "        patch_embeds = self.patch_embedding(x)\n",
    "        \n",
    "        # 重塑为序列: [batch_size, num_patches, d_model]\n",
    "        patch_embeds = patch_embeds.flatten(2).transpose(1, 2)\n",
    "        \n",
    "        # 2. 添加[CLS] token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        # [batch_size, num_patches + 1, d_model]\n",
    "        token_embeds = torch.cat([cls_tokens, patch_embeds], dim=1)\n",
    "        \n",
    "        # 3. 添加位置编码\n",
    "        token_embeds = token_embeds + self.position_embedding\n",
    "        token_embeds = self.dropout(token_embeds)\n",
    "        \n",
    "        # 4. 通过Transformer层\n",
    "        hidden_states = token_embeds\n",
    "        attention_weights_list = []\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            hidden_states, attention_weights = layer(hidden_states)\n",
    "            attention_weights_list.append(attention_weights)\n",
    "        \n",
    "        hidden_states = self.layer_norm(hidden_states)\n",
    "        \n",
    "        # 5. 分类\n",
    "        # 使用[CLS] token（第一个位置）进行分类\n",
    "        cls_output = hidden_states[:, 0, :]\n",
    "        logits = self.classification_head(cls_output)\n",
    "        \n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'hidden_states': hidden_states,\n",
    "            'attention_weights': attention_weights_list\n",
    "        }\n",
    "\n",
    "# 创建ViT模型\n",
    "vit_model = VisionTransformer(\n",
    "    img_size=224,\n",
    "    patch_size=16,\n",
    "    num_classes=10,  # 简化为10类\n",
    "    d_model=256,     # 使用较小的维度用于演示\n",
    "    num_heads=8,\n",
    "    d_ff=1024,\n",
    "    num_layers=4,\n",
    "    channels=3\n",
    ")\n",
    "\n",
    "# 测试ViT模型\n",
    "batch_size = 2\n",
    "test_images = torch.randn(batch_size, 3, 224, 224)\n",
    "\n",
    "outputs = vit_model(test_images)\n",
    "\n",
    "print(f\"Vision Transformer输出:\")\n",
    "print(f\"输入图像形状: {test_images.shape}\")\n",
    "print(f\"分类logits形状: {outputs['logits'].shape}\")\n",
    "print(f\"隐藏状态形状: {outputs['hidden_states'].shape}\")\n",
    "print(f\"Patch数量: {vit_model.num_patches}\")\n",
    "print(f\"序列长度: {vit_model.num_patches + 1} (包含CLS token)\")\n",
    "print(f\"模型参数量: {sum(p.numel() for p in vit_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f7c8f6",
   "metadata": {},
   "source": [
    "## 7.5 可视化ViT的注意力模式\n",
    "\n",
    "让我们可视化Vision Transformer学到的注意力模式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b3b5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_vit_attention(model, image, layer_idx=-1, head_idx=0):\n",
    "    \"\"\"\n",
    "    可视化ViT的注意力模式\n",
    "    \n",
    "    Args:\n",
    "        model: ViT模型\n",
    "        image: 输入图像 [1, 3, H, W]\n",
    "        layer_idx: 要可视化的层索引\n",
    "        head_idx: 要可视化的头索引\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        \n",
    "        # 获取指定层和头的注意力权重\n",
    "        attention_weights = outputs['attention_weights'][layer_idx]  # [1, num_heads, seq_len, seq_len]\n",
    "        attention_map = attention_weights[0, head_idx].cpu().numpy()  # [seq_len, seq_len]\n",
    "        \n",
    "        # 获取[CLS] token对所有patch的注意力\n",
    "        cls_attention = attention_map[0, 1:]  # 去掉[CLS] token自己\n",
    "        \n",
    "        # 重塑为图像形状\n",
    "        num_patches_per_side = int(np.sqrt(len(cls_attention)))\n",
    "        attention_image = cls_attention.reshape(num_patches_per_side, num_patches_per_side)\n",
    "        \n",
    "        # 可视化\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        # 原始图像\n",
    "        if image.shape[1] == 3:  # RGB图像\n",
    "            img_np = image[0].permute(1, 2, 0).cpu().numpy()\n",
    "            img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())  # 归一化到[0,1]\n",
    "            axes[0].imshow(img_np)\n",
    "        else:\n",
    "            axes[0].imshow(image[0, 0].cpu().numpy(), cmap='gray')\n",
    "        axes[0].set_title('原始图像')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # [CLS]注意力热力图\n",
    "        im1 = axes[1].imshow(attention_image, cmap='hot', interpolation='nearest')\n",
    "        axes[1].set_title(f'[CLS] Token注意力\\n(层{layer_idx}, 头{head_idx})')\n",
    "        axes[1].axis('off')\n",
    "        plt.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # 完整注意力矩阵\n",
    "        im2 = axes[2].imshow(attention_map, cmap='Blues')\n",
    "        axes[2].set_title('完整注意力矩阵')\n",
    "        axes[2].set_xlabel('Key位置')\n",
    "        axes[2].set_ylabel('Query位置')\n",
    "        plt.colorbar(im2, ax=axes[2], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return attention_image, attention_map\n",
    "\n",
    "# 创建一个简单的测试图像（中心有一个亮块）\n",
    "test_image = torch.zeros(1, 3, 224, 224)\n",
    "test_image[:, :, 80:144, 80:144] = 1.0  # 中心亮块\n",
    "test_image[:, :, 100:124, 100:124] = 0.5  # 中心更亮的小块\n",
    "\n",
    "# 可视化注意力\n",
    "attention_img, attention_full = visualize_vit_attention(vit_model, test_image, layer_idx=-1, head_idx=0)\n",
    "\n",
    "print(f\"注意力图像形状: {attention_img.shape}\")\n",
    "print(f\"完整注意力矩阵形状: {attention_full.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f7c8f8",
   "metadata": {},
   "source": [
    "## 7.6 效率优化变体\n",
    "\n",
    "随着Transformer模型规模的增长，出现了许多专注于效率优化的变体："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    线性注意力机制\n",
    "    将复杂度从O(n²)降低到O(n)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        super(LinearAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):\n",
    "        batch_size, seq_len, _ = query.size()\n",
    "        \n",
    "        Q = self.W_q(query).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(key).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(value).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # 应用特征映射函数（这里使用ReLU作为示例）\n",
    "        Q = F.relu(Q)\n",
    "        K = F.relu(K)\n",
    "        \n",
    "        # 线性注意力计算: O(n) 复杂度\n",
    "        # 计算 K^T V\n",
    "        KV = torch.matmul(K.transpose(-2, -1), V)  # [batch, heads, d_k, d_k]\n",
    "        \n",
    "        # 计算 Q (K^T V)\n",
    "        output = torch.matmul(Q, KV)  # [batch, heads, seq_len, d_k]\n",
    "        \n",
    "        # 归一化\n",
    "        normalizer = torch.matmul(Q, K.sum(dim=-2, keepdim=True).transpose(-2, -1))\n",
    "        output = output / (normalizer + 1e-6)\n",
    "        \n",
    "        # 重塑和输出投影\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class SlidingWindowAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    滑动窗口注意力\n",
    "    只关注局部窗口内的token，降低复杂度\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int, window_size: int = 128, dropout: float = 0.1):\n",
    "        super(SlidingWindowAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):\n",
    "        batch_size, seq_len, _ = query.size()\n",
    "        \n",
    "        Q = self.W_q(query).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(key).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(value).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # 创建滑动窗口掩码\n",
    "        mask = torch.zeros(seq_len, seq_len, device=query.device)\n",
    "        for i in range(seq_len):\n",
    "            start = max(0, i - self.window_size // 2)\n",
    "            end = min(seq_len, i + self.window_size // 2 + 1)\n",
    "            mask[i, start:end] = 1\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # 应用滑动窗口掩码\n",
    "        scores = scores.masked_fill(mask.unsqueeze(0).unsqueeze(0) == 0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# 比较不同注意力机制的复杂度\n",
    "def compare_attention_complexity():\n",
    "    \"\"\"\n",
    "    比较不同注意力机制的计算复杂度\n",
    "    \"\"\"\n",
    "    seq_lengths = [128, 256, 512, 1024, 2048]\n",
    "    d_model = 512\n",
    "    \n",
    "    # 计算复杂度（简化，以FLOPs为单位）\n",
    "    standard_complexity = []\n",
    "    linear_complexity = []\n",
    "    sliding_window_complexity = []\n",
    "    \n",
    "    window_size = 128\n",
    "    \n",
    "    for n in seq_lengths:\n",
    "        # 标准注意力: O(n² * d)\n",
    "        standard = n * n * d_model\n",
    "        \n",
    "        # 线性注意力: O(n * d²)\n",
    "        linear = n * d_model * d_model\n",
    "        \n",
    "        # 滑动窗口注意力: O(n * w * d), w为窗口大小\n",
    "        sliding = n * window_size * d_model\n",
    "        \n",
    "        standard_complexity.append(standard / 1e9)  # 转换为G FLOPs\n",
    "        linear_complexity.append(linear / 1e9)\n",
    "        sliding_window_complexity.append(sliding / 1e9)\n",
    "    \n",
    "    # 绘制复杂度对比\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.plot(seq_lengths, standard_complexity, 'r-o', label='标准注意力 O(n²d)', linewidth=2)\n",
    "    plt.plot(seq_lengths, linear_complexity, 'g-s', label='线性注意力 O(nd²)', linewidth=2)\n",
    "    plt.plot(seq_lengths, sliding_window_complexity, 'b-^', label=f'滑动窗口注意力 O(nwd), w={window_size}', linewidth=2)\n",
    "    \n",
    "    plt.xlabel('序列长度')\n",
    "    plt.ylabel('计算复杂度 (G FLOPs)')\n",
    "    plt.title('不同注意力机制的计算复杂度对比')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n",
    "    \n",
    "    # 打印复杂度表格\n",
    "    print(\"不同注意力机制的计算复杂度对比 (G FLOPs):\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'序列长度':<10} {'标准注意力':<15} {'线性注意力':<15} {'滑动窗口':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, n in enumerate(seq_lengths):\n",
    "        print(f\"{n:<10} {standard_complexity[i]:<15.2f} {linear_complexity[i]:<15.2f} {sliding_window_complexity[i]:<15.2f}\")\n",
    "\n",
    "# 运行复杂度对比\n",
    "compare_attention_complexity()\n",
    "\n",
    "# 测试线性注意力\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "seq_len = 64\n",
    "batch_size = 2\n",
    "\n",
    "test_input = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# 标准注意力\n",
    "standard_attn = MultiHeadAttention(d_model, num_heads)\n",
    "standard_output, _ = standard_attn(test_input, test_input, test_input)\n",
    "\n",
    "# 线性注意力\n",
    "linear_attn = LinearAttention(d_model, num_heads)\n",
    "linear_output = linear_attn(test_input, test_input, test_input)\n",
    "\n",
    "# 滑动窗口注意力\n",
    "sliding_attn = SlidingWindowAttention(d_model, num_heads, window_size=32)\n",
    "sliding_output = sliding_attn(test_input, test_input, test_input)\n",
    "\n",
    "print(f\"\\n各种注意力机制输出形状对比:\")\n",
    "print(f\"输入: {test_input.shape}\")\n",
    "print(f\"标准注意力输出: {standard_output.shape}\")\n",
    "print(f\"线性注意力输出: {linear_output.shape}\")\n",
    "print(f\"滑动窗口注意力输出: {sliding_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f7c8f9",
   "metadata": {},
   "source": [
    "## 7.7 Transformer应用领域总结\n",
    "\n",
    "让我们总结Transformer在各个领域的应用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f8b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 创建Transformer应用领域总结表\n",
    "transformer_applications = {\n",
    "    '领域': [\n",
    "        '自然语言处理', '自然语言处理', '自然语言处理', '自然语言处理',\n",
    "        '计算机视觉', '计算机视觉', '计算机视觉',\n",
    "        '多模态', '多模态', '多模态',\n",
    "        '科学计算', '科学计算', '科学计算',\n",
    "        '语音处理', '语音处理',\n",
    "        '推荐系统', '时间序列'\n",
    "    ],\n",
    "    '模型名称': [\n",
    "        'BERT', 'GPT-3/4', 'T5', 'PaLM',\n",
    "        'ViT', 'DETR', 'Swin Transformer',\n",
    "        'CLIP', 'DALLE-2', 'GPT-4V',\n",
    "        'AlphaFold', 'ESM', 'ProtTrans',\n",
    "        'Whisper', 'Speech-T5',\n",
    "        'SASRec', 'Informer'\n",
    "    ],\n",
    "    '主要任务': [\n",
    "        '文本理解、分类', '文本生成', '文本到文本生成', '通用语言任务',\n",
    "        '图像分类', '目标检测', '分层视觉任务',\n",
    "        '图文匹配', '文本到图像生成', '多模态理解',\n",
    "        '蛋白质结构预测', '蛋白质序列分析', '蛋白质功能预测',\n",
    "        '语音识别', '语音合成',\n",
    "        '序列推荐', '时间序列预测'\n",
    "    ],\n",
    "    '关键创新': [\n",
    "        '双向编码', '自回归生成', '统一编解码', '规模化训练',\n",
    "        'Patch嵌入', '端到端检测', '分层注意力',\n",
    "        '对比学习', '扩散模型结合', '视觉语言融合',\n",
    "        '几何约束', '进化信息利用', '多尺度特征',\n",
    "        '多语言支持', '统一框架',\n",
    "        '序列建模', '长序列处理'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(transformer_applications)\n",
    "\n",
    "print(\"Transformer在各领域的应用总结\")\n",
    "print(\"=\" * 80)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# 绘制应用领域分布饼图\n",
    "domain_counts = df['领域'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['#FF9999', '#66B2FF', '#99FF99', '#FFCC99', '#FF99CC', '#99CCFF', '#FFB366']\n",
    "wedges, texts, autotexts = plt.pie(domain_counts.values, labels=domain_counts.index, \n",
    "                                  autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "\n",
    "plt.title('Transformer应用领域分布', fontsize=16, pad=20)\n",
    "\n",
    "# 美化文字\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontweight('bold')\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "# 发展时间线\n",
    "timeline_data = {\n",
    "    '年份': [2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024],\n",
    "    '重要模型/事件': [\n",
    "        'Transformer论文发布',\n",
    "        'BERT发布，NLP大突破',\n",
    "        'GPT-2发布，生成能力展现',\n",
    "        'GPT-3发布，ViT论文，多领域扩展',\n",
    "        'CLIP、DALLE等多模态模型',\n",
    "        'ChatGPT发布，AI应用爆发',\n",
    "        'GPT-4、多模态大模型',\n",
    "        'Sora、更强的多模态能力'\n",
    "    ],\n",
    "    '参数规模': [\n",
    "        '65M (Base)',\n",
    "        '340M (BERT-Large)',\n",
    "        '1.5B (GPT-2)',\n",
    "        '175B (GPT-3)',\n",
    "        '12B (CLIP)',\n",
    "        '175B (ChatGPT)',\n",
    "        '未公布 (GPT-4)',\n",
    "        '未公布 (Sora)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "timeline_df = pd.DataFrame(timeline_data)\n",
    "\n",
    "print(\"\\n\\nTransformer发展时间线\")\n",
    "print(\"=\" * 80)\n",
    "print(timeline_df.to_string(index=False))\n",
    "\n",
    "# 绘制参数规模发展趋势\n",
    "param_counts = [65, 340, 1500, 175000, 12000, 175000, 500000, 1000000]  # 估算值，单位M\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(timeline_data['年份'][:6], param_counts[:6], 'bo-', linewidth=3, markersize=8)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('年份')\n",
    "plt.ylabel('参数规模 (百万)')\n",
    "plt.title('Transformer模型参数规模发展趋势')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 添加标注\n",
    "annotations = ['Transformer', 'BERT', 'GPT-2', 'GPT-3', 'CLIP', 'ChatGPT']\n",
    "for i, (year, params, label) in enumerate(zip(timeline_data['年份'][:6], param_counts[:6], annotations)):\n",
    "    plt.annotate(label, (year, params), textcoords=\"offset points\", \n",
    "                xytext=(0,10), ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f7c8f9",
   "metadata": {},
   "source": [
    "## 7.8 未来发展趋势\n",
    "\n",
    "让我们讨论Transformer的未来发展方向："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e7c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 未来发展趋势分析\n",
    "future_trends = {\n",
    "    '发展方向': [\n",
    "        '效率优化',\n",
    "        '多模态融合',\n",
    "        '长序列建模',\n",
    "        '稀疏化技术',\n",
    "        '架构创新',\n",
    "        '训练方法',\n",
    "        '应用拓展',\n",
    "        '可解释性'\n",
    "    ],\n",
    "    '具体技术': [\n",
    "        '线性注意力、Flash Attention、量化压缩',\n",
    "        'CLIP升级版、统一多模态架构',\n",
    "        'Longformer、BigBird、新位置编码',\n",
    "        'MoE、稀疏注意力、动态计算',\n",
    "        'RetNet、Mamba、新激活函数',\n",
    "        'In-context Learning、强化学习优化',\n",
    "        '科学计算、机器人、游戏AI',\n",
    "        '注意力可视化、因果分析'\n",
    "    ],\n",
    "    '挑战': [\n",
    "        '维持性能的同时降低计算成本',\n",
    "        '不同模态的有效对齐和融合',\n",
    "        '内存限制、位置编码外推',\n",
    "        '稀疏模式学习、硬件适配',\n",
    "        '训练稳定性、收敛速度',\n",
    "        '数据效率、安全对齐',\n",
    "        '领域适应、实时推理',\n",
    "        '黑盒问题、决策透明度'\n",
    "    ],\n",
    "    '预期影响': [\n",
    "        '大模型民主化、边缘部署',\n",
    "        '通用人工智能、创意内容生成',\n",
    "        '长文档理解、复杂推理',\n",
    "        '超大规模模型、个性化AI',\n",
    "        '更强的表达能力、新兴任务',\n",
    "        '更安全可靠的AI系统',\n",
    "        '科学发现加速、生产力革命',\n",
    "        '可信AI、负责任的AI部署'\n",
    "    ]\n",
    "}\n",
    "\n",
    "trends_df = pd.DataFrame(future_trends)\n",
    "\n",
    "print(\"Transformer未来发展趋势分析\")\n",
    "print(\"=\" * 100)\n",
    "print(trends_df.to_string(index=False, max_colwidth=30))\n",
    "\n",
    "# 技术成熟度曲线\n",
    "technologies = [\n",
    "    'GPT系列', 'BERT系列', 'Vision Transformer', '多模态Transformer',\n",
    "    '效率优化技术', '长序列建模', '稀疏Transformer', '科学计算应用'\n",
    "]\n",
    "\n",
    "maturity_scores = [0.9, 0.95, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3]  # 成熟度评分\n",
    "adoption_scores = [0.9, 0.95, 0.7, 0.6, 0.4, 0.3, 0.2, 0.2]  # 采用程度评分\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(technologies)))\n",
    "\n",
    "for i, (tech, maturity, adoption, color) in enumerate(zip(technologies, maturity_scores, adoption_scores, colors)):\n",
    "    plt.scatter(maturity, adoption, s=200, c=[color], alpha=0.7, edgecolors='black')\n",
    "    plt.annotate(tech, (maturity, adoption), xytext=(5, 5), \n",
    "                textcoords='offset points', fontsize=9, \n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.xlabel('技术成熟度', fontsize=12)\n",
    "plt.ylabel('市场采用程度', fontsize=12)\n",
    "plt.title('Transformer相关技术成熟度曲线', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# 添加象限标签\n",
    "plt.text(0.05, 0.95, '新兴技术', fontsize=10, alpha=0.7, \n",
    "         bbox=dict(boxstyle='round,pad=0.3', facecolor='lightblue', alpha=0.5))\n",
    "plt.text(0.75, 0.95, '成熟技术', fontsize=10, alpha=0.7,\n",
    "         bbox=dict(boxstyle='round,pad=0.3', facecolor='lightgreen', alpha=0.5))\n",
    "plt.text(0.05, 0.05, '实验阶段', fontsize=10, alpha=0.7,\n",
    "         bbox=dict(boxstyle='round,pad=0.3', facecolor='lightyellow', alpha=0.5))\n",
    "plt.text(0.75, 0.05, '落后技术', fontsize=10, alpha=0.7,\n",
    "         bbox=dict(boxstyle='round,pad=0.3', facecolor='lightcoral', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n关键洞察:\")\n",
    "print(\"1. GPT和BERT系列已经非常成熟，广泛应用\")\n",
    "print(\"2. Vision Transformer快速发展，正在取代CNN\")\n",
    "print(\"3. 多模态技术是当前热点，潜力巨大\")\n",
    "print(\"4. 效率优化和长序列建模是未来重点\")\n",
    "print(\"5. 稀疏化和科学计算应用仍在早期阶段\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f7c8d8",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "在这个最后的教程中，我们全面探索了Transformer的变体和应用：\n",
    "\n",
    "### 🏗️ 主要架构变体：\n",
    "1. **BERT系列**：双向编码器，理解任务的标杆\n",
    "2. **GPT系列**：自回归解码器，生成任务的王者\n",
    "3. **Vision Transformer**：将Transformer引入计算机视觉\n",
    "4. **效率优化变体**：线性注意力、滑动窗口等\n",
    "\n",
    "### 🌍 应用领域扩展：\n",
    "- **自然语言处理**：从理解到生成的全面覆盖\n",
    "- **计算机视觉**：图像分类、目标检测、分割\n",
    "- **多模态学习**：图文理解、视频分析\n",
    "- **科学计算**：蛋白质、材料、气候建模\n",
    "- **其他领域**：语音、推荐、时间序列\n",
    "\n",
    "### 🚀 技术发展趋势：\n",
    "1. **效率优化**：降低计算复杂度，实现民主化部署\n",
    "2. **多模态融合**：走向通用人工智能\n",
    "3. **长序列建模**：处理更复杂的推理任务\n",
    "4. **架构创新**：探索Transformer的替代方案\n",
    "\n",
    "### 💡 关键洞察：\n",
    "- **统一架构的力量**：同一套机制适用于多种任务和模态\n",
    "- **规模化的效应**：更大的模型带来意想不到的能力涌现\n",
    "- **注意力的通用性**：不仅仅是序列，任何关系建模都可以用\n",
    "- **效率与性能的平衡**：实际应用需要在两者间找到最佳点\n",
    "\n",
    "### 🔮 未来展望：\n",
    "Transformer已经从一个机器翻译模型发展成为AI领域的基础架构。未来，我们可能看到：\n",
    "- 更加高效的注意力机制\n",
    "- 真正的多模态统一模型\n",
    "- 在科学发现中的重大突破\n",
    "- 向通用人工智能的进一步迈进\n",
    "\n",
    "Transformer的故事还在继续，它的影响力将继续塑造AI的未来。通过这个完整的教程系列，你已经掌握了理解和应用这一革命性架构的所有基础知识。\n",
    "\n",
    "### 🎓 学习完成！\n",
    "恭喜你完成了完整的Transformer教程！现在你已经具备了：\n",
    "- 深入理解Transformer的工作原理\n",
    "- 实现各种Transformer变体的能力\n",
    "- 将Transformer应用到不同领域的知识\n",
    "- 对未来发展方向的前瞻性认识\n",
    "\n",
    "继续探索，在AI的海洋中航行！🌊"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}