{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8b4c8d0",
   "metadata": {},
   "source": [
    "# 5. Transformer基本块 (Transformer Block)\n",
    "\n",
    "在前面的教程中，我们学习了Transformer的核心组件：注意力机制、多头注意力和位置编码。现在是时候将这些组件组合成完整的Transformer块了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f8a2c0",
   "metadata": {},
   "source": [
    "## 5.1 Transformer块的整体架构\n",
    "\n",
    "一个标准的Transformer块包含以下组件：\n",
    "\n",
    "1. **多头自注意力层** (Multi-Head Self-Attention)\n",
    "2. **残差连接和层归一化** (Residual Connection & Layer Normalization)\n",
    "3. **前馈神经网络** (Feed-Forward Network)\n",
    "4. **再次残差连接和层归一化**\n",
    "\n",
    "### 数学表示：\n",
    "$$\\text{output}_1 = \\text{LayerNorm}(x + \\text{MultiHeadAttention}(x))$$\n",
    "$$\\text{output}_2 = \\text{LayerNorm}(\\text{output}_1 + \\text{FFN}(\\text{output}_1))$$\n",
    "\n",
    "![Transformer块架构图](images/transformer_block.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e7c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "# 设置随机种子和图表样式\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(f\"PyTorch版本: {torch.__version__}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e7c4f5",
   "metadata": {},
   "source": [
    "## 5.2 前馈神经网络 (Feed-Forward Network)\n",
    "\n",
    "前馈网络是Transformer块中的重要组件，通常结构为：\n",
    "- 线性层 → ReLU激活 → 线性层\n",
    "- 中间层的维度通常是输入维度的4倍\n",
    "\n",
    "$$\\text{FFN}(x) = \\text{Linear}_2(\\text{ReLU}(\\text{Linear}_1(x)))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b7f4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    前馈神经网络\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        Args:\n",
    "            x: [batch_size, seq_len, d_model]\n",
    "        \n",
    "        Returns:\n",
    "            output: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # x -> Linear -> ReLU -> Dropout -> Linear\n",
    "        output = self.linear1(x)  # [batch_size, seq_len, d_ff]\n",
    "        output = F.relu(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.linear2(output)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        return output\n",
    "\n",
    "# 测试前馈网络\n",
    "d_model = 512\n",
    "d_ff = 2048  # 通常是d_model的4倍\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "ffn = FeedForwardNetwork(d_model, d_ff)\n",
    "test_input = torch.randn(batch_size, seq_len, d_model)\n",
    "ffn_output = ffn(test_input)\n",
    "\n",
    "print(f\"前馈网络输入形状: {test_input.shape}\")\n",
    "print(f\"前馈网络输出形状: {ffn_output.shape}\")\n",
    "print(f\"前馈网络参数量: {sum(p.numel() for p in ffn.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8b4a3",
   "metadata": {},
   "source": [
    "## 5.3 层归一化 (Layer Normalization)\n",
    "\n",
    "层归一化是Transformer中的关键组件，它有助于稳定训练：\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sigma} \\cdot \\gamma + \\beta$$\n",
    "\n",
    "其中 $\\mu$ 和 $\\sigma$ 是在最后一个维度上计算的均值和标准差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f7c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从前面教程导入多头注意力\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, \n",
    "                mask: Optional[torch.Tensor] = None):\n",
    "        batch_size, seq_len, _ = query.size()\n",
    "        \n",
    "        Q = self.W_q(query).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(key).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(value).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    完整的Transformer块实现\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # 多头自注意力\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # 前馈网络\n",
    "        self.feed_forward = FeedForwardNetwork(d_model, d_ff, dropout)\n",
    "        \n",
    "        # 层归一化\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        Args:\n",
    "            x: [batch_size, seq_len, d_model]\n",
    "            mask: [batch_size, seq_len, seq_len] 或 None\n",
    "        \n",
    "        Returns:\n",
    "            output: [batch_size, seq_len, d_model]\n",
    "            attention_weights: [batch_size, num_heads, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        # 第一个子层：多头自注意力 + 残差连接 + 层归一化\n",
    "        attn_output, attention_weights = self.self_attention(x, x, x, mask)\n",
    "        x1 = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # 第二个子层：前馈网络 + 残差连接 + 层归一化\n",
    "        ff_output = self.feed_forward(x1)\n",
    "        x2 = self.norm2(x1 + self.dropout(ff_output))\n",
    "        \n",
    "        return x2, attention_weights\n",
    "\n",
    "# 测试Transformer块\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "d_ff = 1024\n",
    "seq_len = 12\n",
    "batch_size = 2\n",
    "\n",
    "transformer_block = TransformerBlock(d_model, num_heads, d_ff)\n",
    "test_input = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, attention_weights = transformer_block(test_input)\n",
    "\n",
    "print(f\"Transformer块输入形状: {test_input.shape}\")\n",
    "print(f\"Transformer块输出形状: {output.shape}\")\n",
    "print(f\"注意力权重形状: {attention_weights.shape}\")\n",
    "print(f\"Transformer块参数量: {sum(p.numel() for p in transformer_block.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b8b2f5",
   "metadata": {},
   "source": [
    "## 5.4 可视化Transformer块的信息流\n",
    "\n",
    "让我们可视化数据在Transformer块中的流动过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f8b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_transformer_block_flow(transformer_block, input_tensor):\n",
    "    \"\"\"\n",
    "    可视化Transformer块中的信息流动\n",
    "    \"\"\"\n",
    "    transformer_block.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 获取各个中间步骤的输出\n",
    "        x = input_tensor\n",
    "        \n",
    "        # 第一个子层：自注意力\n",
    "        attn_output, attention_weights = transformer_block.self_attention(x, x, x)\n",
    "        x_after_attn = x + transformer_block.dropout(attn_output)\n",
    "        x1 = transformer_block.norm1(x_after_attn)\n",
    "        \n",
    "        # 第二个子层：前馈网络\n",
    "        ff_output = transformer_block.feed_forward(x1)\n",
    "        x_after_ff = x1 + transformer_block.dropout(ff_output)\n",
    "        x2 = transformer_block.norm2(x_after_ff)\n",
    "        \n",
    "        # 计算各阶段的统计信息\n",
    "        stages = {\n",
    "            '输入': x,\n",
    "            '注意力输出': attn_output,\n",
    "            '注意力+残差': x_after_attn,\n",
    "            '第一层归一化': x1,\n",
    "            '前馈网络输出': ff_output,\n",
    "            '前馈+残差': x_after_ff,\n",
    "            '最终输出': x2\n",
    "        }\n",
    "        \n",
    "        # 计算统计信息\n",
    "        stats = {}\n",
    "        for name, tensor in stages.items():\n",
    "            stats[name] = {\n",
    "                'mean': tensor.mean().item(),\n",
    "                'std': tensor.std().item(),\n",
    "                'min': tensor.min().item(),\n",
    "                'max': tensor.max().item()\n",
    "            }\n",
    "        \n",
    "        # 可视化\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # 1. 均值变化\n",
    "        means = [stats[name]['mean'] for name in stages.keys()]\n",
    "        axes[0, 0].plot(means, 'o-', linewidth=2, markersize=6)\n",
    "        axes[0, 0].set_xticks(range(len(stages)))\n",
    "        axes[0, 0].set_xticklabels(list(stages.keys()), rotation=45)\n",
    "        axes[0, 0].set_title('各阶段输出的均值变化')\n",
    "        axes[0, 0].set_ylabel('均值')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. 标准差变化\n",
    "        stds = [stats[name]['std'] for name in stages.keys()]\n",
    "        axes[0, 1].plot(stds, 's-', linewidth=2, markersize=6, color='orange')\n",
    "        axes[0, 1].set_xticks(range(len(stages)))\n",
    "        axes[0, 1].set_xticklabels(list(stages.keys()), rotation=45)\n",
    "        axes[0, 1].set_title('各阶段输出的标准差变化')\n",
    "        axes[0, 1].set_ylabel('标准差')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. 注意力权重热力图（第一个头）\n",
    "        attn_weights_first_head = attention_weights[0, 0].numpy()\n",
    "        im = axes[1, 0].imshow(attn_weights_first_head, cmap='Blues')\n",
    "        axes[1, 0].set_title('注意力权重（第1个头）')\n",
    "        axes[1, 0].set_xlabel('Key位置')\n",
    "        axes[1, 0].set_ylabel('Query位置')\n",
    "        plt.colorbar(im, ax=axes[1, 0])\n",
    "        \n",
    "        # 4. 输入vs输出对比\n",
    "        input_sample = x[0, 0, :20].numpy()  # 取第一个样本的第一个位置的前20个维度\n",
    "        output_sample = x2[0, 0, :20].numpy()\n",
    "        \n",
    "        x_dims = range(20)\n",
    "        axes[1, 1].plot(x_dims, input_sample, 'b-', label='输入', alpha=0.7)\n",
    "        axes[1, 1].plot(x_dims, output_sample, 'r-', label='输出', alpha=0.7)\n",
    "        axes[1, 1].set_title('输入vs输出对比（前20维）')\n",
    "        axes[1, 1].set_xlabel('维度')\n",
    "        axes[1, 1].set_ylabel('数值')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 打印统计信息\n",
    "        print(\"各阶段统计信息:\")\n",
    "        print(\"-\" * 60)\n",
    "        for name, stat in stats.items():\n",
    "            print(f\"{name:15s}: 均值={stat['mean']:8.4f}, 标准差={stat['std']:6.4f}, \"\n",
    "                  f\"范围=[{stat['min']:6.3f}, {stat['max']:6.3f}]\")\n",
    "        \n",
    "        return stats, attention_weights\n",
    "\n",
    "# 运行可视化\n",
    "stats, attn_weights = visualize_transformer_block_flow(transformer_block, test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f7c8f6",
   "metadata": {},
   "source": [
    "## 5.5 Pre-LN vs Post-LN 对比\n",
    "\n",
    "现代Transformer有两种主要的层归一化位置：\n",
    "- **Post-LN**：原始Transformer论文的设计\n",
    "- **Pre-LN**：GPT-2及后续模型常用的设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b3b5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreLNTransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Pre-LN Transformer块（层归一化在残差连接之前）\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super(PreLNTransformerBlock, self).__init__()\n",
    "        \n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForwardNetwork(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        # Pre-LN: 层归一化在残差连接之前\n",
    "        # 第一个子层\n",
    "        norm_x = self.norm1(x)\n",
    "        attn_output, attention_weights = self.self_attention(norm_x, norm_x, norm_x, mask)\n",
    "        x1 = x + self.dropout(attn_output)\n",
    "        \n",
    "        # 第二个子层\n",
    "        norm_x1 = self.norm2(x1)\n",
    "        ff_output = self.feed_forward(norm_x1)\n",
    "        x2 = x1 + self.dropout(ff_output)\n",
    "        \n",
    "        return x2, attention_weights\n",
    "\n",
    "def compare_ln_positions():\n",
    "    \"\"\"\n",
    "    比较Pre-LN和Post-LN的效果\n",
    "    \"\"\"\n",
    "    d_model = 128\n",
    "    num_heads = 4\n",
    "    d_ff = 512\n",
    "    seq_len = 8\n",
    "    batch_size = 1\n",
    "    \n",
    "    # 创建相同参数的模型\n",
    "    post_ln_block = TransformerBlock(d_model, num_heads, d_ff)\n",
    "    pre_ln_block = PreLNTransformerBlock(d_model, num_heads, d_ff)\n",
    "    \n",
    "    # 创建测试输入\n",
    "    test_input = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # 设置为评估模式\n",
    "    post_ln_block.eval()\n",
    "    pre_ln_block.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 前向传播\n",
    "        post_ln_output, post_ln_attn = post_ln_block(test_input)\n",
    "        pre_ln_output, pre_ln_attn = pre_ln_block(test_input)\n",
    "        \n",
    "        # 计算统计信息\n",
    "        post_ln_stats = {\n",
    "            'mean': post_ln_output.mean().item(),\n",
    "            'std': post_ln_output.std().item(),\n",
    "            'max': post_ln_output.max().item(),\n",
    "            'min': post_ln_output.min().item()\n",
    "        }\n",
    "        \n",
    "        pre_ln_stats = {\n",
    "            'mean': pre_ln_output.mean().item(),\n",
    "            'std': pre_ln_output.std().item(),\n",
    "            'max': pre_ln_output.max().item(),\n",
    "            'min': pre_ln_output.min().item()\n",
    "        }\n",
    "        \n",
    "        # 可视化对比\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # 输出分布对比\n",
    "        axes[0, 0].hist(post_ln_output.flatten().numpy(), bins=30, alpha=0.7, \n",
    "                       label='Post-LN', color='blue')\n",
    "        axes[0, 0].hist(pre_ln_output.flatten().numpy(), bins=30, alpha=0.7, \n",
    "                       label='Pre-LN', color='red')\n",
    "        axes[0, 0].set_title('输出值分布对比')\n",
    "        axes[0, 0].set_xlabel('数值')\n",
    "        axes[0, 0].set_ylabel('频次')\n",
    "        axes[0, 0].legend()\n",
    "        \n",
    "        # 注意力权重对比（第一个头）\n",
    "        im1 = axes[0, 1].imshow(post_ln_attn[0, 0].numpy(), cmap='Blues')\n",
    "        axes[0, 1].set_title('Post-LN 注意力权重')\n",
    "        plt.colorbar(im1, ax=axes[0, 1])\n",
    "        \n",
    "        im2 = axes[1, 0].imshow(pre_ln_attn[0, 0].numpy(), cmap='Reds')\n",
    "        axes[1, 0].set_title('Pre-LN 注意力权重')\n",
    "        plt.colorbar(im2, ax=axes[1, 0])\n",
    "        \n",
    "        # 统计信息对比\n",
    "        metrics = ['mean', 'std', 'max', 'min']\n",
    "        post_values = [post_ln_stats[m] for m in metrics]\n",
    "        pre_values = [pre_ln_stats[m] for m in metrics]\n",
    "        \n",
    "        x_pos = np.arange(len(metrics))\n",
    "        width = 0.35\n",
    "        \n",
    "        axes[1, 1].bar(x_pos - width/2, post_values, width, label='Post-LN', alpha=0.8)\n",
    "        axes[1, 1].bar(x_pos + width/2, pre_values, width, label='Pre-LN', alpha=0.8)\n",
    "        axes[1, 1].set_xticks(x_pos)\n",
    "        axes[1, 1].set_xticklabels(metrics)\n",
    "        axes[1, 1].set_title('统计指标对比')\n",
    "        axes[1, 1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Post-LN vs Pre-LN 对比:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"{'指标':<10} {'Post-LN':<12} {'Pre-LN':<12} {'差异':<10}\")\n",
    "        print(\"-\" * 40)\n",
    "        for metric in metrics:\n",
    "            post_val = post_ln_stats[metric]\n",
    "            pre_val = pre_ln_stats[metric]\n",
    "            diff = abs(post_val - pre_val)\n",
    "            print(f\"{metric:<10} {post_val:<12.4f} {pre_val:<12.4f} {diff:<10.4f}\")\n",
    "\n",
    "# 运行对比\n",
    "compare_ln_positions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f7c8f8",
   "metadata": {},
   "source": [
    "## 5.6 完整的编码器实现\n",
    "\n",
    "现在让我们将多个Transformer块堆叠起来，创建完整的编码器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_seq_len: int = 5000):\n",
    "        super(SinusoidalPositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                            (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    完整的Transformer编码器\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, d_model: int, num_heads: int, \n",
    "                 d_ff: int, num_layers: int, max_seq_len: int = 5000, \n",
    "                 dropout: float = 0.1, use_pre_ln: bool = False):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # 词嵌入\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # 位置编码\n",
    "        self.pos_encoding = SinusoidalPositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "        # Transformer块\n",
    "        if use_pre_ln:\n",
    "            self.layers = nn.ModuleList([\n",
    "                PreLNTransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ])\n",
    "        else:\n",
    "            self.layers = nn.ModuleList([\n",
    "                TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ])\n",
    "        \n",
    "        # 输入dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 最终层归一化（Pre-LN架构需要）\n",
    "        self.final_norm = nn.LayerNorm(d_model) if use_pre_ln else None\n",
    "        \n",
    "    def forward(self, input_ids: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        Args:\n",
    "            input_ids: [batch_size, seq_len]\n",
    "            mask: [batch_size, seq_len, seq_len] 或 None\n",
    "        \n",
    "        Returns:\n",
    "            output: [batch_size, seq_len, d_model]\n",
    "            attention_weights_list: 各层的注意力权重列表\n",
    "        \"\"\"\n",
    "        # 词嵌入和位置编码\n",
    "        x = self.embedding(input_ids) * math.sqrt(self.d_model)  # 缩放嵌入\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 通过各层\n",
    "        attention_weights_list = []\n",
    "        for layer in self.layers:\n",
    "            x, attention_weights = layer(x, mask)\n",
    "            attention_weights_list.append(attention_weights)\n",
    "        \n",
    "        # 最终层归一化（Pre-LN）\n",
    "        if self.final_norm is not None:\n",
    "            x = self.final_norm(x)\n",
    "        \n",
    "        return x, attention_weights_list\n",
    "\n",
    "# 创建完整的编码器\n",
    "vocab_size = 1000\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "d_ff = 1024\n",
    "num_layers = 6\n",
    "seq_len = 16\n",
    "batch_size = 2\n",
    "\n",
    "encoder = TransformerEncoder(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ff=d_ff,\n",
    "    num_layers=num_layers,\n",
    "    use_pre_ln=True  # 使用Pre-LN架构\n",
    ")\n",
    "\n",
    "# 创建测试输入\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "# 前向传播\n",
    "output, attention_weights_all = encoder(input_ids)\n",
    "\n",
    "print(f\"编码器输入形状: {input_ids.shape}\")\n",
    "print(f\"编码器输出形状: {output.shape}\")\n",
    "print(f\"注意力权重层数: {len(attention_weights_all)}\")\n",
    "print(f\"每层注意力权重形状: {attention_weights_all[0].shape}\")\n",
    "print(f\"编码器总参数量: {sum(p.numel() for p in encoder.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f7c8f9",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "在这个教程中，我们学习了如何构建完整的Transformer块：\n",
    "\n",
    "### 核心组件：\n",
    "1. **多头自注意力**：捕获序列内的依赖关系\n",
    "2. **前馈网络**：提供非线性变换和特征处理\n",
    "3. **残差连接**：帮助梯度流动，缓解梯度消失\n",
    "4. **层归一化**：稳定训练，加速收敛\n",
    "\n",
    "### 架构变体：\n",
    "- **Post-LN**：原始设计，层归一化在残差连接之后\n",
    "- **Pre-LN**：现代设计，层归一化在残差连接之前，训练更稳定\n",
    "\n",
    "### 设计原则：\n",
    "- **残差连接**：确保信息能够直接流过深层网络\n",
    "- **层归一化**：保持激活值的合理范围\n",
    "- **Dropout**：防止过拟合\n",
    "- **维度一致性**：所有子层的输出维度都是d_model\n",
    "\n",
    "### 实际应用：\n",
    "- Transformer块是BERT、GPT等模型的基础构建单元\n",
    "- 通过堆叠多个块可以创建深层的编码器或解码器\n",
    "- 不同任务可能需要不同的层数和维度配置\n",
    "\n",
    "在下一个教程中，我们将学习如何构建**完整的Transformer模型**，包括编码器-解码器架构。\n",
    "\n",
    "### 下一步学习：\n",
    "- [06-complete-transformer.ipynb](06-complete-transformer.ipynb) - 完整Transformer实现"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}