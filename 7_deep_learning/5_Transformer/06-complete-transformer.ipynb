{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8b4c8d0",
   "metadata": {},
   "source": [
    "# 6. å®Œæ•´Transformerå®ç°\n",
    "\n",
    "åœ¨å‰é¢çš„æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†Transformerçš„å„ä¸ªç»„ä»¶ã€‚ç°åœ¨æ˜¯æ—¶å€™å°†å®ƒä»¬ç»„åˆæˆå®Œæ•´çš„Transformeræ¨¡å‹ï¼Œå¹¶å®ç°ä¸€ä¸ªç«¯åˆ°ç«¯çš„æœºå™¨ç¿»è¯‘ä»»åŠ¡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f8a2c0",
   "metadata": {},
   "source": [
    "## 6.1 å®Œæ•´Transformeræ¶æ„\n",
    "\n",
    "åŸå§‹çš„TransformeråŒ…å«ç¼–ç å™¨ï¼ˆEncoderï¼‰å’Œè§£ç å™¨ï¼ˆDecoderï¼‰ä¸¤éƒ¨åˆ†ï¼š\n",
    "\n",
    "### ç¼–ç å™¨ï¼ˆEncoderï¼‰\n",
    "- è¾“å…¥ï¼šæºåºåˆ— + ä½ç½®ç¼–ç \n",
    "- ç»“æ„ï¼šNå±‚ç¼–ç å™¨å—çš„å †å \n",
    "- è¾“å‡ºï¼šç¼–ç åçš„è¡¨ç¤º\n",
    "\n",
    "### è§£ç å™¨ï¼ˆDecoderï¼‰\n",
    "- è¾“å…¥ï¼šç›®æ ‡åºåˆ— + ä½ç½®ç¼–ç  + ç¼–ç å™¨è¾“å‡º\n",
    "- ç»“æ„ï¼šNå±‚è§£ç å™¨å—çš„å †å \n",
    "- è¾“å‡ºï¼šç”Ÿæˆçš„åºåˆ—æ¦‚ç‡åˆ†å¸ƒ\n",
    "\n",
    "![å®Œæ•´Transformeræ¶æ„](images/complete_transformer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e7c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import copy\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "# ä»utilså¯¼å…¥åŸºç¡€ç»„ä»¶\n",
    "from utils import (\n",
    "    MultiHeadAttention, \n",
    "    SinusoidalPositionalEncoding, \n",
    "    FeedForwardNetwork,\n",
    "    create_padding_mask,\n",
    "    create_causal_mask,\n",
    "    setup_matplotlib_chinese,\n",
    "    set_random_seed\n",
    ")\n",
    "\n",
    "# è®¾ç½®ç¯å¢ƒ\n",
    "setup_matplotlib_chinese()\n",
    "set_random_seed(42)\n",
    "\n",
    "print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ä½¿ç”¨è®¾å¤‡: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e7c4f5",
   "metadata": {},
   "source": [
    "## 6.2 è§£ç å™¨å—ï¼ˆDecoder Blockï¼‰\n",
    "\n",
    "è§£ç å™¨å—ä¸ç¼–ç å™¨å—ç±»ä¼¼ï¼Œä½†åŒ…å«ä¸‰ä¸ªå­å±‚ï¼š\n",
    "1. **æ©ç å¤šå¤´è‡ªæ³¨æ„åŠ›**ï¼šé˜²æ­¢çœ‹åˆ°æœªæ¥çš„token\n",
    "2. **ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›**ï¼šå…³æ³¨ç¼–ç å™¨çš„è¾“å‡º\n",
    "3. **å‰é¦ˆç½‘ç»œ**ï¼šéçº¿æ€§å˜æ¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b7f4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformerè§£ç å™¨å—\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super(TransformerDecoderBlock, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # ä¸‰ä¸ªæ³¨æ„åŠ›å±‚\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)  # æ©ç è‡ªæ³¨æ„åŠ›\n",
    "        self.cross_attention = MultiHeadAttention(d_model, num_heads, dropout)  # ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›\n",
    "        \n",
    "        # å‰é¦ˆç½‘ç»œ\n",
    "        self.feed_forward = FeedForwardNetwork(d_model, d_ff, dropout)\n",
    "        \n",
    "        # å±‚å½’ä¸€åŒ–\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor,\n",
    "                src_mask: Optional[torch.Tensor] = None,\n",
    "                tgt_mask: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­\n",
    "        \n",
    "        Args:\n",
    "            x: è§£ç å™¨è¾“å…¥ [batch_size, tgt_len, d_model]\n",
    "            encoder_output: ç¼–ç å™¨è¾“å‡º [batch_size, src_len, d_model]\n",
    "            src_mask: æºåºåˆ—æ©ç  [batch_size, src_len, src_len]\n",
    "            tgt_mask: ç›®æ ‡åºåˆ—æ©ç  [batch_size, tgt_len, tgt_len]\n",
    "        \n",
    "        Returns:\n",
    "            output: [batch_size, tgt_len, d_model]\n",
    "            self_attn_weights: è‡ªæ³¨æ„åŠ›æƒé‡\n",
    "            cross_attn_weights: äº¤å‰æ³¨æ„åŠ›æƒé‡\n",
    "        \"\"\"\n",
    "        # ç¬¬ä¸€ä¸ªå­å±‚ï¼šæ©ç å¤šå¤´è‡ªæ³¨æ„åŠ›\n",
    "        self_attn_output, self_attn_weights = self.self_attention(x, x, x, tgt_mask)\n",
    "        x1 = self.norm1(x + self.dropout(self_attn_output))\n",
    "        \n",
    "        # ç¬¬äºŒä¸ªå­å±‚ï¼šç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›\n",
    "        cross_attn_output, cross_attn_weights = self.cross_attention(\n",
    "            x1, encoder_output, encoder_output, src_mask\n",
    "        )\n",
    "        x2 = self.norm2(x1 + self.dropout(cross_attn_output))\n",
    "        \n",
    "        # ç¬¬ä¸‰ä¸ªå­å±‚ï¼šå‰é¦ˆç½‘ç»œ\n",
    "        ff_output = self.feed_forward(x2)\n",
    "        x3 = self.norm3(x2 + self.dropout(ff_output))\n",
    "        \n",
    "        return x3, self_attn_weights, cross_attn_weights\n",
    "\n",
    "# æµ‹è¯•è§£ç å™¨å—\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "d_ff = 1024\n",
    "tgt_len = 10\n",
    "src_len = 12\n",
    "batch_size = 2\n",
    "\n",
    "decoder_block = TransformerDecoderBlock(d_model, num_heads, d_ff)\n",
    "\n",
    "# åˆ›å»ºæµ‹è¯•è¾“å…¥\n",
    "decoder_input = torch.randn(batch_size, tgt_len, d_model)\n",
    "encoder_output = torch.randn(batch_size, src_len, d_model)\n",
    "\n",
    "# åˆ›å»ºæ©ç \n",
    "tgt_mask = create_causal_mask(tgt_len).unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "# å‰å‘ä¼ æ’­\n",
    "output, self_attn, cross_attn = decoder_block(\n",
    "    decoder_input, encoder_output, tgt_mask=tgt_mask\n",
    ")\n",
    "\n",
    "print(f\"è§£ç å™¨å—è¾“å…¥å½¢çŠ¶: {decoder_input.shape}\")\n",
    "print(f\"ç¼–ç å™¨è¾“å‡ºå½¢çŠ¶: {encoder_output.shape}\")\n",
    "print(f\"è§£ç å™¨å—è¾“å‡ºå½¢çŠ¶: {output.shape}\")\n",
    "print(f\"è‡ªæ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {self_attn.shape}\")\n",
    "print(f\"äº¤å‰æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {cross_attn.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8b4a3",
   "metadata": {},
   "source": [
    "## 6.3 å®Œæ•´Transformeræ¨¡å‹\n",
    "\n",
    "ç°åœ¨æˆ‘ä»¬å°†ç¼–ç å™¨å’Œè§£ç å™¨ç»„åˆæˆå®Œæ•´çš„Transformeræ¨¡å‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f7c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformerç¼–ç å™¨\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, d_model: int, num_heads: int, \n",
    "                 d_ff: int, num_layers: int, max_seq_len: int = 5000, \n",
    "                 dropout: float = 0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # è¯åµŒå…¥å’Œä½ç½®ç¼–ç \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = SinusoidalPositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "        # ç¼–ç å™¨å±‚\n",
    "        from utils import TransformerBlock\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src: torch.Tensor, src_mask: Optional[torch.Tensor] = None):\n",
    "        # åµŒå…¥å’Œä½ç½®ç¼–ç \n",
    "        x = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # é€šè¿‡ç¼–ç å™¨å±‚\n",
    "        attention_weights = []\n",
    "        for layer in self.layers:\n",
    "            x, attn_weights = layer(x, src_mask)\n",
    "            attention_weights.append(attn_weights)\n",
    "        \n",
    "        return x, attention_weights\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformerè§£ç å™¨\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, d_model: int, num_heads: int, \n",
    "                 d_ff: int, num_layers: int, max_seq_len: int = 5000, \n",
    "                 dropout: float = 0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # è¯åµŒå…¥å’Œä½ç½®ç¼–ç \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = SinusoidalPositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "        # è§£ç å™¨å±‚\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, tgt: torch.Tensor, encoder_output: torch.Tensor,\n",
    "                src_mask: Optional[torch.Tensor] = None,\n",
    "                tgt_mask: Optional[torch.Tensor] = None):\n",
    "        # åµŒå…¥å’Œä½ç½®ç¼–ç \n",
    "        x = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # é€šè¿‡è§£ç å™¨å±‚\n",
    "        self_attention_weights = []\n",
    "        cross_attention_weights = []\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x, self_attn, cross_attn = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "            self_attention_weights.append(self_attn)\n",
    "            cross_attention_weights.append(cross_attn)\n",
    "        \n",
    "        return x, self_attention_weights, cross_attention_weights\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    å®Œæ•´çš„Transformeræ¨¡å‹\n",
    "    \"\"\"\n",
    "    def __init__(self, src_vocab_size: int, tgt_vocab_size: int,\n",
    "                 d_model: int = 512, num_heads: int = 8, \n",
    "                 d_ff: int = 2048, num_layers: int = 6,\n",
    "                 max_seq_len: int = 5000, dropout: float = 0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # ç¼–ç å™¨å’Œè§£ç å™¨\n",
    "        self.encoder = TransformerEncoder(\n",
    "            src_vocab_size, d_model, num_heads, d_ff, num_layers, max_seq_len, dropout\n",
    "        )\n",
    "        self.decoder = TransformerDecoder(\n",
    "            tgt_vocab_size, d_model, num_heads, d_ff, num_layers, max_seq_len, dropout\n",
    "        )\n",
    "        \n",
    "        # è¾“å‡ºæŠ•å½±å±‚\n",
    "        self.output_projection = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "        # æƒé‡åˆå§‹åŒ–\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"åˆå§‹åŒ–æƒé‡\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, mean=0, std=0.1)\n",
    "    \n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor,\n",
    "                src_mask: Optional[torch.Tensor] = None,\n",
    "                tgt_mask: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­\n",
    "        \n",
    "        Args:\n",
    "            src: æºåºåˆ— [batch_size, src_len]\n",
    "            tgt: ç›®æ ‡åºåˆ— [batch_size, tgt_len]\n",
    "            src_mask: æºåºåˆ—æ©ç \n",
    "            tgt_mask: ç›®æ ‡åºåˆ—æ©ç \n",
    "        \n",
    "        Returns:\n",
    "            output: [batch_size, tgt_len, tgt_vocab_size]\n",
    "            attention_weights: æ³¨æ„åŠ›æƒé‡å­—å…¸\n",
    "        \"\"\"\n",
    "        # ç¼–ç å™¨\n",
    "        encoder_output, encoder_attention = self.encoder(src, src_mask)\n",
    "        \n",
    "        # è§£ç å™¨\n",
    "        decoder_output, decoder_self_attention, decoder_cross_attention = self.decoder(\n",
    "            tgt, encoder_output, src_mask, tgt_mask\n",
    "        )\n",
    "        \n",
    "        # è¾“å‡ºæŠ•å½±\n",
    "        output = self.output_projection(decoder_output)\n",
    "        \n",
    "        attention_weights = {\n",
    "            'encoder_attention': encoder_attention,\n",
    "            'decoder_self_attention': decoder_self_attention,\n",
    "            'decoder_cross_attention': decoder_cross_attention\n",
    "        }\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# åˆ›å»ºå®Œæ•´çš„Transformeræ¨¡å‹\n",
    "src_vocab_size = 1000\n",
    "tgt_vocab_size = 1000\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "d_ff = 1024\n",
    "num_layers = 4\n",
    "\n",
    "transformer = Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ff=d_ff,\n",
    "    num_layers=num_layers\n",
    ")\n",
    "\n",
    "# è®¡ç®—å‚æ•°é‡\n",
    "total_params = sum(p.numel() for p in transformer.parameters())\n",
    "trainable_params = sum(p.numel() for p in transformer.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Transformeræ¨¡å‹å‚æ•°æ€»é‡: {total_params:,}\")\n",
    "print(f\"å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}\")\n",
    "print(f\"æ¨¡å‹å¤§å°: {total_params * 4 / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b8b2f5",
   "metadata": {},
   "source": [
    "## 6.4 æœºå™¨ç¿»è¯‘ä»»åŠ¡å®æˆ˜\n",
    "\n",
    "è®©æˆ‘ä»¬ç”¨å®Œæ•´çš„Transformeræ¨¡å‹æ¥å®ç°ä¸€ä¸ªç®€å•çš„æœºå™¨ç¿»è¯‘ä»»åŠ¡ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f8b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    \"\"\"\n",
    "    ç®€å•çš„åˆ†è¯å™¨ï¼Œç”¨äºæ¼”ç¤º\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.pad_token = '<PAD>'\n",
    "        self.sos_token = '<SOS>'\n",
    "        self.eos_token = '<EOS>'\n",
    "        self.unk_token = '<UNK>'\n",
    "        \n",
    "        self.special_tokens = [self.pad_token, self.sos_token, self.eos_token, self.unk_token]\n",
    "        self.vocab = {}\n",
    "        self.idx_to_token = {}\n",
    "        \n",
    "    def build_vocab(self, sentences: List[str]):\n",
    "        \"\"\"æ„å»ºè¯æ±‡è¡¨\"\"\"\n",
    "        # æ·»åŠ ç‰¹æ®Štoken\n",
    "        for i, token in enumerate(self.special_tokens):\n",
    "            self.vocab[token] = i\n",
    "            self.idx_to_token[i] = token\n",
    "        \n",
    "        # æ·»åŠ è¯æ±‡\n",
    "        word_count = {}\n",
    "        for sentence in sentences:\n",
    "            for word in sentence.lower().split():\n",
    "                word_count[word] = word_count.get(word, 0) + 1\n",
    "        \n",
    "        # æŒ‰é¢‘ç‡æ’åºå¹¶æ·»åŠ åˆ°è¯æ±‡è¡¨\n",
    "        sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for word, _ in sorted_words:\n",
    "            if word not in self.vocab:\n",
    "                idx = len(self.vocab)\n",
    "                self.vocab[word] = idx\n",
    "                self.idx_to_token[idx] = word\n",
    "    \n",
    "    def encode(self, sentence: str, max_len: int = None) -> List[int]:\n",
    "        \"\"\"ç¼–ç å¥å­\"\"\"\n",
    "        tokens = [self.vocab.get(word.lower(), self.vocab[self.unk_token]) \n",
    "                 for word in sentence.split()]\n",
    "        \n",
    "        # æ·»åŠ SOSå’ŒEOS\n",
    "        tokens = [self.vocab[self.sos_token]] + tokens + [self.vocab[self.eos_token]]\n",
    "        \n",
    "        # æˆªæ–­æˆ–å¡«å……\n",
    "        if max_len:\n",
    "            if len(tokens) > max_len:\n",
    "                tokens = tokens[:max_len]\n",
    "            else:\n",
    "                tokens += [self.vocab[self.pad_token]] * (max_len - len(tokens))\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def decode(self, tokens: List[int]) -> str:\n",
    "        \"\"\"è§£ç tokenåºåˆ—\"\"\"\n",
    "        words = []\n",
    "        for token in tokens:\n",
    "            word = self.idx_to_token.get(token, self.unk_token)\n",
    "            if word == self.eos_token:\n",
    "                break\n",
    "            if word not in self.special_tokens:\n",
    "                words.append(word)\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    @property\n",
    "    def pad_token_id(self):\n",
    "        return self.vocab[self.pad_token]\n",
    "    \n",
    "    @property\n",
    "    def sos_token_id(self):\n",
    "        return self.vocab[self.sos_token]\n",
    "    \n",
    "    @property\n",
    "    def eos_token_id(self):\n",
    "        return self.vocab[self.eos_token]\n",
    "\n",
    "# åˆ›å»ºç¤ºä¾‹æ•°æ®é›†ï¼ˆè‹±è¯­->ä¸­æ–‡çš„ç®€å•ä¾‹å­ï¼‰\n",
    "def create_toy_dataset():\n",
    "    \"\"\"åˆ›å»ºç©å…·æ•°æ®é›†\"\"\"\n",
    "    en_sentences = [\n",
    "        \"hello world\",\n",
    "        \"how are you\",\n",
    "        \"good morning\",\n",
    "        \"thank you\",\n",
    "        \"goodbye\",\n",
    "        \"i love you\",\n",
    "        \"what is your name\",\n",
    "        \"nice to meet you\",\n",
    "        \"have a good day\",\n",
    "        \"see you later\"\n",
    "    ]\n",
    "    \n",
    "    zh_sentences = [\n",
    "        \"ä½ å¥½ ä¸–ç•Œ\",\n",
    "        \"ä½  å¥½ å—\",\n",
    "        \"æ—©ä¸Š å¥½\",\n",
    "        \"è°¢è°¢ ä½ \",\n",
    "        \"å†è§\",\n",
    "        \"æˆ‘ çˆ± ä½ \",\n",
    "        \"ä½  çš„ åå­— æ˜¯ ä»€ä¹ˆ\",\n",
    "        \"å¾ˆ é«˜å…´ è§åˆ° ä½ \",\n",
    "        \"ç¥ ä½  æœ‰ ç¾å¥½ çš„ ä¸€å¤©\",\n",
    "        \"å›å¤´ è§\"\n",
    "    ]\n",
    "    \n",
    "    return en_sentences, zh_sentences\n",
    "\n",
    "# å‡†å¤‡æ•°æ®\n",
    "en_sentences, zh_sentences = create_toy_dataset()\n",
    "\n",
    "# åˆ›å»ºåˆ†è¯å™¨\n",
    "en_tokenizer = SimpleTokenizer()\n",
    "zh_tokenizer = SimpleTokenizer()\n",
    "\n",
    "en_tokenizer.build_vocab(en_sentences)\n",
    "zh_tokenizer.build_vocab(zh_sentences)\n",
    "\n",
    "print(f\"è‹±è¯­è¯æ±‡è¡¨å¤§å°: {en_tokenizer.vocab_size}\")\n",
    "print(f\"ä¸­æ–‡è¯æ±‡è¡¨å¤§å°: {zh_tokenizer.vocab_size}\")\n",
    "\n",
    "# ç¼–ç ç¤ºä¾‹\n",
    "example_en = \"hello world\"\n",
    "example_zh = \"ä½ å¥½ ä¸–ç•Œ\"\n",
    "\n",
    "en_encoded = en_tokenizer.encode(example_en, max_len=10)\n",
    "zh_encoded = zh_tokenizer.encode(example_zh, max_len=10)\n",
    "\n",
    "print(f\"\\nç¼–ç ç¤ºä¾‹:\")\n",
    "print(f\"è‹±è¯­: '{example_en}' -> {en_encoded}\")\n",
    "print(f\"ä¸­æ–‡: '{example_zh}' -> {zh_encoded}\")\n",
    "print(f\"è§£ç : {en_tokenizer.decode(en_encoded)}\")\n",
    "print(f\"è§£ç : {zh_tokenizer.decode(zh_encoded)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f7c8f6",
   "metadata": {},
   "source": [
    "## 6.5 è®­ç»ƒè¿‡ç¨‹å®ç°\n",
    "\n",
    "ç°åœ¨è®©æˆ‘ä»¬å®ç°å®Œæ•´çš„è®­ç»ƒè¿‡ç¨‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b3b5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    ç¿»è¯‘æ•°æ®é›†\n",
    "    \"\"\"\n",
    "    def __init__(self, src_sentences, tgt_sentences, src_tokenizer, tgt_tokenizer, max_len=20):\n",
    "        self.src_sentences = src_sentences\n",
    "        self.tgt_sentences = tgt_sentences\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src_encoded = self.src_tokenizer.encode(self.src_sentences[idx], self.max_len)\n",
    "        tgt_encoded = self.tgt_tokenizer.encode(self.tgt_sentences[idx], self.max_len)\n",
    "        \n",
    "        return {\n",
    "            'src': torch.tensor(src_encoded, dtype=torch.long),\n",
    "            'tgt': torch.tensor(tgt_encoded, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def create_masks(src, tgt, src_pad_idx, tgt_pad_idx):\n",
    "    \"\"\"\n",
    "    åˆ›å»ºè®­ç»ƒæ‰€éœ€çš„æ©ç \n",
    "    \"\"\"\n",
    "    # æºåºåˆ—å¡«å……æ©ç \n",
    "    src_mask = (src != src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    \n",
    "    # ç›®æ ‡åºåˆ—å¡«å……æ©ç å’Œå› æœæ©ç \n",
    "    tgt_len = tgt.size(1)\n",
    "    tgt_pad_mask = (tgt != tgt_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    tgt_causal_mask = torch.tril(torch.ones(tgt_len, tgt_len, device=tgt.device))\n",
    "    tgt_mask = tgt_pad_mask & tgt_causal_mask\n",
    "    \n",
    "    return src_mask, tgt_mask\n",
    "\n",
    "def train_step(model, batch, criterion, optimizer, src_tokenizer, tgt_tokenizer):\n",
    "    \"\"\"\n",
    "    å•æ­¥è®­ç»ƒ\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    src = batch['src']\n",
    "    tgt = batch['tgt']\n",
    "    \n",
    "    # ç›®æ ‡åºåˆ—åˆ†ä¸ºè¾“å…¥å’Œæ ‡ç­¾\n",
    "    tgt_input = tgt[:, :-1]  # å»æ‰æœ€åä¸€ä¸ªtokenä½œä¸ºè¾“å…¥\n",
    "    tgt_label = tgt[:, 1:]   # å»æ‰ç¬¬ä¸€ä¸ªtokenä½œä¸ºæ ‡ç­¾\n",
    "    \n",
    "    # åˆ›å»ºæ©ç \n",
    "    src_mask, tgt_mask = create_masks(\n",
    "        src, tgt_input, src_tokenizer.pad_token_id, tgt_tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    # å‰å‘ä¼ æ’­\n",
    "    output, _ = model(src, tgt_input, src_mask, tgt_mask)\n",
    "    \n",
    "    # è®¡ç®—æŸå¤±\n",
    "    loss = criterion(output.reshape(-1, output.size(-1)), tgt_label.reshape(-1))\n",
    "    \n",
    "    # åå‘ä¼ æ’­\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # æ¢¯åº¦è£å‰ª\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "# åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨\n",
    "dataset = TranslationDataset(en_sentences, zh_sentences, en_tokenizer, zh_tokenizer, max_len=15)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# åˆ›å»ºæ¨¡å‹\n",
    "model = Transformer(\n",
    "    src_vocab_size=en_tokenizer.vocab_size,\n",
    "    tgt_vocab_size=zh_tokenizer.vocab_size,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    d_ff=512,\n",
    "    num_layers=2,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=zh_tokenizer.pad_token_id)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# è®­ç»ƒå¾ªç¯\n",
    "num_epochs = 50\n",
    "losses = []\n",
    "\n",
    "print(\"å¼€å§‹è®­ç»ƒ...\")\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        loss = train_step(model, batch, criterion, optimizer, en_tokenizer, zh_tokenizer)\n",
    "        epoch_losses.append(loss)\n",
    "    \n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"è®­ç»ƒå®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f7c8f8",
   "metadata": {},
   "source": [
    "## 6.6 æ¨ç†å’Œç¿»è¯‘\n",
    "\n",
    "è®­ç»ƒå®Œæˆåï¼Œè®©æˆ‘ä»¬å®ç°ç¿»è¯‘åŠŸèƒ½ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, src_sentence, src_tokenizer, tgt_tokenizer, max_len=20):\n",
    "    \"\"\"\n",
    "    ç¿»è¯‘å‡½æ•°\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # ç¼–ç æºå¥å­\n",
    "        src_tokens = src_tokenizer.encode(src_sentence, max_len)\n",
    "        src = torch.tensor(src_tokens).unsqueeze(0)  # æ·»åŠ batchç»´åº¦\n",
    "        \n",
    "        # ç¼–ç å™¨å‰å‘ä¼ æ’­\n",
    "        src_mask = (src != src_tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "        encoder_output, _ = model.encoder(src, src_mask)\n",
    "        \n",
    "        # åˆå§‹åŒ–è§£ç åºåˆ—\n",
    "        tgt_tokens = [tgt_tokenizer.sos_token_id]\n",
    "        \n",
    "        # é€æ­¥ç”Ÿæˆ\n",
    "        for _ in range(max_len - 1):\n",
    "            tgt = torch.tensor(tgt_tokens).unsqueeze(0)\n",
    "            tgt_len = tgt.size(1)\n",
    "            \n",
    "            # åˆ›å»ºç›®æ ‡æ©ç \n",
    "            tgt_mask = torch.tril(torch.ones(tgt_len, tgt_len))\n",
    "            tgt_mask = tgt_mask.unsqueeze(0)\n",
    "            \n",
    "            # è§£ç å™¨å‰å‘ä¼ æ’­\n",
    "            decoder_output, _, _ = model.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "            \n",
    "            # è¾“å‡ºæŠ•å½±\n",
    "            output = model.output_projection(decoder_output)\n",
    "            \n",
    "            # è·å–ä¸‹ä¸€ä¸ªtoken\n",
    "            next_token = output[0, -1, :].argmax().item()\n",
    "            tgt_tokens.append(next_token)\n",
    "            \n",
    "            # å¦‚æœç”Ÿæˆäº†ç»“æŸtokenï¼Œåœæ­¢ç”Ÿæˆ\n",
    "            if next_token == tgt_tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    # è§£ç ç”Ÿæˆçš„åºåˆ—\n",
    "    translated = tgt_tokenizer.decode(tgt_tokens[1:])  # å»æ‰SOS token\n",
    "    return translated\n",
    "\n",
    "# æµ‹è¯•ç¿»è¯‘\n",
    "test_sentences = [\n",
    "    \"hello world\",\n",
    "    \"good morning\",\n",
    "    \"thank you\",\n",
    "    \"i love you\"\n",
    "]\n",
    "\n",
    "print(\"ç¿»è¯‘æµ‹è¯•ç»“æœ:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for en_sentence in test_sentences:\n",
    "    translated = translate(model, en_sentence, en_tokenizer, zh_tokenizer)\n",
    "    print(f\"è‹±è¯­: {en_sentence}\")\n",
    "    print(f\"ç¿»è¯‘: {translated}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f7c8f9",
   "metadata": {},
   "source": [
    "## 6.7 å¯è§†åŒ–åˆ†æ\n",
    "\n",
    "è®©æˆ‘ä»¬åˆ†æè®­ç»ƒè¿‡ç¨‹å’Œæ³¨æ„åŠ›æƒé‡ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f8b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses, 'b-', linewidth=2)\n",
    "plt.title('è®­ç»ƒæŸå¤±æ›²çº¿')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('æŸå¤±')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡\n",
    "def visualize_translation_attention(model, src_sentence, tgt_sentence, \n",
    "                                   src_tokenizer, tgt_tokenizer):\n",
    "    \"\"\"\n",
    "    å¯è§†åŒ–ç¿»è¯‘è¿‡ç¨‹ä¸­çš„æ³¨æ„åŠ›æƒé‡\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # ç¼–ç è¾“å…¥\n",
    "        src_tokens = src_tokenizer.encode(src_sentence, max_len=15)\n",
    "        tgt_tokens = tgt_tokenizer.encode(tgt_sentence, max_len=15)\n",
    "        \n",
    "        src = torch.tensor(src_tokens).unsqueeze(0)\n",
    "        tgt_input = torch.tensor(tgt_tokens[:-1]).unsqueeze(0)  # å»æ‰EOS\n",
    "        \n",
    "        # åˆ›å»ºæ©ç \n",
    "        src_mask, tgt_mask = create_masks(\n",
    "            src, tgt_input, src_tokenizer.pad_token_id, tgt_tokenizer.pad_token_id\n",
    "        )\n",
    "        \n",
    "        # å‰å‘ä¼ æ’­\n",
    "        output, attention_weights = model(src, tgt_input, src_mask, tgt_mask)\n",
    "        \n",
    "        # è·å–cross attentionæƒé‡ï¼ˆæœ€åä¸€å±‚ï¼‰\n",
    "        cross_attention = attention_weights['decoder_cross_attention'][-1][0, 0]  # ç¬¬ä¸€ä¸ªå¤´\n",
    "        \n",
    "        # è·å–æœ‰æ•ˆçš„token\n",
    "        src_words = [src_tokenizer.idx_to_token[idx] for idx in src_tokens \n",
    "                    if idx != src_tokenizer.pad_token_id]\n",
    "        tgt_words = [tgt_tokenizer.idx_to_token[idx] for idx in tgt_tokens[:-1] \n",
    "                    if idx != tgt_tokenizer.pad_token_id]\n",
    "        \n",
    "        # æˆªå–æœ‰æ•ˆçš„æ³¨æ„åŠ›æƒé‡\n",
    "        attention_matrix = cross_attention[:len(tgt_words), :len(src_words)].cpu().numpy()\n",
    "        \n",
    "        # å¯è§†åŒ–\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(attention_matrix, annot=True, fmt='.3f', cmap='Blues',\n",
    "                   xticklabels=src_words, yticklabels=tgt_words)\n",
    "        plt.title(f'äº¤å‰æ³¨æ„åŠ›æƒé‡\\næºå¥: {src_sentence}\\nç›®æ ‡å¥: {tgt_sentence}')\n",
    "        plt.xlabel('æºè¯­è¨€ (Key)')\n",
    "        plt.ylabel('ç›®æ ‡è¯­è¨€ (Query)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡ç¤ºä¾‹\n",
    "visualize_translation_attention(\n",
    "    model, \"hello world\", \"ä½ å¥½ ä¸–ç•Œ\", en_tokenizer, zh_tokenizer\n",
    ")\n",
    "\n",
    "visualize_translation_attention(\n",
    "    model, \"thank you\", \"è°¢è°¢ ä½ \", en_tokenizer, zh_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f7c8f9",
   "metadata": {},
   "source": [
    "## 6.8 æ¨¡å‹è¯„ä¼°å’Œæ”¹è¿›\n",
    "\n",
    "è®©æˆ‘ä»¬åˆ†ææ¨¡å‹æ€§èƒ½å¹¶è®¨è®ºæ”¹è¿›æ–¹å‘ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e7c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_data, src_tokenizer, tgt_tokenizer):\n",
    "    \"\"\"\n",
    "    è¯„ä¼°æ¨¡å‹æ€§èƒ½\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    correct_translations = 0\n",
    "    total_translations = len(test_data)\n",
    "    \n",
    "    print(\"æ¨¡å‹è¯„ä¼°ç»“æœ:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for en_sentence, expected_zh in test_data:\n",
    "        predicted_zh = translate(model, en_sentence, src_tokenizer, tgt_tokenizer)\n",
    "        \n",
    "        # ç®€å•çš„å®Œå…¨åŒ¹é…è¯„ä¼°\n",
    "        is_correct = predicted_zh.strip() == expected_zh.strip()\n",
    "        if is_correct:\n",
    "            correct_translations += 1\n",
    "        \n",
    "        print(f\"æºå¥: {en_sentence}\")\n",
    "        print(f\"æœŸæœ›: {expected_zh}\")\n",
    "        print(f\"é¢„æµ‹: {predicted_zh}\")\n",
    "        print(f\"æ­£ç¡®: {'âœ“' if is_correct else 'âœ—'}\")\n",
    "        print(\"-\" * 30)\n",
    "    \n",
    "    accuracy = correct_translations / total_translations\n",
    "    print(f\"\\næ•´ä½“å‡†ç¡®ç‡: {accuracy:.2%} ({correct_translations}/{total_translations})\")\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# è¯„ä¼°æ¨¡å‹\n",
    "test_data = list(zip(en_sentences[:5], zh_sentences[:5]))\n",
    "accuracy = evaluate_model(model, test_data, en_tokenizer, zh_tokenizer)\n",
    "\n",
    "# æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯\n",
    "print(f\"\\næ¨¡å‹ç»Ÿè®¡ä¿¡æ¯:\")\n",
    "print(f\"å‚æ•°æ€»é‡: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"æ¨¡å‹å¤§å°: {sum(p.numel() for p in model.parameters()) * 4 / (1024**2):.2f} MB\")\n",
    "print(f\"è®­ç»ƒè½®æ•°: {num_epochs}\")\n",
    "print(f\"æœ€ç»ˆæŸå¤±: {losses[-1]:.4f}\")\n",
    "print(f\"æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f7c8d8",
   "metadata": {},
   "source": [
    "## 6.9 è®­ç»ƒæŠ€å·§å’Œä¼˜åŒ–ç­–ç•¥\n",
    "\n",
    "è®©æˆ‘ä»¬è®¨è®ºä¸€äº›é‡è¦çš„è®­ç»ƒæŠ€å·§ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler:\n",
    "    \"\"\"\n",
    "    Transformerè®ºæ–‡ä¸­çš„å­¦ä¹ ç‡è°ƒåº¦å™¨\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.step_num = 0\n",
    "    \n",
    "    def get_lr(self):\n",
    "        self.step_num += 1\n",
    "        arg1 = self.step_num ** (-0.5)\n",
    "        arg2 = self.step_num * (self.warmup_steps ** (-1.5))\n",
    "        return (self.d_model ** (-0.5)) * min(arg1, arg2)\n",
    "\n",
    "# å¯è§†åŒ–å­¦ä¹ ç‡è°ƒåº¦\n",
    "scheduler = LearningRateScheduler(d_model=128, warmup_steps=1000)\n",
    "steps = list(range(1, 5000))\n",
    "lrs = []\n",
    "\n",
    "for step in steps:\n",
    "    scheduler.step_num = step - 1\n",
    "    lrs.append(scheduler.get_lr())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(steps, lrs, 'b-', linewidth=2)\n",
    "plt.title('Transformerå­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥')\n",
    "plt.xlabel('è®­ç»ƒæ­¥æ•°')\n",
    "plt.ylabel('å­¦ä¹ ç‡')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axvline(x=1000, color='r', linestyle='--', alpha=0.7, label='Warmupç»“æŸç‚¹')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# è®­ç»ƒæŠ€å·§æ€»ç»“\n",
    "print(\"Transformerè®­ç»ƒæŠ€å·§å’Œä¼˜åŒ–ç­–ç•¥:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"1. å­¦ä¹ ç‡è°ƒåº¦:\")\n",
    "print(\"   - é¢„çƒ­é˜¶æ®µï¼šçº¿æ€§å¢åŠ \")\n",
    "print(\"   - åç»­é˜¶æ®µï¼šæŒ‰æ­¥æ•°å¹³æ–¹æ ¹è¡°å‡\")\n",
    "print()\n",
    "print(\"2. æ­£åˆ™åŒ–æŠ€æœ¯:\")\n",
    "print(\"   - Dropout: é˜²æ­¢è¿‡æ‹Ÿåˆ\")\n",
    "print(\"   - Layer Normalization: ç¨³å®šè®­ç»ƒ\")\n",
    "print(\"   - æ¢¯åº¦è£å‰ª: é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸\")\n",
    "print()\n",
    "print(\"3. æƒé‡åˆå§‹åŒ–:\")\n",
    "print(\"   - Xavier/Glorotåˆå§‹åŒ–\")\n",
    "print(\"   - åµŒå…¥å±‚ä½¿ç”¨è¾ƒå°çš„æ ‡å‡†å·®\")\n",
    "print()\n",
    "print(\"4. è®­ç»ƒç­–ç•¥:\")\n",
    "print(\"   - Label Smoothing: æé«˜æ³›åŒ–èƒ½åŠ›\")\n",
    "print(\"   - Beam Search: æé«˜è§£ç è´¨é‡\")\n",
    "print(\"   - æ•°æ®å¢å¼º: å¢åŠ è®­ç»ƒæ•°æ®å¤šæ ·æ€§\")\n",
    "print()\n",
    "print(\"5. æ•ˆç‡ä¼˜åŒ–:\")\n",
    "print(\"   - æ¢¯åº¦ç´¯ç§¯: æ¨¡æ‹Ÿå¤§æ‰¹æ¬¡è®­ç»ƒ\")\n",
    "print(\"   - æ··åˆç²¾åº¦è®­ç»ƒ: å‡å°‘æ˜¾å­˜å ç”¨\")\n",
    "print(\"   - æ¨¡å‹å¹¶è¡Œ: å¤„ç†å¤§æ¨¡å‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f8b5c7",
   "metadata": {},
   "source": [
    "## æ€»ç»“\n",
    "\n",
    "åœ¨è¿™ä¸ªæ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å®ç°äº†å®Œæ•´çš„Transformeræ¨¡å‹ï¼š\n",
    "\n",
    "### ğŸ—ï¸ æ¶æ„å®ç°ï¼š\n",
    "1. **ç¼–ç å™¨**ï¼šå¤šå±‚ç¼–ç å™¨å—çš„å †å \n",
    "2. **è§£ç å™¨**ï¼šåŒ…å«è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›çš„è§£ç å™¨å—\n",
    "3. **å®Œæ•´æ¨¡å‹**ï¼šç¼–ç å™¨-è§£ç å™¨æ¶æ„\n",
    "\n",
    "### ğŸ’¼ å®é™…åº”ç”¨ï¼š\n",
    "- **æœºå™¨ç¿»è¯‘ä»»åŠ¡**ï¼šè‹±è¯­åˆ°ä¸­æ–‡çš„ç®€å•ç¿»è¯‘\n",
    "- **ç«¯åˆ°ç«¯è®­ç»ƒ**ï¼šä»æ•°æ®é¢„å¤„ç†åˆ°æ¨¡å‹è®­ç»ƒ\n",
    "- **æ¨ç†å®ç°**ï¼šè‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹\n",
    "\n",
    "### ğŸ”§ å…³é”®æŠ€å·§ï¼š\n",
    "- **æ©ç æœºåˆ¶**ï¼šå¡«å……æ©ç å’Œå› æœæ©ç \n",
    "- **å­¦ä¹ ç‡è°ƒåº¦**ï¼šé¢„çƒ­å’Œè¡°å‡ç­–ç•¥\n",
    "- **æ¢¯åº¦è£å‰ª**ï¼šç¨³å®šè®­ç»ƒè¿‡ç¨‹\n",
    "- **æƒé‡åˆå§‹åŒ–**ï¼šåˆé€‚çš„å‚æ•°åˆå§‹åŒ–\n",
    "\n",
    "### ğŸ“Š å¯è§†åŒ–åˆ†æï¼š\n",
    "- **è®­ç»ƒè¿‡ç¨‹**ï¼šæŸå¤±æ›²çº¿ç›‘æ§\n",
    "- **æ³¨æ„åŠ›æƒé‡**ï¼šç†è§£æ¨¡å‹å…³æ³¨ç‚¹\n",
    "- **ç¿»è¯‘è´¨é‡**ï¼šå®šæ€§å’Œå®šé‡è¯„ä¼°\n",
    "\n",
    "### ğŸš€ æ‰©å±•æ–¹å‘ï¼š\n",
    "1. **æ›´å¤§çš„æ•°æ®é›†**ï¼šä½¿ç”¨çœŸå®çš„å¹¶è¡Œè¯­æ–™\n",
    "2. **æ›´å¤æ‚çš„åˆ†è¯**ï¼šBPEã€SentencePieceç­‰\n",
    "3. **è¯„ä¼°æŒ‡æ ‡**ï¼šBLEUã€ROUGEç­‰è‡ªåŠ¨è¯„ä¼°\n",
    "4. **ä¼˜åŒ–ç­–ç•¥**ï¼šLabel Smoothingã€Beam Searchç­‰\n",
    "5. **æ•ˆç‡æå‡**ï¼šæ¨¡å‹å‹ç¼©ã€çŸ¥è¯†è’¸é¦ç­‰\n",
    "\n",
    "è¿™ä¸ªå®Œæ•´çš„å®ç°å±•ç¤ºäº†Transformerçš„å¼ºå¤§èƒ½åŠ›ï¼Œä¹Ÿä¸ºç†è§£ç°ä»£NLPæ¨¡å‹ï¼ˆå¦‚BERTã€GPTï¼‰å¥ å®šäº†åŸºç¡€ã€‚\n",
    "\n",
    "### ä¸‹ä¸€æ­¥å­¦ä¹ ï¼š\n",
    "- [07-transformer-variants.ipynb](07-transformer-variants.ipynb) - Transformerå˜ä½“å’Œç°ä»£åº”ç”¨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}