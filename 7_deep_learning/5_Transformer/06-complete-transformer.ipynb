{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8b4c8d0",
   "metadata": {},
   "source": [
    "# 6. 完整Transformer实现\n",
    "\n",
    "在前面的教程中，我们学习了Transformer的各个组件。现在是时候将它们组合成完整的Transformer模型，并实现一个端到端的机器翻译任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f8a2c0",
   "metadata": {},
   "source": [
    "## 6.1 完整Transformer架构\n",
    "\n",
    "原始的Transformer包含编码器（Encoder）和解码器（Decoder）两部分：\n",
    "\n",
    "### 编码器（Encoder）\n",
    "- 输入：源序列 + 位置编码\n",
    "- 结构：N层编码器块的堆叠\n",
    "- 输出：编码后的表示\n",
    "\n",
    "### 解码器（Decoder）\n",
    "- 输入：目标序列 + 位置编码 + 编码器输出\n",
    "- 结构：N层解码器块的堆叠\n",
    "- 输出：生成的序列概率分布\n",
    "\n",
    "![完整Transformer架构](images/complete_transformer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e7c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import copy\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "# 从utils导入基础组件\n",
    "from utils import (\n",
    "    MultiHeadAttention, \n",
    "    SinusoidalPositionalEncoding, \n",
    "    FeedForwardNetwork,\n",
    "    create_padding_mask,\n",
    "    create_causal_mask,\n",
    "    setup_matplotlib_chinese,\n",
    "    set_random_seed\n",
    ")\n",
    "\n",
    "# 设置环境\n",
    "setup_matplotlib_chinese()\n",
    "set_random_seed(42)\n",
    "\n",
    "print(f\"PyTorch版本: {torch.__version__}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e7c4f5",
   "metadata": {},
   "source": [
    "## 6.2 解码器块（Decoder Block）\n",
    "\n",
    "解码器块与编码器块类似，但包含三个子层：\n",
    "1. **掩码多头自注意力**：防止看到未来的token\n",
    "2. **编码器-解码器注意力**：关注编码器的输出\n",
    "3. **前馈网络**：非线性变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b7f4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer解码器块\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super(TransformerDecoderBlock, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # 三个注意力层\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)  # 掩码自注意力\n",
    "        self.cross_attention = MultiHeadAttention(d_model, num_heads, dropout)  # 编码器-解码器注意力\n",
    "        \n",
    "        # 前馈网络\n",
    "        self.feed_forward = FeedForwardNetwork(d_model, d_ff, dropout)\n",
    "        \n",
    "        # 层归一化\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor,\n",
    "                src_mask: Optional[torch.Tensor] = None,\n",
    "                tgt_mask: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        Args:\n",
    "            x: 解码器输入 [batch_size, tgt_len, d_model]\n",
    "            encoder_output: 编码器输出 [batch_size, src_len, d_model]\n",
    "            src_mask: 源序列掩码 [batch_size, src_len, src_len]\n",
    "            tgt_mask: 目标序列掩码 [batch_size, tgt_len, tgt_len]\n",
    "        \n",
    "        Returns:\n",
    "            output: [batch_size, tgt_len, d_model]\n",
    "            self_attn_weights: 自注意力权重\n",
    "            cross_attn_weights: 交叉注意力权重\n",
    "        \"\"\"\n",
    "        # 第一个子层：掩码多头自注意力\n",
    "        self_attn_output, self_attn_weights = self.self_attention(x, x, x, tgt_mask)\n",
    "        x1 = self.norm1(x + self.dropout(self_attn_output))\n",
    "        \n",
    "        # 第二个子层：编码器-解码器注意力\n",
    "        cross_attn_output, cross_attn_weights = self.cross_attention(\n",
    "            x1, encoder_output, encoder_output, src_mask\n",
    "        )\n",
    "        x2 = self.norm2(x1 + self.dropout(cross_attn_output))\n",
    "        \n",
    "        # 第三个子层：前馈网络\n",
    "        ff_output = self.feed_forward(x2)\n",
    "        x3 = self.norm3(x2 + self.dropout(ff_output))\n",
    "        \n",
    "        return x3, self_attn_weights, cross_attn_weights\n",
    "\n",
    "# 测试解码器块\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "d_ff = 1024\n",
    "tgt_len = 10\n",
    "src_len = 12\n",
    "batch_size = 2\n",
    "\n",
    "decoder_block = TransformerDecoderBlock(d_model, num_heads, d_ff)\n",
    "\n",
    "# 创建测试输入\n",
    "decoder_input = torch.randn(batch_size, tgt_len, d_model)\n",
    "encoder_output = torch.randn(batch_size, src_len, d_model)\n",
    "\n",
    "# 创建掩码\n",
    "tgt_mask = create_causal_mask(tgt_len).unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "# 前向传播\n",
    "output, self_attn, cross_attn = decoder_block(\n",
    "    decoder_input, encoder_output, tgt_mask=tgt_mask\n",
    ")\n",
    "\n",
    "print(f\"解码器块输入形状: {decoder_input.shape}\")\n",
    "print(f\"编码器输出形状: {encoder_output.shape}\")\n",
    "print(f\"解码器块输出形状: {output.shape}\")\n",
    "print(f\"自注意力权重形状: {self_attn.shape}\")\n",
    "print(f\"交叉注意力权重形状: {cross_attn.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8b4a3",
   "metadata": {},
   "source": [
    "## 6.3 完整Transformer模型\n",
    "\n",
    "现在我们将编码器和解码器组合成完整的Transformer模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f7c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer编码器\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, d_model: int, num_heads: int, \n",
    "                 d_ff: int, num_layers: int, max_seq_len: int = 5000, \n",
    "                 dropout: float = 0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # 词嵌入和位置编码\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = SinusoidalPositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "        # 编码器层\n",
    "        from utils import TransformerBlock\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src: torch.Tensor, src_mask: Optional[torch.Tensor] = None):\n",
    "        # 嵌入和位置编码\n",
    "        x = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 通过编码器层\n",
    "        attention_weights = []\n",
    "        for layer in self.layers:\n",
    "            x, attn_weights = layer(x, src_mask)\n",
    "            attention_weights.append(attn_weights)\n",
    "        \n",
    "        return x, attention_weights\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer解码器\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, d_model: int, num_heads: int, \n",
    "                 d_ff: int, num_layers: int, max_seq_len: int = 5000, \n",
    "                 dropout: float = 0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # 词嵌入和位置编码\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = SinusoidalPositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "        # 解码器层\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, tgt: torch.Tensor, encoder_output: torch.Tensor,\n",
    "                src_mask: Optional[torch.Tensor] = None,\n",
    "                tgt_mask: Optional[torch.Tensor] = None):\n",
    "        # 嵌入和位置编码\n",
    "        x = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 通过解码器层\n",
    "        self_attention_weights = []\n",
    "        cross_attention_weights = []\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x, self_attn, cross_attn = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "            self_attention_weights.append(self_attn)\n",
    "            cross_attention_weights.append(cross_attn)\n",
    "        \n",
    "        return x, self_attention_weights, cross_attention_weights\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    完整的Transformer模型\n",
    "    \"\"\"\n",
    "    def __init__(self, src_vocab_size: int, tgt_vocab_size: int,\n",
    "                 d_model: int = 512, num_heads: int = 8, \n",
    "                 d_ff: int = 2048, num_layers: int = 6,\n",
    "                 max_seq_len: int = 5000, dropout: float = 0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # 编码器和解码器\n",
    "        self.encoder = TransformerEncoder(\n",
    "            src_vocab_size, d_model, num_heads, d_ff, num_layers, max_seq_len, dropout\n",
    "        )\n",
    "        self.decoder = TransformerDecoder(\n",
    "            tgt_vocab_size, d_model, num_heads, d_ff, num_layers, max_seq_len, dropout\n",
    "        )\n",
    "        \n",
    "        # 输出投影层\n",
    "        self.output_projection = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "        # 权重初始化\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"初始化权重\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, mean=0, std=0.1)\n",
    "    \n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor,\n",
    "                src_mask: Optional[torch.Tensor] = None,\n",
    "                tgt_mask: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        Args:\n",
    "            src: 源序列 [batch_size, src_len]\n",
    "            tgt: 目标序列 [batch_size, tgt_len]\n",
    "            src_mask: 源序列掩码\n",
    "            tgt_mask: 目标序列掩码\n",
    "        \n",
    "        Returns:\n",
    "            output: [batch_size, tgt_len, tgt_vocab_size]\n",
    "            attention_weights: 注意力权重字典\n",
    "        \"\"\"\n",
    "        # 编码器\n",
    "        encoder_output, encoder_attention = self.encoder(src, src_mask)\n",
    "        \n",
    "        # 解码器\n",
    "        decoder_output, decoder_self_attention, decoder_cross_attention = self.decoder(\n",
    "            tgt, encoder_output, src_mask, tgt_mask\n",
    "        )\n",
    "        \n",
    "        # 输出投影\n",
    "        output = self.output_projection(decoder_output)\n",
    "        \n",
    "        attention_weights = {\n",
    "            'encoder_attention': encoder_attention,\n",
    "            'decoder_self_attention': decoder_self_attention,\n",
    "            'decoder_cross_attention': decoder_cross_attention\n",
    "        }\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# 创建完整的Transformer模型\n",
    "src_vocab_size = 1000\n",
    "tgt_vocab_size = 1000\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "d_ff = 1024\n",
    "num_layers = 4\n",
    "\n",
    "transformer = Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ff=d_ff,\n",
    "    num_layers=num_layers\n",
    ")\n",
    "\n",
    "# 计算参数量\n",
    "total_params = sum(p.numel() for p in transformer.parameters())\n",
    "trainable_params = sum(p.numel() for p in transformer.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Transformer模型参数总量: {total_params:,}\")\n",
    "print(f\"可训练参数量: {trainable_params:,}\")\n",
    "print(f\"模型大小: {total_params * 4 / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b8b2f5",
   "metadata": {},
   "source": [
    "## 6.4 机器翻译任务实战\n",
    "\n",
    "让我们用完整的Transformer模型来实现一个简单的机器翻译任务："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f8b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    \"\"\"\n",
    "    简单的分词器，用于演示\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.pad_token = '<PAD>'\n",
    "        self.sos_token = '<SOS>'\n",
    "        self.eos_token = '<EOS>'\n",
    "        self.unk_token = '<UNK>'\n",
    "        \n",
    "        self.special_tokens = [self.pad_token, self.sos_token, self.eos_token, self.unk_token]\n",
    "        self.vocab = {}\n",
    "        self.idx_to_token = {}\n",
    "        \n",
    "    def build_vocab(self, sentences: List[str]):\n",
    "        \"\"\"构建词汇表\"\"\"\n",
    "        # 添加特殊token\n",
    "        for i, token in enumerate(self.special_tokens):\n",
    "            self.vocab[token] = i\n",
    "            self.idx_to_token[i] = token\n",
    "        \n",
    "        # 添加词汇\n",
    "        word_count = {}\n",
    "        for sentence in sentences:\n",
    "            for word in sentence.lower().split():\n",
    "                word_count[word] = word_count.get(word, 0) + 1\n",
    "        \n",
    "        # 按频率排序并添加到词汇表\n",
    "        sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for word, _ in sorted_words:\n",
    "            if word not in self.vocab:\n",
    "                idx = len(self.vocab)\n",
    "                self.vocab[word] = idx\n",
    "                self.idx_to_token[idx] = word\n",
    "    \n",
    "    def encode(self, sentence: str, max_len: int = None) -> List[int]:\n",
    "        \"\"\"编码句子\"\"\"\n",
    "        tokens = [self.vocab.get(word.lower(), self.vocab[self.unk_token]) \n",
    "                 for word in sentence.split()]\n",
    "        \n",
    "        # 添加SOS和EOS\n",
    "        tokens = [self.vocab[self.sos_token]] + tokens + [self.vocab[self.eos_token]]\n",
    "        \n",
    "        # 截断或填充\n",
    "        if max_len:\n",
    "            if len(tokens) > max_len:\n",
    "                tokens = tokens[:max_len]\n",
    "            else:\n",
    "                tokens += [self.vocab[self.pad_token]] * (max_len - len(tokens))\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def decode(self, tokens: List[int]) -> str:\n",
    "        \"\"\"解码token序列\"\"\"\n",
    "        words = []\n",
    "        for token in tokens:\n",
    "            word = self.idx_to_token.get(token, self.unk_token)\n",
    "            if word == self.eos_token:\n",
    "                break\n",
    "            if word not in self.special_tokens:\n",
    "                words.append(word)\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    @property\n",
    "    def pad_token_id(self):\n",
    "        return self.vocab[self.pad_token]\n",
    "    \n",
    "    @property\n",
    "    def sos_token_id(self):\n",
    "        return self.vocab[self.sos_token]\n",
    "    \n",
    "    @property\n",
    "    def eos_token_id(self):\n",
    "        return self.vocab[self.eos_token]\n",
    "\n",
    "# 创建示例数据集（英语->中文的简单例子）\n",
    "def create_toy_dataset():\n",
    "    \"\"\"创建玩具数据集\"\"\"\n",
    "    en_sentences = [\n",
    "        \"hello world\",\n",
    "        \"how are you\",\n",
    "        \"good morning\",\n",
    "        \"thank you\",\n",
    "        \"goodbye\",\n",
    "        \"i love you\",\n",
    "        \"what is your name\",\n",
    "        \"nice to meet you\",\n",
    "        \"have a good day\",\n",
    "        \"see you later\"\n",
    "    ]\n",
    "    \n",
    "    zh_sentences = [\n",
    "        \"你好 世界\",\n",
    "        \"你 好 吗\",\n",
    "        \"早上 好\",\n",
    "        \"谢谢 你\",\n",
    "        \"再见\",\n",
    "        \"我 爱 你\",\n",
    "        \"你 的 名字 是 什么\",\n",
    "        \"很 高兴 见到 你\",\n",
    "        \"祝 你 有 美好 的 一天\",\n",
    "        \"回头 见\"\n",
    "    ]\n",
    "    \n",
    "    return en_sentences, zh_sentences\n",
    "\n",
    "# 准备数据\n",
    "en_sentences, zh_sentences = create_toy_dataset()\n",
    "\n",
    "# 创建分词器\n",
    "en_tokenizer = SimpleTokenizer()\n",
    "zh_tokenizer = SimpleTokenizer()\n",
    "\n",
    "en_tokenizer.build_vocab(en_sentences)\n",
    "zh_tokenizer.build_vocab(zh_sentences)\n",
    "\n",
    "print(f\"英语词汇表大小: {en_tokenizer.vocab_size}\")\n",
    "print(f\"中文词汇表大小: {zh_tokenizer.vocab_size}\")\n",
    "\n",
    "# 编码示例\n",
    "example_en = \"hello world\"\n",
    "example_zh = \"你好 世界\"\n",
    "\n",
    "en_encoded = en_tokenizer.encode(example_en, max_len=10)\n",
    "zh_encoded = zh_tokenizer.encode(example_zh, max_len=10)\n",
    "\n",
    "print(f\"\\n编码示例:\")\n",
    "print(f\"英语: '{example_en}' -> {en_encoded}\")\n",
    "print(f\"中文: '{example_zh}' -> {zh_encoded}\")\n",
    "print(f\"解码: {en_tokenizer.decode(en_encoded)}\")\n",
    "print(f\"解码: {zh_tokenizer.decode(zh_encoded)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f7c8f6",
   "metadata": {},
   "source": [
    "## 6.5 训练过程实现\n",
    "\n",
    "现在让我们实现完整的训练过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b3b5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    翻译数据集\n",
    "    \"\"\"\n",
    "    def __init__(self, src_sentences, tgt_sentences, src_tokenizer, tgt_tokenizer, max_len=20):\n",
    "        self.src_sentences = src_sentences\n",
    "        self.tgt_sentences = tgt_sentences\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src_encoded = self.src_tokenizer.encode(self.src_sentences[idx], self.max_len)\n",
    "        tgt_encoded = self.tgt_tokenizer.encode(self.tgt_sentences[idx], self.max_len)\n",
    "        \n",
    "        return {\n",
    "            'src': torch.tensor(src_encoded, dtype=torch.long),\n",
    "            'tgt': torch.tensor(tgt_encoded, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def create_masks(src, tgt, src_pad_idx, tgt_pad_idx):\n",
    "    \"\"\"\n",
    "    创建训练所需的掩码\n",
    "    \"\"\"\n",
    "    # 源序列填充掩码\n",
    "    src_mask = (src != src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    \n",
    "    # 目标序列填充掩码和因果掩码\n",
    "    tgt_len = tgt.size(1)\n",
    "    tgt_pad_mask = (tgt != tgt_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    tgt_causal_mask = torch.tril(torch.ones(tgt_len, tgt_len, device=tgt.device))\n",
    "    tgt_mask = tgt_pad_mask & tgt_causal_mask\n",
    "    \n",
    "    return src_mask, tgt_mask\n",
    "\n",
    "def train_step(model, batch, criterion, optimizer, src_tokenizer, tgt_tokenizer):\n",
    "    \"\"\"\n",
    "    单步训练\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    src = batch['src']\n",
    "    tgt = batch['tgt']\n",
    "    \n",
    "    # 目标序列分为输入和标签\n",
    "    tgt_input = tgt[:, :-1]  # 去掉最后一个token作为输入\n",
    "    tgt_label = tgt[:, 1:]   # 去掉第一个token作为标签\n",
    "    \n",
    "    # 创建掩码\n",
    "    src_mask, tgt_mask = create_masks(\n",
    "        src, tgt_input, src_tokenizer.pad_token_id, tgt_tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    # 前向传播\n",
    "    output, _ = model(src, tgt_input, src_mask, tgt_mask)\n",
    "    \n",
    "    # 计算损失\n",
    "    loss = criterion(output.reshape(-1, output.size(-1)), tgt_label.reshape(-1))\n",
    "    \n",
    "    # 反向传播\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # 梯度裁剪\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "dataset = TranslationDataset(en_sentences, zh_sentences, en_tokenizer, zh_tokenizer, max_len=15)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# 创建模型\n",
    "model = Transformer(\n",
    "    src_vocab_size=en_tokenizer.vocab_size,\n",
    "    tgt_vocab_size=zh_tokenizer.vocab_size,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    d_ff=512,\n",
    "    num_layers=2,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# 损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=zh_tokenizer.pad_token_id)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# 训练循环\n",
    "num_epochs = 50\n",
    "losses = []\n",
    "\n",
    "print(\"开始训练...\")\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        loss = train_step(model, batch, criterion, optimizer, en_tokenizer, zh_tokenizer)\n",
    "        epoch_losses.append(loss)\n",
    "    \n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"训练完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f7c8f8",
   "metadata": {},
   "source": [
    "## 6.6 推理和翻译\n",
    "\n",
    "训练完成后，让我们实现翻译功能："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, src_sentence, src_tokenizer, tgt_tokenizer, max_len=20):\n",
    "    \"\"\"\n",
    "    翻译函数\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 编码源句子\n",
    "        src_tokens = src_tokenizer.encode(src_sentence, max_len)\n",
    "        src = torch.tensor(src_tokens).unsqueeze(0)  # 添加batch维度\n",
    "        \n",
    "        # 编码器前向传播\n",
    "        src_mask = (src != src_tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "        encoder_output, _ = model.encoder(src, src_mask)\n",
    "        \n",
    "        # 初始化解码序列\n",
    "        tgt_tokens = [tgt_tokenizer.sos_token_id]\n",
    "        \n",
    "        # 逐步生成\n",
    "        for _ in range(max_len - 1):\n",
    "            tgt = torch.tensor(tgt_tokens).unsqueeze(0)\n",
    "            tgt_len = tgt.size(1)\n",
    "            \n",
    "            # 创建目标掩码\n",
    "            tgt_mask = torch.tril(torch.ones(tgt_len, tgt_len))\n",
    "            tgt_mask = tgt_mask.unsqueeze(0)\n",
    "            \n",
    "            # 解码器前向传播\n",
    "            decoder_output, _, _ = model.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "            \n",
    "            # 输出投影\n",
    "            output = model.output_projection(decoder_output)\n",
    "            \n",
    "            # 获取下一个token\n",
    "            next_token = output[0, -1, :].argmax().item()\n",
    "            tgt_tokens.append(next_token)\n",
    "            \n",
    "            # 如果生成了结束token，停止生成\n",
    "            if next_token == tgt_tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    # 解码生成的序列\n",
    "    translated = tgt_tokenizer.decode(tgt_tokens[1:])  # 去掉SOS token\n",
    "    return translated\n",
    "\n",
    "# 测试翻译\n",
    "test_sentences = [\n",
    "    \"hello world\",\n",
    "    \"good morning\",\n",
    "    \"thank you\",\n",
    "    \"i love you\"\n",
    "]\n",
    "\n",
    "print(\"翻译测试结果:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for en_sentence in test_sentences:\n",
    "    translated = translate(model, en_sentence, en_tokenizer, zh_tokenizer)\n",
    "    print(f\"英语: {en_sentence}\")\n",
    "    print(f\"翻译: {translated}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f7c8f9",
   "metadata": {},
   "source": [
    "## 6.7 可视化分析\n",
    "\n",
    "让我们分析训练过程和注意力权重："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f8b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制训练损失曲线\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses, 'b-', linewidth=2)\n",
    "plt.title('训练损失曲线')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('损失')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 可视化注意力权重\n",
    "def visualize_translation_attention(model, src_sentence, tgt_sentence, \n",
    "                                   src_tokenizer, tgt_tokenizer):\n",
    "    \"\"\"\n",
    "    可视化翻译过程中的注意力权重\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 编码输入\n",
    "        src_tokens = src_tokenizer.encode(src_sentence, max_len=15)\n",
    "        tgt_tokens = tgt_tokenizer.encode(tgt_sentence, max_len=15)\n",
    "        \n",
    "        src = torch.tensor(src_tokens).unsqueeze(0)\n",
    "        tgt_input = torch.tensor(tgt_tokens[:-1]).unsqueeze(0)  # 去掉EOS\n",
    "        \n",
    "        # 创建掩码\n",
    "        src_mask, tgt_mask = create_masks(\n",
    "            src, tgt_input, src_tokenizer.pad_token_id, tgt_tokenizer.pad_token_id\n",
    "        )\n",
    "        \n",
    "        # 前向传播\n",
    "        output, attention_weights = model(src, tgt_input, src_mask, tgt_mask)\n",
    "        \n",
    "        # 获取cross attention权重（最后一层）\n",
    "        cross_attention = attention_weights['decoder_cross_attention'][-1][0, 0]  # 第一个头\n",
    "        \n",
    "        # 获取有效的token\n",
    "        src_words = [src_tokenizer.idx_to_token[idx] for idx in src_tokens \n",
    "                    if idx != src_tokenizer.pad_token_id]\n",
    "        tgt_words = [tgt_tokenizer.idx_to_token[idx] for idx in tgt_tokens[:-1] \n",
    "                    if idx != tgt_tokenizer.pad_token_id]\n",
    "        \n",
    "        # 截取有效的注意力权重\n",
    "        attention_matrix = cross_attention[:len(tgt_words), :len(src_words)].cpu().numpy()\n",
    "        \n",
    "        # 可视化\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(attention_matrix, annot=True, fmt='.3f', cmap='Blues',\n",
    "                   xticklabels=src_words, yticklabels=tgt_words)\n",
    "        plt.title(f'交叉注意力权重\\n源句: {src_sentence}\\n目标句: {tgt_sentence}')\n",
    "        plt.xlabel('源语言 (Key)')\n",
    "        plt.ylabel('目标语言 (Query)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# 可视化注意力权重示例\n",
    "visualize_translation_attention(\n",
    "    model, \"hello world\", \"你好 世界\", en_tokenizer, zh_tokenizer\n",
    ")\n",
    "\n",
    "visualize_translation_attention(\n",
    "    model, \"thank you\", \"谢谢 你\", en_tokenizer, zh_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f7c8f9",
   "metadata": {},
   "source": [
    "## 6.8 模型评估和改进\n",
    "\n",
    "让我们分析模型性能并讨论改进方向："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e7c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_data, src_tokenizer, tgt_tokenizer):\n",
    "    \"\"\"\n",
    "    评估模型性能\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    correct_translations = 0\n",
    "    total_translations = len(test_data)\n",
    "    \n",
    "    print(\"模型评估结果:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for en_sentence, expected_zh in test_data:\n",
    "        predicted_zh = translate(model, en_sentence, src_tokenizer, tgt_tokenizer)\n",
    "        \n",
    "        # 简单的完全匹配评估\n",
    "        is_correct = predicted_zh.strip() == expected_zh.strip()\n",
    "        if is_correct:\n",
    "            correct_translations += 1\n",
    "        \n",
    "        print(f\"源句: {en_sentence}\")\n",
    "        print(f\"期望: {expected_zh}\")\n",
    "        print(f\"预测: {predicted_zh}\")\n",
    "        print(f\"正确: {'✓' if is_correct else '✗'}\")\n",
    "        print(\"-\" * 30)\n",
    "    \n",
    "    accuracy = correct_translations / total_translations\n",
    "    print(f\"\\n整体准确率: {accuracy:.2%} ({correct_translations}/{total_translations})\")\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# 评估模型\n",
    "test_data = list(zip(en_sentences[:5], zh_sentences[:5]))\n",
    "accuracy = evaluate_model(model, test_data, en_tokenizer, zh_tokenizer)\n",
    "\n",
    "# 模型统计信息\n",
    "print(f\"\\n模型统计信息:\")\n",
    "print(f\"参数总量: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"模型大小: {sum(p.numel() for p in model.parameters()) * 4 / (1024**2):.2f} MB\")\n",
    "print(f\"训练轮数: {num_epochs}\")\n",
    "print(f\"最终损失: {losses[-1]:.4f}\")\n",
    "print(f\"测试准确率: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f7c8d8",
   "metadata": {},
   "source": [
    "## 6.9 训练技巧和优化策略\n",
    "\n",
    "让我们讨论一些重要的训练技巧："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler:\n",
    "    \"\"\"\n",
    "    Transformer论文中的学习率调度器\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.step_num = 0\n",
    "    \n",
    "    def get_lr(self):\n",
    "        self.step_num += 1\n",
    "        arg1 = self.step_num ** (-0.5)\n",
    "        arg2 = self.step_num * (self.warmup_steps ** (-1.5))\n",
    "        return (self.d_model ** (-0.5)) * min(arg1, arg2)\n",
    "\n",
    "# 可视化学习率调度\n",
    "scheduler = LearningRateScheduler(d_model=128, warmup_steps=1000)\n",
    "steps = list(range(1, 5000))\n",
    "lrs = []\n",
    "\n",
    "for step in steps:\n",
    "    scheduler.step_num = step - 1\n",
    "    lrs.append(scheduler.get_lr())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(steps, lrs, 'b-', linewidth=2)\n",
    "plt.title('Transformer学习率调度策略')\n",
    "plt.xlabel('训练步数')\n",
    "plt.ylabel('学习率')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axvline(x=1000, color='r', linestyle='--', alpha=0.7, label='Warmup结束点')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 训练技巧总结\n",
    "print(\"Transformer训练技巧和优化策略:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"1. 学习率调度:\")\n",
    "print(\"   - 预热阶段：线性增加\")\n",
    "print(\"   - 后续阶段：按步数平方根衰减\")\n",
    "print()\n",
    "print(\"2. 正则化技术:\")\n",
    "print(\"   - Dropout: 防止过拟合\")\n",
    "print(\"   - Layer Normalization: 稳定训练\")\n",
    "print(\"   - 梯度裁剪: 防止梯度爆炸\")\n",
    "print()\n",
    "print(\"3. 权重初始化:\")\n",
    "print(\"   - Xavier/Glorot初始化\")\n",
    "print(\"   - 嵌入层使用较小的标准差\")\n",
    "print()\n",
    "print(\"4. 训练策略:\")\n",
    "print(\"   - Label Smoothing: 提高泛化能力\")\n",
    "print(\"   - Beam Search: 提高解码质量\")\n",
    "print(\"   - 数据增强: 增加训练数据多样性\")\n",
    "print()\n",
    "print(\"5. 效率优化:\")\n",
    "print(\"   - 梯度累积: 模拟大批次训练\")\n",
    "print(\"   - 混合精度训练: 减少显存占用\")\n",
    "print(\"   - 模型并行: 处理大模型\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f8b5c7",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "在这个教程中，我们实现了完整的Transformer模型：\n",
    "\n",
    "### 🏗️ 架构实现：\n",
    "1. **编码器**：多层编码器块的堆叠\n",
    "2. **解码器**：包含自注意力和交叉注意力的解码器块\n",
    "3. **完整模型**：编码器-解码器架构\n",
    "\n",
    "### 💼 实际应用：\n",
    "- **机器翻译任务**：英语到中文的简单翻译\n",
    "- **端到端训练**：从数据预处理到模型训练\n",
    "- **推理实现**：自回归生成过程\n",
    "\n",
    "### 🔧 关键技巧：\n",
    "- **掩码机制**：填充掩码和因果掩码\n",
    "- **学习率调度**：预热和衰减策略\n",
    "- **梯度裁剪**：稳定训练过程\n",
    "- **权重初始化**：合适的参数初始化\n",
    "\n",
    "### 📊 可视化分析：\n",
    "- **训练过程**：损失曲线监控\n",
    "- **注意力权重**：理解模型关注点\n",
    "- **翻译质量**：定性和定量评估\n",
    "\n",
    "### 🚀 扩展方向：\n",
    "1. **更大的数据集**：使用真实的并行语料\n",
    "2. **更复杂的分词**：BPE、SentencePiece等\n",
    "3. **评估指标**：BLEU、ROUGE等自动评估\n",
    "4. **优化策略**：Label Smoothing、Beam Search等\n",
    "5. **效率提升**：模型压缩、知识蒸馏等\n",
    "\n",
    "这个完整的实现展示了Transformer的强大能力，也为理解现代NLP模型（如BERT、GPT）奠定了基础。\n",
    "\n",
    "### 下一步学习：\n",
    "- [07-transformer-variants.ipynb](07-transformer-variants.ipynb) - Transformer变体和现代应用"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}